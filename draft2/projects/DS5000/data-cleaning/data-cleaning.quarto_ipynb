{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction  \n",
        "\n",
        "Data cleaning is the next step in processing data into actionable insights. Data cleaning involves removing NaNs, removing outliers, renaming columns, and applying transformations. Without performing such actions, we would not be able to draw any conclusions from our data as we will be analyzing inconsistent and incompatible data. My goal for data cleaning was to make sure my data had consistent, easy to read column names and row values, consistently formatted data, and standarization.\n",
        "\n",
        "### Standardization\n",
        "\n",
        "Standarizaiton, also known as Z-score Normalization is needed when a distribution is going to be used in a k means clustering, PCA, or , as it minimizes the amount of bias larger features get. Z score Standardization alters the data set so each variable will have a mean of 0, and a standard deviation of 1. This is done by subtracting the mean by every data point, and dividing by the standard deviation. After this process, data is ready to be further analyzed in clustering and dimensional reduction. \n",
        "\n",
        " \n",
        "##  Data Cleaning\n",
        "\n",
        "After obtaining the data for census_df, it was time to clean. The first thing I did was check for NA values, and found there was none. Next, I chose to perform column mapping on my dataset, as the current columns were code names such as K202801_004E and were confusing to understand. After cleaning the column names, the next step in data cleaning was to fix the state column. Currently this held the state FIPS identifier, with 1 for Alabama, 2 for Alaska, etc. I chose to map each FIPS code to the state name for easier identification and integrability. \n"
      ],
      "id": "eebefe77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# census_df = pd.read_csv('../../data/raw-data/census_raw_data.csv') \n",
        "\n",
        "# column_mapping = {\n",
        "#     \"K200101_001E\": \"Total_Population\",\n",
        "#     \"K200201_002E\": \"White\",\n",
        "#     \"K200201_003E\": \"African_American\",\n",
        "#     \"K200201_005E\": \"Asian\",\n",
        "#     \"K200201_004E\": \"American_Indian_and_Alaska_Native\",\n",
        "#     \"K200201_006E\": \"Native_Hawaiian_and_Pacific_Islander\",\n",
        "#     \"K202101_002E\": \"Veterans\",\n",
        "#     \"K202101_006E\": \"Nonveterans\",\n",
        "#     \"K201701_002E\": \"Below_Poverty_Level\",\n",
        "#     \"K201902_001E\": \"Median_Household_Income\",\n",
        "#     \"K201501_004E\": \"High_School_Graduate\",\n",
        "#     \"K201501_007E\": \"Bachelors_Degree_or_Higher\",\n",
        "#     \"K202801_002E\": \"Households_with_Computers\",\n",
        "#     \"K202801_004E\": \"Households_with_Internet\",\n",
        "#     \"K200103_001E\": \"Median_Age\",\n",
        "#     \"NAME\": \"District_Name\", \n",
        "#     \"congressional district\": \"Congressional_District\"\n",
        "# }\n",
        "\n",
        "# census_df.rename(columns=column_mapping, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# print(census_df.head())"
      ],
      "id": "eaa8bbd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After exploring the data through plots and statistics, I realized that it was best not to remove outliers, as this could be a key indicator of voter turnout (Urban vs rural, socioeconomic standing, small districts vs big districts). I chose to use standardization instead to prepare data for further analysis. I made a copy of the original data frame to maintain for testing and comparison, and then called StandardScalar from SKLearn to fit the numeric values of the dataset. After doing so the mean of every variable is 0, with a standard deviation of 1.\n",
        "\n",
        "\n",
        "\n",
        "This API endpoint was “https://api.census.gov/data/2018/acs/acsse?get=NAME,K200101_001E&for=congressional%20district:*&in=state:*&key=YOUR_KEY_GOES_HERE”. \n",
        "\n",
        "The United States Census Bureau API contains many codes, with each code relating to another variable statistic you wish to include. In the plain API call, the code K200101_001E is used, which correlates to total population. To add variables wanted to your API call, I referenced the website:\n",
        "\n",
        "“https://api.census.gov/data/2019/acs/acsse/variables.html”,  to find them.\n",
        "\n",
        " To add them to the API call, simply add them after the previous variable with a comma, such as K200101_001E,K200201_002E&for…etc. \n",
        "\n",
        "For my project, I chose to add:\n",
        "    \n",
        "  - **K200101_001E**: Total_Population  \n",
        "- **K200201_002E**: White  \n",
        "- **K200201_003E**: African_American  \n",
        "- **K200201_005E**: Asian  \n",
        "- **K200201_004E**: American_Indian_and_Alaska_Native  \n",
        "- **K200201_006E**: Native_Hawaiian_and_Pacific_Islander  \n",
        "- **K202101_002E**: Veterans  \n",
        "- **K202101_006E**: Nonveterans  \n",
        "- **K201701_002E**: Below_Poverty_Level  \n",
        "- **K201902_001E**: Median_Household_Income  \n",
        "- **K201501_004E**: High_School_Graduate  \n",
        "- **K201501_007E**: Bachelors_Degree_or_Higher  \n",
        "- **K202801_002E**: Households_with_Computers  \n",
        "- **K202801_004E**: Households_with_Internet\n",
        "\n",
        "\n",
        "Which I believed to be driving factors to which influence voter turn out. Thus, the full API call looked like:   \n",
        "  \"https://api.census.gov/data/2018/acs/acsse?\"\"get=NAME,K200101_001E,K200201_003E,K200201_002E, K200201_005E,K202101_002E,\"\"K202101_006E,K201701_002E,K201902_001E,K201501_004E,K201501_007E, K202801_002E,K202801_004E,K200201_004E,K200201_006E&for=congressional%20district:*\"\n",
        "\n",
        "This APi call produced JSON data such as:\n",
        "\n",
        "![](./images/json.png)\n",
        "\n",
        "\n",
        "For all Congressional Districts, with each line corresponding to a specific variable. \n",
        "\n",
        "## Census Voting Rates\n",
        "\n",
        "\n",
        "After the API call, I had my data and statistics for each congressional district. I then had to find data showcasing voter turnout in each congressional district. \n",
        "For this, I also utilized the United States Census Bureu. The reason my API called was for 2018, and not a more recent election such as 2020 or 2022 was because the US census only contained data for the 2018 campaign. To maintain data integrity, it would not be right to compare 2018 voter turnout to 2023 congressional district population statistics.\n",
        "\n",
        "\n",
        "The data set I found was found here, further choosing “table 1” as this showcases the total amount of votes cast.  \n",
        "https://www.census.gov/data/tables/time-series/demo/voting-and-registration/congressional-voting-tables.html\n",
        "\n",
        "\n",
        "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\n",
        "\n",
        "`Audience`:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts. \n",
        "\n",
        "- **Introduction and Motivation:** Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\n",
        "- **Overview of Methods:** Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\n",
        "- **Code:** Include the code you used to implement your workflow.\n",
        "- **Summary and Interpretation of Results:** Summarize your findings, interpret the results, and discuss their technical implications.\n",
        "\n",
        "## General comments:\n",
        "\n",
        "- **Iterative Process**: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\n",
        "- **Clarity and Reproducibility**: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\n",
        "- **Visualizations**: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n",
        "\n",
        "By the end of this phase, your cleaned data should be well-documented and ready for further stages, such as **Exploratory Data Analysis (EDA)** and **Machine Learning**.\n",
        "\n",
        "## What to address \n",
        "\n",
        "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\n",
        "\n",
        "The **Data Cleaning** page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\n",
        "\n",
        "The following is a guide to help you get started with possible thing to address on this page .\n",
        "\n",
        "- **Description of the Data Cleaning Process**: Explain the steps you took to clean and preprocess the data.\n",
        "- **Code Documentation**: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\n",
        "- **Provide examples of data before and after cleaning**: e.g. with df.head() or df.describe()\n",
        "- **Raw and Cleaned Data Links**: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in `data/processed-data`, or similar location which doesn't get synced to GitHub)\n",
        "\n",
        "Possible things to include:\n",
        "\n",
        "**Introduction to Data Cleaning**:\n",
        "\n",
        "- Provide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\n",
        "- Mention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n",
        "\n",
        "**Managing Missing Data**:\n",
        "\n",
        "- **Identify Missing Values**: Explain how you identified missing data and where it occurred.\n",
        "- **Handling Missing Data**: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\n",
        "- **Visualize Missing Data**: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n",
        "\n",
        "**Outlier Detection and Treatment**:\n",
        "\n",
        "- **Identify Outliers**: Describe the methods you used to detect outliers in the dataset.\n",
        "- **Addressing Outliers**: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\n",
        "- **Visualize Outliers**: Use visualizations (e.g., box plots) to show how outliers were managed.\n",
        "\n",
        "\n",
        "**Data Type Correction and Formatting**:\n",
        "\n",
        "- **Review Data Types**: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\n",
        "- **Transformation**: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\n",
        "- **Impact of Changes**: Briefly explain why these changes were necessary for accurate analysis.\n",
        "\n",
        "**Normalization and Scaling**:\n",
        "\n",
        "- **Data Distribution Analysis**: Check and discuss the distribution of numerical variables (e.g., skewness).\n",
        "- **Normalization Techniques**: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\n",
        "- **Before-and-After Visualizations**: Provide visualizations comparing the data before and after scaling or normalization.\n",
        "\n",
        "**Subsetting the Data**:\n",
        "\n",
        "- **Data Filtering**: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\n",
        "- **Rationale**: Justify why you chose to work with a particular subset of the data.\n"
      ],
      "id": "c7453429"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/iphone10/miniconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}