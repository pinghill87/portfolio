[
  {
    "objectID": "projects/Scholarship/index.html",
    "href": "projects/Scholarship/index.html",
    "title": "Risking Everything to Save Lives",
    "section": "",
    "text": "This project explores 25+ years of data to uncover where, how, and why attacks happen — and what patterns may help predict them in the future.",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#attacks-and-deaths-are-on-the-rise",
    "href": "projects/Scholarship/index.html#attacks-and-deaths-are-on-the-rise",
    "title": "Risking Everything to Save Lives",
    "section": "Attacks and Deaths are on the rise",
    "text": "Attacks and Deaths are on the rise\nFrom 1997-2024, attack frequency and aid workers deaths have had an upward trend, with a sharp increase in both starting in 2022.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Load data\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Clean year column\ndf = df.dropna(subset=['Year', 'Country Code'])\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\ndf = df.dropna(subset=['Year'])\ndf['Year'] = df['Year'].astype(int)\n\n# Group by year and count incidents\nyearly_incidents = df.groupby('Year').size().reset_index(name='Incident Count')\n\n# Remove 2025\nyearly_incidents = yearly_incidents[yearly_incidents['Year'] &lt; 2025]\n\n# Create line chart\nline = alt.Chart(yearly_incidents).mark_line(point=True).encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Incident Count:Q', title='Total Incidents'),\n    tooltip=[\n        alt.Tooltip('Year:O'),\n        alt.Tooltip('Incident Count:Q')\n    ]\n).properties(\n    width=700,\n    height=400,\n    title='Total Attacks on Aid Workers by Year (1997–2024)'\n)\n\nline\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Load data\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Clean and prepare year column\ndf = df.dropna(subset=['Year', 'Country Code', 'Total killed'])\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\ndf['Total killed'] = pd.to_numeric(df['Total killed'], errors='coerce')\ndf = df.dropna(subset=['Year', 'Total killed'])\ndf['Year'] = df['Year'].astype(int)\n\n# Group by year and sum total deaths\nyearly_deaths = df.groupby('Year')['Total killed'].sum().reset_index()\n\n\nyearly_deaths = yearly_deaths[yearly_deaths['Year'] &lt; 2025]\n\n# Create line chart\nline = alt.Chart(yearly_deaths).mark_line(point=True).encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Total killed:Q', title='Total Deaths'),\n    tooltip=[\n        alt.Tooltip('Year:O'),\n        alt.Tooltip('Total killed:Q', title='Total Deaths')\n    ]\n).properties(\n    width=700,\n    height=400,\n    title='Total Aid Worker Deaths by Year (1997–2024)'\n)\n\nline\n\n\n\n\n\n\n\n\nNote: Data for 2025 is incomplete and excluded from the trend line.",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#top-10-most-dangerous-countries-for-aid-workers",
    "href": "projects/Scholarship/index.html#top-10-most-dangerous-countries-for-aid-workers",
    "title": "Risking Everything to Save Lives",
    "section": "Top 10 Most Dangerous Countries for Aid Workers",
    "text": "Top 10 Most Dangerous Countries for Aid Workers\nNearly 70% of all attacks occurred in just these countries, making up 3029 of 4332 total incidents. x\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Clean and prepare the data\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\ndf = df.dropna(subset=['Year', 'Country'])\ndf['Year'] = df['Year'].astype(int)\n\n# Group by country for top 10 dangerous countries\ncountry_incidents = df.groupby('Country').size().reset_index(name='Incident Count')\ntop_countries = country_incidents.sort_values('Incident Count', ascending=False).head(10)\n\n# Bar chart: Top 10 countries\nbar_chart = alt.Chart(top_countries).mark_bar().encode(\n    x=alt.X('Incident Count:Q', title='Total Incidents'),\n    y=alt.Y('Country:N', sort='-x', title='Country'),\n    tooltip=['Country', 'Incident Count']\n).properties(\n    width=600,\n    height=400,\n    title='Top 10 Countries with Most Aid Worker Incidents (1997–2025)'\n)\n\nbar_chart",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#yearly-trend-of-incidents-in-top-5-countries",
    "href": "projects/Scholarship/index.html#yearly-trend-of-incidents-in-top-5-countries",
    "title": "Risking Everything to Save Lives",
    "section": "Yearly Trend of incidents in top 5 countries",
    "text": "Yearly Trend of incidents in top 5 countries\nAfghanistan and Syria peaked in 2013, but while afghanistan has declined since then, Syria peaked again in 2016-2020, finally declining in 2021.\nSouth Sudan had been on the rise througout the 2010-2020, but has declined starting in 2024.\nSudan and Palestine on the other hand, saw a sharp uptick starting in 2023.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Load and clean\ndf = pd.read_csv(\"security_incidents.csv\")\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\ndf = df.dropna(subset=['Year', 'Country'])\ndf['Year'] = df['Year'].astype(int)\ndf = df[df['Year'] &lt; 2025]\n\n# Top 5 countries\ntop_countries = df['Country'].value_counts().nlargest(7).index.tolist()\ndf_top = df[df['Country'].isin(top_countries)].copy()\n\n# Group by year and country\ndf_yearly = df_top.groupby(['Country', 'Year']).size().reset_index(name='Incident Count')\n\n# Duplicate and label\ndf_show_all = df_yearly.copy()\ndf_show_all['Country Selection'] = 'Show All'\ndf_yearly['Country Selection'] = df_yearly['Country']\n\n# Combine both\ndf_combined = pd.concat([df_yearly, df_show_all], ignore_index=True)\n\n# Dropdown selection\ndropdown = alt.binding_select(options=['Show All'] + sorted(top_countries), name='Select Country: ')\ncountry_select = alt.selection_point(fields=['Country Selection'], bind=dropdown, value='Show All')\n\n# Line chart\nchart = alt.Chart(df_combined).transform_filter(\n    country_select\n).mark_line(point=True).encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Incident Count:Q', title='Incident Count'),\n    color=alt.Color('Country:N', title='Country'),\n    tooltip=['Country', 'Year', 'Incident Count']\n).add_params(\n    country_select\n).properties(\n    width=580,\n    height=400,\n    title='Incident Trends: Select a Country or Show All'\n)\n\nchart",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#deadliness-of-attacks-by-country",
    "href": "projects/Scholarship/index.html#deadliness-of-attacks-by-country",
    "title": "Risking Everything to Save Lives",
    "section": "Deadliness of Attacks by Country",
    "text": "Deadliness of Attacks by Country\nThis chart maps countries by total number of incidents (x-axis) and total deaths (y-axis), with color representing the average number of people killed per attack.\nCountries in the upper-left quadrant, such as Palestine, tend to have fewer incidents but high death tolls — suggesting that when attacks occur, they are especially deadly.\nMeanwhile, countries in the upper-right quadrant, such as Afghanistan, face both frequent and deadly violence, posing ongoing risks to humanitarian workers.\nAfghanistan has experienced the most incidents (610) and the highest total deaths (486), averaging 0.8 deaths per attack. In contrast, the Occupied Palestinian Territories have seen only 158 incidents but nearly as many deaths (431), with an average of 2.73 deaths per attack — one of the highest among countries with significant incident counts.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Load data\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Drop missing data\ndf = df.dropna(subset=['Country Code', 'Country', 'Total killed'])\n\n# Convert values\ndf['Total killed'] = pd.to_numeric(df['Total killed'], errors='coerce')\ndf = df.dropna(subset=['Total killed'])\n\n# Group by country\ndf_grouped = df.groupby(['Country Code', 'Country']).agg(\n    Incident_Count=('Incident ID', 'count'),\n    Total_Killed=('Total killed', 'sum')\n).reset_index()\n\n# Compute average\ndf_grouped['Avg_Killed_Per_Incident'] = df_grouped['Total_Killed'] / df_grouped['Incident_Count']\n\nrisk_matrix = alt.Chart(df_grouped).mark_circle(size=100).encode(\n    x=alt.X('Incident_Count:Q', title='Total Incidents'),\n    y=alt.Y('Total_Killed:Q', title='Total Deaths'),\n    color=alt.Color('Avg_Killed_Per_Incident:Q',\n                    scale=alt.Scale(scheme='reds'),\n                    legend=alt.Legend(title='Avg Deaths/Incident')),\n    tooltip=[\n        alt.Tooltip('Country:N'),\n        alt.Tooltip('Incident_Count:Q', title='Total Incidents'),\n        alt.Tooltip('Total_Killed:Q', title='Total Deaths'),\n        alt.Tooltip('Avg_Killed_Per_Incident:Q', format='.2f', title='Avg Deaths/Incident')\n    ]\n).properties(\n    width=600,\n    height=400,\n    title='Total Deaths vs Incidents (Color = Avg Deaths/Incident)'\n)\n\nrisk_matrix",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#are-attack-types-evolving-over-time",
    "href": "projects/Scholarship/index.html#are-attack-types-evolving-over-time",
    "title": "Risking Everything to Save Lives",
    "section": "Are attack types Evolving over time?",
    "text": "Are attack types Evolving over time?\nThroughout the 2000s and 2010s, the main type of attacks were attributed to shootings and kidnapping, while starting in 2016, Aeriel Bombardment and Bodily Assault has been on the rise.\n\n\nCode\nimport altair as alt\nimport pandas as pd\n\n# Load & clean\ndf = pd.read_csv(\"security_incidents.csv\")\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\ndf = df.dropna(subset=['Year', 'Country', 'Means of attack'])\ndf['Year'] = df['Year'].astype(int)\ndf['Means of attack'] = df['Means of attack'].fillna('Unknown').str.strip().str.lower().str.title()\n\n# Top 10 countries by number of incidents\ntop_countries = df['Country'].value_counts().nlargest(7).index.tolist()\ndf_top = df[df['Country'].isin(top_countries)].copy()\n\n# Clean Means of attack\ntop_attacks = df_top['Means of attack'].value_counts().nlargest(5).index.tolist()\ndf_top['Means Clean'] = df_top['Means of attack'].apply(lambda x: x if x in top_attacks else 'Other')\n\n# Group by country\ncountry_counts = df_top.groupby(['Country', 'Year', 'Means Clean']).size().reset_index(name='Incident Count')\n\n# Group for \"All Countries\" total\nglobal_counts = df_top.groupby(['Year', 'Means Clean']).size().reset_index(name='Incident Count')\nglobal_counts['Country'] = 'All Countries'\n\n# Combine both\nattack_counts = pd.concat([country_counts, global_counts], ignore_index=True)\n\n# Sort order for color legend\nattack_order = attack_counts['Means Clean'].value_counts().index.tolist()\n\n\ndropdown_options = ['All Countries'] + sorted(top_countries)\ndropdown = alt.binding_select(options=dropdown_options, name=\"Select Country: \")\ncountry_select = alt.selection_point(fields=['Country'], bind=dropdown, value='All Countries')\n\n# Altair chart\nstacked_count = alt.Chart(attack_counts).transform_filter(\n    country_select\n).mark_bar().encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Incident Count:Q', title='Number of Incidents'),\n    color=alt.Color('Means Clean:N', title='Attack Type', sort=attack_order),\n    tooltip=['Country', 'Year', 'Means Clean', 'Incident Count']\n).add_params(\n    country_select\n).properties(\n    width=600,\n    height=400,\n    title='Top Types of Attacks on Aid Workers Over Time (Top 10 Countries)'\n)\n\nstacked_count",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#which-aid-workers-are-impacted-the-most",
    "href": "projects/Scholarship/index.html#which-aid-workers-are-impacted-the-most",
    "title": "Risking Everything to Save Lives",
    "section": "Which Aid workers are impacted the most",
    "text": "Which Aid workers are impacted the most\nNationals killed outnumber internationals killed by a large margin. Of the 380 total aid worker deaths, 369 were nationals, while only 11 were internationals. This means nearly all aid worker fatalities (97%) were local staff, highlighting the disproportionate risk borne by national aid workers on the ground.\n\n\nCode\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\ndf = df.dropna(subset=['Year'])\ndf['Year'] = df['Year'].astype(int)\n\n# Melt data\nvictim_df = df[['Year', 'Nationals killed', 'Internationals killed']].fillna(0)\nvictim_df = victim_df.groupby('Year').sum().reset_index()\n\nvictim_long = victim_df.melt(id_vars='Year', \n                             var_name='Victim Type', \n                             value_name='Count')\n\n# Plot\nvictim_bar = alt.Chart(victim_long).mark_bar().encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Count:Q', title='People Killed'),\n    color=alt.Color('Victim Type:N', title='Victim Group'),\n    tooltip=['Year:O', 'Victim Type:N', 'Count:Q']\n).properties(\n    width=600,\n    height=400,\n    title='Who Gets Killed in Attacks on Aid Workers'\n)\n\nvictim_bar",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#terrorism-and-counter-insurgency",
    "href": "projects/Scholarship/index.html#terrorism-and-counter-insurgency",
    "title": "Risking Everything to Save Lives",
    "section": "Terrorism and Counter-Insurgency",
    "text": "Terrorism and Counter-Insurgency\nAfghanistan has been in a constant conflict for over 40 years. In the 1970s-80s, Afghanistan was occupied by Soviets, however after withdrawal in 1989, Afghanistan fell to civil war, with the Taliban rising in power and by 1997-2001, controlled most of Afghanistan.\nAfter 9/11, the US invaded Afghanistan, targeting the Taliban and al-Qaeda. From 2001-2014, the US military and other foreign countries engage in combat, with NATO finally ending their missions in 2014 and the US signing a peace deal with the Taliban in 2020.\nAfter the US military withdrawals in 2021, the Taliban quickly rise to power again, raising concerns regarding human rights for women and aid workers.\nPBS News Desk 2021",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#where-are-the-incidents",
    "href": "projects/Scholarship/index.html#where-are-the-incidents",
    "title": "Risking Everything to Save Lives",
    "section": "Where are the Incidents?",
    "text": "Where are the Incidents?\nMajor confict areas include cities like Kabul , Jalalabad and Kandahar, where the political significance, international presence, population density, supply routes and the presence of insurgent groups all contributing to the risks and incidents involving aid workers.\n\n\nCode\nimport pandas as pd\nimport altair as alt\nimport json\n\n# Load the data\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Filter to Afghanistan and drop missing coordinates\ndf_afg = df[df['Country'] == 'Afghanistan'].dropna(subset=['Latitude', 'Longitude', 'City'])\n\n# Clean columns\ndf_afg['Latitude'] = pd.to_numeric(df_afg['Latitude'], errors='coerce')\ndf_afg['Longitude'] = pd.to_numeric(df_afg['Longitude'], errors='coerce')\ndf_afg['Total killed'] = pd.to_numeric(df_afg['Total killed'], errors='coerce')\ndf_afg = df_afg.dropna(subset=['Latitude', 'Longitude', 'City'])\n\n# Group by City\ndf_grouped = df_afg.groupby('City').agg(\n    Latitude=('Latitude', 'mean'),\n    Longitude=('Longitude', 'mean'),\n    Incident_Count=('Incident ID', 'count'),\n    Total_Killed=('Total killed', 'sum'),\n    Avg_Killed=('Total killed', 'mean')\n).reset_index()\n\n# Load Afghanistan GeoJSON outline\nwith open(\"custom.geo.json\", \"r\") as f:\n    geojson_data = json.load(f)\n\n# Base map outline\nbase_map = alt.Chart(alt.Data(values=geojson_data['features'])).mark_geoshape(\n    fill='lightgray',\n    stroke='black'\n).project(\n    type='mercator',\n    center=[65.1, 33.5],\n    scale=3600\n).properties(\n    width=800,\n    height=500,\n    title=' City-Level Hotspots of Attacks on Aid Workers in Afghanistan (1997–2025)'\n)\n\n# City-level bubbles\nbubbles = alt.Chart(df_grouped).mark_circle(opacity=0.9).encode(\n    longitude='Longitude:Q',\n    latitude='Latitude:Q',\n    size=alt.Size('Incident_Count:Q', scale=alt.Scale(range=[30, 300]), title='Incidents'),\n    color=alt.Color(\n    'Total_Killed:Q',\n    scale=alt.Scale(\n        domain=[0, df_grouped['Total_Killed'].max()],\n        range=['orange', 'red']\n    ),\n    title='Total Deaths'\n),\n\n    tooltip=[\n        alt.Tooltip('City:N'),\n        alt.Tooltip('Incident_Count:Q', title='Total Incidents'),\n        alt.Tooltip('Total_Killed:Q', title='Total Deaths'),\n        alt.Tooltip('Avg_Killed:Q', title='Avg Deaths/Incident')\n    ]\n)\n\n# Combine map + bubbles\nafghanistan_city_map = base_map + bubbles\nafghanistan_city_map\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Load the data\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Filter to Afghanistan\ndf_afg = df[df['Country'] == 'Afghanistan'].copy()\n\n# Clean and normalize the District column\ndf_afg['District'] = df_afg['District'].str.strip().str.title()\n\n# Drop missing or \"Unknown\" districts\ndf_afg = df_afg[df_afg['District'].notna()]\ndf_afg = df_afg[df_afg['District'].str.lower() != 'unknown']\n\n# Group by District and summarize\ndistrict_summary_afg = df_afg.groupby('District').agg(\n    Incidents=('Incident ID', 'count'),\n    Total_Deaths=('Total killed', 'sum'),\n    Avg_Deaths=('Total killed', 'mean')\n).reset_index().sort_values('Incidents', ascending=False)\n\n# Take Top 5\ntop5_districts_afg = district_summary_afg.head(5)\n\n# Plot as horizontal bar chart\nalt.Chart(top5_districts_afg).mark_bar().encode(\n    x=alt.X('Incidents:Q', title='Number of Incidents'),\n    y=alt.Y('District:N', sort='-x', title='District'),\n    tooltip=['District', 'Incidents', 'Total_Deaths', 'Avg_Deaths'],\n    color=alt.Color('District:N', legend=None)\n).properties(\n    title='Top 5 Districts in Afghanistan by Number of Security Incidents',\n    width=600,\n    height=300\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#perpetrators-in-afghanistan",
    "href": "projects/Scholarship/index.html#perpetrators-in-afghanistan",
    "title": "Risking Everything to Save Lives",
    "section": "Perpetrators in Afghanistan",
    "text": "Perpetrators in Afghanistan\nThe biggest perpetrators are attributed to National Non-state armed groups and other Non state armed groups. These include the Taliban, who are the most prominent non-state actor in Afghanistan, and responsible for numerous attacks on aid workers. Another example of a non-state groups include the Islamic State Khorasan Province, a branch of ISIS located and operating in Afghanistan since 2015\nU.S. Department of State 2022\n\n\nCode\n# Filter to Afghanistan and exclude unknown actor types\nactor_df_afg = df[\n    (df['Country'] == 'Afghanistan') &\n    (df['Actor type'].notna()) &\n    (df['Actor type'] != 'Unknown')\n]\n\n# Get top 5 actor types within Afghanistan\ntop_actors_afg = (\n    actor_df_afg['Actor type']\n    .value_counts()\n    .head(5)\n    .index.tolist()\n)\n\n# Filter to top 5 actor types\nactor_df_afg_top5 = actor_df_afg[actor_df_afg['Actor type'].isin(top_actors_afg)]\n\n# Group by year and actor type\nactor_yearly_afg = actor_df_afg_top5.groupby(['Year', 'Actor type']).size().reset_index(name='Count')\n\n# Line chart\nalt.Chart(actor_yearly_afg).mark_line(point=True).encode(\n    x='Year:O',\n    y='Count:Q',\n    color='Actor type:N',\n    tooltip=['Year', 'Actor type', 'Count']\n).properties(\n    title='Top 5 Actor Types in Afghanistan Over Time',\n    width=580,\n    height=400\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#how-deadly-are-attack-types-over-time-in-afghanistan",
    "href": "projects/Scholarship/index.html#how-deadly-are-attack-types-over-time-in-afghanistan",
    "title": "Risking Everything to Save Lives",
    "section": "How Deadly Are Attack Types Over Time in Afghanistan",
    "text": "How Deadly Are Attack Types Over Time in Afghanistan\nIn Afghanistan, shootings are the most commonly used method of attack against aid workers, followed by aerial bombardments, however after 2022 there has been a decrease in incidents leading to death. The previous pattern reflects the prevalence of small arms violence and ongoing conflict dynamics within the country.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n\n# Filter to Afghanistan\ndf_afg = df[df['Country'] == 'Afghanistan'].copy()\n\n# Clean and prep\ndf_afg['Year'] = pd.to_numeric(df_afg['Year'], errors='coerce')\ndf_afg['Total killed'] = pd.to_numeric(df_afg['Total killed'], errors='coerce')\ndf_afg = df_afg.dropna(subset=['Year', 'Total killed', 'Means of attack'])\ndf_afg['Year'] = df_afg['Year'].astype(int)\n\n# Get top 5 most deadly attack types\ntop_attacks = (\n    df_afg.groupby('Means of attack')['Total killed'].sum()\n    .sort_values(ascending=False)\n    .head(5)\n    .index\n)\n\n# Filter to top 5 attack types\ndf_afg_top = df_afg[df_afg['Means of attack'].isin(top_attacks)]\n\n# Group by year and attack type\ndeaths_by_type = df_afg_top.groupby(['Year', 'Means of attack'])['Total killed'].sum().reset_index()\n\n# Stacked bar chart\nalt.Chart(deaths_by_type).mark_bar().encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Total killed:Q', title='Total Deaths'),\n    color=alt.Color('Means of attack:N', title='Attack Type'),\n    tooltip=['Year', 'Means of attack', 'Total killed']\n).properties(\n    width=600,\n    height=400,\n    title='Total Deaths in Afghanistan by Attack Type (Top 5)'\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#comparing-frequency-and-lethality-of-attack-types-in-afghanistan",
    "href": "projects/Scholarship/index.html#comparing-frequency-and-lethality-of-attack-types-in-afghanistan",
    "title": "Risking Everything to Save Lives",
    "section": "Comparing Frequency and Lethality of Attack Types in Afghanistan",
    "text": "Comparing Frequency and Lethality of Attack Types in Afghanistan\nThe average deaths of shooting incidents is 1.14, with 154 total incidents and 175 total deaths. Aerial Bombardments had an average of 2.5, with 12 incidents and 30 deaths. Bodily Assault had the lowest lethality per attack, with an average death of 0.23 per incident.\n\n\nCode\n# Bubble chart: x = Total Incidents, y = Total Deaths, size = Avg Deaths/Incident\n\ndf = pd.read_csv(\"security_incidents.csv\")\ndf_afg = df[df['Country'] == 'Afghanistan'].copy()\n\n# Clean necessary columns\ndf_afg['Total killed'] = pd.to_numeric(df_afg['Total killed'], errors='coerce')\ndf_afg['Means of attack'] = df_afg['Means of attack'].fillna('Unknown').str.strip()\n\n# Drop missing or invalid values\ndf_afg = df_afg.dropna(subset=['Total killed', 'Means of attack'])\n\n# Group by Means of Attack\nattack_stats = df_afg.groupby('Means of attack').agg(\n    Total_Incidents=('Incident ID', 'count'),\n    Total_Deaths=('Total killed', 'sum'),\n    Avg_Deaths_Per_Incident=('Total killed', 'mean')\n).sort_values(by='Total_Deaths', ascending=False).reset_index()\n\n# Round for cleaner display\nattack_stats['Avg_Deaths_Per_Incident'] = attack_stats['Avg_Deaths_Per_Incident'].round(2)\n\nalt.Chart(attack_stats).mark_circle().encode(\n    x=alt.X('Total_Incidents:Q', title='Total Incidents'),\n    y=alt.Y('Total_Deaths:Q', title='Total Deaths'),\n    size=alt.Size('Avg_Deaths_Per_Incident:Q', title='Avg Deaths per Incident'),\n    color=alt.Color('Means of attack:N', title='Attack Type'),\n    tooltip=[\n        'Means of attack',\n        'Total_Incidents',\n        'Total_Deaths',\n        'Avg_Deaths_Per_Incident'\n    ]\n).properties(\n    title='How Deadly Are Different Attack Types in Afghanistan?',\n    width=600,\n    height=400\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#fatalities-by-attack-context-in-afghanistan",
    "href": "projects/Scholarship/index.html#fatalities-by-attack-context-in-afghanistan",
    "title": "Risking Everything to Save Lives",
    "section": "Fatalities by Attack Context in Afghanistan",
    "text": "Fatalities by Attack Context in Afghanistan\nThroughout the 2000s, Ambush attacks were the most common, however starting in 2014, Combat/Crossfire incidents become more prevalent. Starting in 2021, Individual Attacks and Raids have taken the lead, likely due to shifting power structures, increased targeted violence, and withdrawal of international forces.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Filter to Afghanistan\ndf_afg = df[df['Country'] == 'Afghanistan'].copy()\n\n# Clean and prep\ndf_afg['Year'] = pd.to_numeric(df_afg['Year'], errors='coerce')\ndf_afg['Total killed'] = pd.to_numeric(df_afg['Total killed'], errors='coerce')\ndf_afg['Attack context'] = df_afg['Attack context'].fillna('Unknown').str.strip()\ndf_afg = df_afg.dropna(subset=['Year', 'Total killed', 'Attack context'])\ndf_afg['Year'] = df_afg['Year'].astype(int)\n\n# Get top 5 most deadly attack contexts\ntop_contexts = (\n    df_afg.groupby('Attack context')['Total killed'].sum()\n    .sort_values(ascending=False)\n    .head(5)\n    .index\n)\n\n# Filter to top 5 contexts\ndf_afg_top = df_afg[df_afg['Attack context'].isin(top_contexts)]\n\n# Group by year and attack context\ndeaths_by_context = df_afg_top.groupby(['Year', 'Attack context'])['Total killed'].sum().reset_index()\n\n# Stacked bar chart\nalt.Chart(deaths_by_context).mark_bar().encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Total killed:Q', title='Total Deaths'),\n    color=alt.Color('Attack context:N', title='Attack Context'),\n    tooltip=['Year', 'Attack context', 'Total killed']\n).properties(\n    width=600,\n    height=400,\n    title='Total Deaths in Afghanistan by Attack Context (Top 5)'\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#disputed-territories",
    "href": "projects/Scholarship/index.html#disputed-territories",
    "title": "Risking Everything to Save Lives",
    "section": "Disputed Territories",
    "text": "Disputed Territories\nPalestine and Gaza have a complex and long history. From 1948-1967, Gaza was in Egypt’s rule, however after the Six Day war in 1967, Israel seized control of Gaza, the West Bank and East Jerusalem. This marked a turning point, displacing many Palestinians and intensifying the struggle over occupied land.\nThe first Palestinian Uprising took place in 1987, known as the first Intifada. For 5 years, protests and violence sprung up in Palestine. It was at this time, Hamas, a militant group based out of Gaza was formed. In 1993, peace talks were attempted, however fell short due to uprisings and continued violence.\nIn 2005, Israel withdrew all troops and settlers in Palestine, with Hamas winning elections. In 2007, Hamas violently seized full control of Gaza from Fatah(the ruling faction of Palestine). This prompted Israel and Egypt to impose a strict blockade on the territory, citing security concern Overtime, Hamas became more and more powerful, amassing more weapons and rockets\nIn October 2023, Hamas planned a surprise attack on Israel, killing 1,200 people and took 250 people hostage. This has sparked an huge military response, which has resulted in tens of thousands of Palestinians deaths, violence, and huge displacement in Gaza’s population.\nFrankel 2025",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#where-are-the-incidents-1",
    "href": "projects/Scholarship/index.html#where-are-the-incidents-1",
    "title": "Risking Everything to Save Lives",
    "section": "Where Are the Incidents?",
    "text": "Where Are the Incidents?\nSimilar to Afghanistan, areas of conflict include Gaza, Khan Younis, Nablus, and Rafah, where ongoing political tensions, dense urban populations, border proximity, and frequent military operations all contribute to higher risks and incidents involving aid workers.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Filter to Palestine\ndf_pal = df[df['Country'] == 'Occupied Palestinian Territories'].copy()\n\n# Clean and normalize the District column\ndf_pal['District'] = df_pal['District'].str.strip().str.title()\n\n# Drop missing or \"Unknown\" districts\ndf_pal = df_pal[df_pal['District'].notna()]\ndf_pal = df_pal[df_pal['District'].str.lower() != 'unknown']\n\n# Group by District and summarize\ndistrict_summary = df_pal.groupby('District').agg(\n    Incidents=('Incident ID', 'count'),\n    Total_Deaths=('Total killed', 'sum'),\n    Avg_Deaths=('Total killed', 'mean')\n).reset_index().sort_values('Incidents', ascending=False)\n\n# Take Top 5\ntop5_districts = district_summary.head(5)\n\n# Plot as horizontal bar chart (easier to read long names)\nalt.Chart(top5_districts).mark_bar().encode(\n    x=alt.X('Incidents:Q', title='Number of Incidents'),\n    y=alt.Y('District:N', sort='-x', title='District'),\n    tooltip=['District', 'Incidents', 'Total_Deaths', 'Avg_Deaths'],\n    color=alt.Color('District:N', legend=None)\n).properties(\n    title='Top 5 Districts in Palestine by Number of Security Incidents',\n    width=600,\n    height=300\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#perpetrators-in-palestine",
    "href": "projects/Scholarship/index.html#perpetrators-in-palestine",
    "title": "Risking Everything to Save Lives",
    "section": "Perpetrators in Palestine",
    "text": "Perpetrators in Palestine\nStarting in 2002, the majority of Perpetrators in Palestine are the host state itself, however in 2024 and 2025 there’s been a rise in incidents involving foreign or coalition forces. The host state can be attributed to the workings of Hamas, while attacks by foreign states can be attributed to the workings of Israel and Iran backed Hezbollah.\n\n\nCode\nimport altair as alt\n\n# Filter to Palestine and exclude unknown actor types\nactor_df_pal = df[\n    (df['Country'] == 'Occupied Palestinian Territories') &\n    (df['Actor type'].notna()) &\n    (df['Actor type'] != 'Unknown')\n]\n\n#top 5 actor types within Palestine\ntop_actors_pal = (\n    actor_df_pal['Actor type']\n    .value_counts()\n    .head(5)\n    .index.tolist()\n)\n\n# Filter to top 5 actor types\nactor_df_pal_top5 = actor_df_pal[actor_df_pal['Actor type'].isin(top_actors_pal)]\n\n# Group by year and actor type\nactor_yearly_pal = actor_df_pal_top5.groupby(['Year', 'Actor type']).size().reset_index(name='Count')\n\n# Stacked bar chart\nalt.Chart(actor_yearly_pal).mark_bar().encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Count:Q', title='Number of Incidents'),\n    color=alt.Color('Actor type:N', title='Actor Type'),\n    tooltip=['Year', 'Actor type', 'Count']\n).properties(\n    title='Top 5 Actor Types in Palestine Over Time',\n    width=600,\n    height=400\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#how-deadly-are-attack-types-over-time-in-palestine",
    "href": "projects/Scholarship/index.html#how-deadly-are-attack-types-over-time-in-palestine",
    "title": "Risking Everything to Save Lives",
    "section": "How Deadly Are Attack Types Over Time in Palestine",
    "text": "How Deadly Are Attack Types Over Time in Palestine\nThroughout the 2000s and 2010s, the main types of attacks conducted involved shootings, with occasional Aeriel Bombardments (only attributing to 11 deaths), however starting in 2023, Aeriel Bombardment has seen a massive increase in incidents, accounting for 294 of the 381 deaths (77%).\n\n\nCode\n# Filter to Palestine\ndf_pal = df[df['Country'] == 'Occupied Palestinian Territories'].copy()\n\n# Clean and prep\ndf_pal['Year'] = pd.to_numeric(df_pal['Year'], errors='coerce')\ndf_pal['Total killed'] = pd.to_numeric(df_pal['Total killed'], errors='coerce')\ndf_pal = df_pal.dropna(subset=['Year', 'Total killed', 'Means of attack'])\ndf_pal['Year'] = df_pal['Year'].astype(int)\n\n# Get top 5 most deadly attack types\ntop_attacks_pal = (\n    df_pal.groupby('Means of attack')['Total killed'].sum()\n    .sort_values(ascending=False)\n    .head(5)\n    .index\n)\n\n# Filter to top 5 attack types\ndf_pal_top = df_pal[df_pal['Means of attack'].isin(top_attacks_pal)]\n\n# Group by year and attack type\ndeaths_by_type_pal = df_pal_top.groupby(['Year', 'Means of attack'])['Total killed'].sum().reset_index()\n\n# Stacked bar chart\nalt.Chart(deaths_by_type_pal).mark_bar().encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Total killed:Q', title='Total Deaths'),\n    color=alt.Color('Means of attack:N', title='Attack Type'),\n    tooltip=['Year', 'Means of attack', 'Total killed']\n).properties(\n    width=600,\n    height=400,\n    title='Total Deaths in Palestine by Attack Type (Top 5)'\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#comparing-frequency-and-lethality-of-attack-types-in-palestine",
    "href": "projects/Scholarship/index.html#comparing-frequency-and-lethality-of-attack-types-in-palestine",
    "title": "Risking Everything to Save Lives",
    "section": "Comparing Frequency and Lethality of Attack Types in Palestine",
    "text": "Comparing Frequency and Lethality of Attack Types in Palestine\nThe average deaths of Aeriel Bombardment is 4.55, with 67 total incidents and 305 total deaths. Shootings had an average of 0.78, with 41 incidents and 32 deaths. With an average fatality of 4.55 per incident, this is the deadliest attack type we have seen in our analysis.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Load and filter data to Palestine\ndf = pd.read_csv(\"security_incidents.csv\")\ndf_pal = df[df['Country'] == 'Occupied Palestinian Territories'].copy()\n\n# Clean necessary columns\ndf_pal['Total killed'] = pd.to_numeric(df_pal['Total killed'], errors='coerce')\ndf_pal['Means of attack'] = df_pal['Means of attack'].fillna('Unknown').str.strip()\n\n# Drop missing or invalid values\ndf_pal = df_pal.dropna(subset=['Total killed', 'Means of attack'])\n\n# Group by Means of Attack\nattack_stats_pal = df_pal.groupby('Means of attack').agg(\n    Total_Incidents=('Incident ID', 'count'),\n    Total_Deaths=('Total killed', 'sum'),\n    Avg_Deaths_Per_Incident=('Total killed', 'mean')\n).sort_values(by='Total_Deaths', ascending=False).reset_index()\n\n\nattack_stats_pal['Avg_Deaths_Per_Incident'] = attack_stats_pal['Avg_Deaths_Per_Incident'].round(2)\n\n# Bubble chart\nalt.Chart(attack_stats_pal).mark_circle().encode(\n    x=alt.X('Total_Incidents:Q', title='Total Incidents'),\n    y=alt.Y('Total_Deaths:Q', title='Total Deaths'),\n    size=alt.Size('Avg_Deaths_Per_Incident:Q', title='Avg Deaths per Incident'),\n    color=alt.Color('Means of attack:N', title='Attack Type'),\n    tooltip=[\n        'Means of attack',\n        'Total_Incidents',\n        'Total_Deaths',\n        'Avg_Deaths_Per_Incident'\n    ]\n).properties(\n    title='How Deadly Are Different Attack Types in Palestine?',\n    width=560,\n    height=450\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#fatalities-by-attack-context-in-palestine",
    "href": "projects/Scholarship/index.html#fatalities-by-attack-context-in-palestine",
    "title": "Risking Everything to Save Lives",
    "section": "Fatalities by Attack Context in Palestine",
    "text": "Fatalities by Attack Context in Palestine\nCombat and individual attacks dominate the attack context in Palestine, likely due to the intensity of urban conflict, military operations, and acts of violence in densely populated areas like Gaza.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Filter to Afghanistan\ndf_afg = df[df['Country'] == 'Occupied Palestinian Territories'].copy()\n\n# Clean and prep\ndf_afg['Year'] = pd.to_numeric(df_afg['Year'], errors='coerce')\ndf_afg['Total killed'] = pd.to_numeric(df_afg['Total killed'], errors='coerce')\ndf_afg['Attack context'] = df_afg['Attack context'].fillna('Unknown').str.strip()\ndf_afg = df_afg.dropna(subset=['Year', 'Total killed', 'Attack context'])\ndf_afg['Year'] = df_afg['Year'].astype(int)\n\n# Get top 5 most deadly attack contexts\ntop_contexts = (\n    df_afg.groupby('Attack context')['Total killed'].sum()\n    .sort_values(ascending=False)\n    .head(5)\n    .index\n)\n\n# Filter to top 5 contexts\ndf_afg_top = df_afg[df_afg['Attack context'].isin(top_contexts)]\n\n# Group by year and attack context\ndeaths_by_context = df_afg_top.groupby(['Year', 'Attack context'])['Total killed'].sum().reset_index()\n\n# Stacked bar chart\nalt.Chart(deaths_by_context).mark_bar().encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Total killed:Q', title='Total Deaths'),\n    color=alt.Color('Attack context:N', title='Attack Context'),\n    tooltip=['Year', 'Attack context', 'Total killed']\n).properties(\n    width=600,\n    height=400,\n    title='Total Deaths in Palestine by Attack Context (Top 5)'\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#civil-war-fallout",
    "href": "projects/Scholarship/index.html#civil-war-fallout",
    "title": "Risking Everything to Save Lives",
    "section": "Civil War Fallout",
    "text": "Civil War Fallout\nAfter decades of civil war and oppression under Sudanese rule, South Sudan emerged as the world’s newest nation. Starting in 2011, South Sudan finally achieved independence, however just 2 years later in 2013, President Kiir fired his vice president Riek Machar, accusing him of starting a coup. This started a civil war, splitting the country along ethnic lines, causing widespread destruction and displacing more than 2 million people.\nIn 2015, a peace deal was reached, however, violence quickly resumed in 2016 when Machar returned as Vice President and was forced to flee. In 2018 a second peace deal was reached, however slow implementation led to more than 7 million people facing hunger and food shortages by 2019, with this being where Sudan is today. Today, South Sudan remains conflicted by political instability, continuing to face violence and instability with constant need for humanitarian aid.\nConcern Worldwide 2022",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#where-are-the-incidents-2",
    "href": "projects/Scholarship/index.html#where-are-the-incidents-2",
    "title": "Risking Everything to Save Lives",
    "section": "Where are the Incidents?",
    "text": "Where are the Incidents?\nMajor conflict areas in South Sudan include cities such as Juba, Rubkona, Pibor, Torit, and Maba. Aid Workers in these cities face higher risks due to political instability, ongoing ethnic and communal tensions, proximity to military activity, all of which contribute to frequent attacks on aid workers.\n\n\nCode\nimport pandas as pd\nimport altair as alt\nimport json\n\n# Load the data\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Filter to South Sudan and drop missing coordinates\ndf_ss = df[df['Country'] == 'South Sudan'].dropna(subset=['Latitude', 'Longitude', 'City'])\n\n# Clean columns\ndf_ss['Latitude'] = pd.to_numeric(df_ss['Latitude'], errors='coerce')\ndf_ss['Longitude'] = pd.to_numeric(df_ss['Longitude'], errors='coerce')\ndf_ss['Total killed'] = pd.to_numeric(df_ss['Total killed'], errors='coerce')\ndf_ss = df_ss.dropna(subset=['Latitude', 'Longitude', 'City'])\n\n# Group by City\ndf_grouped_ss = df_ss.groupby('City').agg(\n    Latitude=('Latitude', 'mean'),\n    Longitude=('Longitude', 'mean'),\n    Incident_Count=('Incident ID', 'count'),\n    Total_Killed=('Total killed', 'sum'),\n    Avg_Killed=('Total killed', 'mean')\n).reset_index()\n\n# Load South Sudan GeoJSON\nwith open(\"southsudan.geo.json\", \"r\") as f:\n    geojson_ss = json.load(f)\n\n# Base map outline for South Sudan\nbase_map_ss = alt.Chart(alt.Data(values=geojson_ss['features'])).mark_geoshape(\n    fill='lightgray',\n    stroke='black'\n).project(\n    type='mercator',\n    center=[26.31, 7.6],  # Approximate center of South Sudan\n    scale=3700\n).properties(\n    width=1000,\n    height=700,\n    title='City-Level Hotspots of Attacks on Aid Workers in South Sudan (1997–2025)'\n)\n\n\n# City-level bubbles\nbubbles_ss = alt.Chart(df_grouped_ss).mark_circle(opacity=0.9).encode(\n    longitude='Longitude:Q',\n    latitude='Latitude:Q',\n    size=alt.Size('Incident_Count:Q', scale=alt.Scale(range=[30, 300]), title='Incidents'),\n    color=alt.Color(\n        'Total_Killed:Q',\n        scale=alt.Scale(\n            domain=[0, df_grouped_ss['Total_Killed'].max()],\n            range=['orange', 'red']\n        ),\n        title='Total Deaths'\n    ),\n    tooltip=[\n        alt.Tooltip('City:N'),\n        alt.Tooltip('Incident_Count:Q', title='Total Incidents'),\n        alt.Tooltip('Total_Killed:Q', title='Total Deaths'),\n        alt.Tooltip('Avg_Killed:Q', title='Avg Deaths/Incident')\n    ]\n)\n\n# Combine placeholder base + bubbles\nsouthsudan_city_map = base_map_ss + bubbles_ss\nsouthsudan_city_map",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#top-actor-types-in-south-sudan-over-time",
    "href": "projects/Scholarship/index.html#top-actor-types-in-south-sudan-over-time",
    "title": "Risking Everything to Save Lives",
    "section": "Top Actor Types in South Sudan Over Time",
    "text": "Top Actor Types in South Sudan Over Time\nThroughout the 2010s, unaffiliated groups and unknown armed groups have been the main culprit of incidents, however starting in 2021, Subnational Non-state groups have been the main group that has. This is likely due to post civil war instability and power struggles.\n\n\nCode\n# Filter to South Sudan and exclude unknown actor types\nactor_df_ss = df[\n    (df['Country'] == 'South Sudan') &\n    (df['Actor type'].notna()) &\n    (df['Actor type'] != 'Unknown')\n]\n\n# Top 5 actor types in South Sudan\ntop_actors_ss = (\n    actor_df_ss['Actor type']\n    .value_counts()\n    .head(5)\n    .index.tolist()\n)\n\n# Filter to top 5 actor types\nactor_df_ss_top5 = actor_df_ss[actor_df_ss['Actor type'].isin(top_actors_ss)]\n\n# Group by year and actor type\nactor_yearly_ss = actor_df_ss_top5.groupby(['Year', 'Actor type']).size().reset_index(name='Count')\n\n# Line chart\nalt.Chart(actor_yearly_ss).mark_line(point=True).encode(\n    x='Year:O',\n    y='Count:Q',\n    color='Actor type:N',\n    tooltip=['Year', 'Actor type', 'Count']\n).properties(\n    title='Top 5 Actor Types in South Sudan Over Time',\n    width=580,\n    height=400\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#how-deadly-are-attack-types-over-time-in-south-sudan",
    "href": "projects/Scholarship/index.html#how-deadly-are-attack-types-over-time-in-south-sudan",
    "title": "Risking Everything to Save Lives",
    "section": "How Deadly Are Attack Types Over Time in South Sudan",
    "text": "How Deadly Are Attack Types Over Time in South Sudan\nIn South Sudan, shootings are the most commonly used method of attack against aid workers, followed by Bodily Assults, however following 2023 there has been a decrease in incidents leading to death.\n\n\nCode\n# Filter to South Sudan\ndf_ss = df[df['Country'] == 'South Sudan'].copy()\n\n# Clean and prep\ndf_ss['Year'] = pd.to_numeric(df_ss['Year'], errors='coerce')\ndf_ss['Total killed'] = pd.to_numeric(df_ss['Total killed'], errors='coerce')\ndf_ss = df_ss.dropna(subset=['Year', 'Total killed', 'Means of attack'])\ndf_ss['Year'] = df_ss['Year'].astype(int)\n\n# Top 5 most deadly attack types\ntop_attacks_ss = (\n    df_ss.groupby('Means of attack')['Total killed'].sum()\n    .sort_values(ascending=False)\n    .head(5)\n    .index\n)\n\n# Filter to top 5 attack types\ndf_ss_top = df_ss[df_ss['Means of attack'].isin(top_attacks_ss)]\n\n# Group by year and attack type\ndeaths_by_type_ss = df_ss_top.groupby(['Year', 'Means of attack'])['Total killed'].sum().reset_index()\n\n# Stacked bar chart\nalt.Chart(deaths_by_type_ss).mark_bar().encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Total killed:Q', title='Total Deaths'),\n    color=alt.Color('Means of attack:N', title='Attack Type'),\n    tooltip=['Year', 'Means of attack', 'Total killed']\n).properties(\n    width=600,\n    height=400,\n    title='Total Deaths in South Sudan by Attack Type (Top 5)'\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#comparing-frequency-and-lethality-of-attack-types-in-south-sudan",
    "href": "projects/Scholarship/index.html#comparing-frequency-and-lethality-of-attack-types-in-south-sudan",
    "title": "Risking Everything to Save Lives",
    "section": "Comparing Frequency and Lethality of Attack Types in South Sudan",
    "text": "Comparing Frequency and Lethality of Attack Types in South Sudan\nThe average deaths of Shootings incidents is 0.69 with 248 total incidents and 171 total deaths. Bodily Assaults had an average fatality of 0.1 per incident, with 219 incidents and 22 deaths. Kidnap Killings had the highest fatality rate, with 2.29, however only had 7 total incidents and 9 total deaths.\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Load the data\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Filter to South Sudan\ndf_ss = df[df['Country'] == 'South Sudan'].copy()\n\n# Clean relevant columns\ndf_ss['Total killed'] = pd.to_numeric(df_ss['Total killed'], errors='coerce')\ndf_ss['Means of attack'] = df_ss['Means of attack'].fillna('Unknown').str.strip()\n\n# Drop missing values\ndf_ss = df_ss.dropna(subset=['Total killed', 'Means of attack'])\n\n# Group by Means of Attack\nattack_stats_ss = df_ss.groupby('Means of attack').agg(\n    Total_Incidents=('Incident ID', 'count'),\n    Total_Deaths=('Total killed', 'sum'),\n    Avg_Deaths_Per_Incident=('Total killed', 'mean')\n).sort_values(by='Total_Deaths', ascending=False).reset_index()\n\n\nattack_stats_ss['Avg_Deaths_Per_Incident'] = attack_stats_ss['Avg_Deaths_Per_Incident'].round(2)\n\n# Bubble chart\nalt.Chart(attack_stats_ss).mark_circle().encode(\n    x=alt.X('Total_Incidents:Q', title='Total Incidents'),\n    y=alt.Y('Total_Deaths:Q', title='Total Deaths'),\n    size=alt.Size('Avg_Deaths_Per_Incident:Q', title='Avg Deaths per Incident'),\n    color=alt.Color('Means of attack:N', title='Attack Type'),\n    tooltip=[\n        'Means of attack',\n        'Total_Incidents',\n        'Total_Deaths',\n        'Avg_Deaths_Per_Incident'\n    ]\n).properties(\n    title='Lethality of Attack Types in South Sudan',\n    width=600,\n    height=450\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#fatalities-by-attack-context-in-south-sudan",
    "href": "projects/Scholarship/index.html#fatalities-by-attack-context-in-south-sudan",
    "title": "Risking Everything to Save Lives",
    "section": "Fatalities by Attack Context in South Sudan",
    "text": "Fatalities by Attack Context in South Sudan\nAmbush attacks are the most prevalent in South Sudan, with combat and crossfire incidents following closely. South Sudan is also the only country analyzed where raids were used throughout. This pattern likely reflects shifting power dynamics, ongoing intercommunal violence, and limited state control\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Filter to Afghanistan\ndf_afg = df[df['Country'] == 'South Sudan'].copy()\n\n# Clean and prep\ndf_afg['Year'] = pd.to_numeric(df_afg['Year'], errors='coerce')\ndf_afg['Total killed'] = pd.to_numeric(df_afg['Total killed'], errors='coerce')\ndf_afg['Attack context'] = df_afg['Attack context'].fillna('Unknown').str.strip()\ndf_afg = df_afg.dropna(subset=['Year', 'Total killed', 'Attack context'])\ndf_afg['Year'] = df_afg['Year'].astype(int)\n\n# Get top 5 most deadly attack contexts\ntop_contexts = (\n    df_afg.groupby('Attack context')['Total killed'].sum()\n    .sort_values(ascending=False)\n    .head(5)\n    .index\n)\n\n# Filter to top 5 contexts\ndf_afg_top = df_afg[df_afg['Attack context'].isin(top_contexts)]\n\n# Group by year and attack context\ndeaths_by_context = df_afg_top.groupby(['Year', 'Attack context'])['Total killed'].sum().reset_index()\n\n# Stacked bar chart\nalt.Chart(deaths_by_context).mark_bar().encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Total killed:Q', title='Total Deaths'),\n    color=alt.Color('Attack context:N', title='Attack Context'),\n    tooltip=['Year', 'Attack context', 'Total killed']\n).properties(\n    width=600,\n    height=400,\n    title='Total Deaths in South Sudan by Attack Context (Top 5)'\n)",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#afghanistan",
    "href": "projects/Scholarship/index.html#afghanistan",
    "title": "Risking Everything to Save Lives",
    "section": "Afghanistan",
    "text": "Afghanistan\n\nConflict: Terrorism and Counter-Insurgency\nHad the highest amount of violence toward aid workers, totalling 610 incidents since 1997, with an average of 21 per year.\nDriven by terrorism and counter-insurgency groups including the Taliban and IS-K.\nViolence is widespread and frequent, with shootings being the most widespread form.",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#palestine",
    "href": "projects/Scholarship/index.html#palestine",
    "title": "Risking Everything to Save Lives",
    "section": "Palestine",
    "text": "Palestine\n\nConflict: Occupation Conflict\nWith recent political unrest, Palestine has seen a sharp uptick in attacks starting in 2023, driven largely by the rise in aerial bombardments. These attacks average 4.55 deaths per attack, making them the deadliest in the observed countries\nIncidents are less frequent, however more deadly, with an average of 2.7 deaths per incident—the highest among all countries studied.\nThe host state (Hamas), and in later years, foreign or coalition forces are the primary perpetrators.",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#south-sudan",
    "href": "projects/Scholarship/index.html#south-sudan",
    "title": "Risking Everything to Save Lives",
    "section": "South Sudan",
    "text": "South Sudan\n\nConflict: Civil War Fallout & Ethnic Violence\nReflects post-civil war instability.\nShootings most common attacks, however kidnap-killings are highly lethal.\nAid workers are frequently caught in the crossfire.",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#afghanistan-1",
    "href": "projects/Scholarship/index.html#afghanistan-1",
    "title": "Risking Everything to Save Lives",
    "section": "Afghanistan",
    "text": "Afghanistan\n\nTerrorist insurgencies and decades-long conflict.\nMajor actors include Taliban, ISIS-K, and other extremist groups.",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#palestine-1",
    "href": "projects/Scholarship/index.html#palestine-1",
    "title": "Risking Everything to Save Lives",
    "section": "Palestine",
    "text": "Palestine\n\nLongstanding territorial and occupation conflicts.\nEscalation in 2023 with major operations in Gaza and surrounding areas.",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/Scholarship/index.html#south-sudan-1",
    "href": "projects/Scholarship/index.html#south-sudan-1",
    "title": "Risking Everything to Save Lives",
    "section": "South Sudan",
    "text": "South Sudan\n\nPower vacuums and interethnic violence.\nWeak government institutions allow armed groups to flourish.",
    "crumbs": [
      "About",
      "Overview"
    ]
  },
  {
    "objectID": "projects/DS5100/index.html",
    "href": "projects/DS5100/index.html",
    "title": "DSAN 5100",
    "section": "",
    "text": "Author: Jessica Joy, Adam Stein, Soong-Ping Hill, Hung Tran, David Corcoran\nCourse: DSAN 5100: Probabilistic Modeling and Statistical Computing\n\n\nUnderstanding the relationship between neighborhood safety, listing features, and pricing models is crucial for both Airbnb hosts and potential guests. This analysis examines Airbnb listings alongside city crime data, leveraging statistical testing and geospatial analysis to uncover patterns and insights that influence rental pricing and desirability."
  },
  {
    "objectID": "projects/DS5100/index.html#introduction",
    "href": "projects/DS5100/index.html#introduction",
    "title": "DSAN 5100",
    "section": "",
    "text": "Understanding the relationship between neighborhood safety, listing features, and pricing models is crucial for both Airbnb hosts and potential guests. This analysis examines Airbnb listings alongside city crime data, leveraging statistical testing and geospatial analysis to uncover patterns and insights that influence rental pricing and desirability."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Soong Ping Hill. I am from San Francisco, California, and have a Bachelors Degree in Neuroscience and Minor in Data Science. I am currently pursuing a M.S degree in Data Science and Analytics at Georgetown University’s DSAN program.\nThis program has allowed me to develop a strong foundation in data-driven methodologies, machine learning, and advanced analytics while honing my skills in extracting meaningful insights from large datasets to solve real-world problems.\nI am passionate about leveraging data science to uncover patterns, drive decision-making, and contribute to impactful solutions across industries."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nGeorgetown University | Washington, D.C.\n- Master’s in Data Science (DSAN)\n- GPA: 3.95\nBoston University College of Arts and Science | Boston, MA\n- Bachelor of Arts in Neuroscience, Minor in Data Science\n- GPA: 3.57"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "About Me",
    "section": "Work Experience",
    "text": "Work Experience\nCenter for Retirement Initiatives | Washington, D.C.\nGraduate Student Research Assistant (Georgetown University)\n\nAnalyzed state program data, creating monthly reports on financial metrics and retirement trends in Tableau.\n\nConducted data collection, cleaning, and dataset uploads using SQL and Python, ensuring data integrity.\n\nCollaborated with CRI stakeholders to identify new research inquiries and provide evidence-based insights.\n\nRESET.BUILD | Shanghai, China | June 2024 – August 2024\nData Science Intern (BU Shanghai Internship Program)\n\nDeveloped Python scripts to automate big data cleaning, extraction, format corrections, and air quality analysis, saving 8 hours of manual work per dataset.\n\nCollaborated with cross-functional teams to design and implement data dashboards in Google Looker Studio, improving data access and driving internal insights.\n\nEnhanced front-end UI using HTML and CSS, ensuring a professional and user-friendly experience.\n\nEcommerce Store | San Francisco, CA | June 2023 – Present\nOwner and Manager\n\nManaged e-commerce store with $9,000 in revenue, overseeing inventory, customer service, SEO, and marketing strategies.\n\nApplied business insights to adjust pricing strategies and product offerings, enhancing store profitability.\n\nRadboud University | Nijmegen, Netherlands | May 2023 – July 2023\nMachine Learning Research Intern\n\nProgrammed pipeline to cut, synchronize, and analyze over 2 TB of neuronal data and video.\n\nUtilized DeepLabCut pose-estimation to analyze cortico-hippocampal activity during memory consolidation.\n\nPartnered with a diverse research team and mentored new interns, creating a written guide for onboarding.\n\nUCSF Medical Center – Anesthesia | San Francisco, CA | May 2022 – August 2022\nData Research Intern\n\nContributed to a research study on pupillary unrest in ambient light in response to opioids.\n\nCleaned and visualized data for analysis, designing figures and reports.\n\nPublication: Merlin D. Larsson, Rachel McKay, Soong Ping Hill, Practical Guide to Portable…"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "About Me",
    "section": "Projects",
    "text": "Projects\n\nVoting Rate Analysis and Predictive Modeling\n\nIntegrated U.S Census Bureau API to collect and process demographic and socioeconomic data related to voting rates\nApplied clustering techniques (K-Means, DBSCAN, Agglomerative Clustering) to segment voters by behavior\nBuilt and evaluated predictive models (Random Forest, Regression) to analyze key factors influencing voting rates and predict voter participation trends\n\n\n\nStatistical Analysis of Airbnb Listing Prices Across Cities\n\nConducted statistical analysis (t-tests, ANOVAs, correlation, chi-squared) to evaluate how location, host status, and other factors impact Airbnb pricing\nVisualized data patterns with ggplot and presented actionable insights to optimize pricing strategies for hosts\n\n\n\nCovid-19 Data Analysis and Policy Identification\n\nIntegrated COVID-19 data from three sources into data lake via Azure Data Factory, prepared for processing in Azure Synapse, and analyzed in Power BI to identify the top three policies for controlling COVID spread\n\n\n\nAnalysis of U.S Airline Operational Performance\n\nQueried Bureau of Transportation Statistics flight data using SQL to analyze delays and cancellations across airlines and airports\nCreated an R-based report with visualizations to assess operational efficiency and highlight opportunities for improving flight scheduling and customer experience\nIdentified key delay and cancellation factors, providing data-driven insights to enhance airline performance\n\n\n\nNBA Player Performance Model\n\nBuilt and optimized an LSTM deep learning model to forecast NBA player performance based on historical game data\nModel explained 68% of variation in player performance, improving predictive power over traditional methods\nProvided data-driven insights for player evaluation and strategy optimization in sports management"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nProgramming & Data Science\nProficient: Python (pandas, numpy, scikit-learn, matplotlib, seaborn), R (ggplot2, dplyr, tidyverse), SQL, PyTorch, TensorFlow\nFamiliar: Java, C++, MATLAB, Swift, JavaScript, HTML, CSS\n\n\nMachine Learning & Analytics\nProficient: Supervised Learning (Random Forest, Regression, SVM), Unsupervised Learning (K-Means, DBSCAN, Hierarchical Clustering), Deep Learning (LSTM, Neural Networks), Feature Selection, Cross-Validation, Statistical Analysis\nFamiliar: NLP, Computer Vision, Time Series Analysis, A/B Testing, Bayesian Methods\n\n\nData Engineering & Cloud\nProficient: AWS (S3, EC2, Lambda), Microsoft Azure (Data Factory, Synapse), SQL Server, PostgreSQL, Git/GitHub, Hadoop, Snowflake\nFamiliar: Google Cloud Platform, Apache Spark, Docker, Kubernetes, DBT, Apache Airflow\n\n\nVisualization & BI Tools\nProficient: Tableau, Looker Studio, Power BI, Matplotlib, Seaborn, ggplot2, Plotly\nFamiliar: D3.js, Bokeh, ArcGIS,\n\n\nDevelopment & Collaboration\nProficient: Git/GitHub, Jupyter, RStudio, VS Code, Excel, PowerPoint, Visio\nFamiliar: Linux/Unix, Bash scripting\nSpoken Languages: English (Native), Mandarin (Fluent), Cantonese (Conversational)\nSpoken Languages: English, Mandarin, Cantonese\nDownload Resume (PDF)"
  },
  {
    "objectID": "projects/Scholarship/supervised.html",
    "href": "projects/Scholarship/supervised.html",
    "title": "Predicting Attack Severity: Low, Medium, High",
    "section": "",
    "text": "We classify each incident into one of three severity levels — Low, Medium, or High — based on relevant features using a multiclass classification model.\n#| code-fold: true\n\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Create 'Total Affected' and 'Severity Level'\ndf['Total Affected'] = df[['Total killed', 'Total wounded', 'Total kidnapped']].sum(axis=1).fillna(0)\n\ndef categorize_severity(x):\n    if x &lt;= 1:\n        return 'Low'\n    elif x &lt;= 4:\n        return 'Medium'\n    else:\n        return 'High'\n\ndf['Severity Level'] = df['Total Affected'].apply(categorize_severity)\n\n\n#| code-fold: true\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Select relevant features (numeric + categorical)\nselected_features = [\n    'Year', 'Month', 'Country Code', 'Region', 'District', 'City',\n    'Means of attack', 'Attack context', 'Actor type', 'Motive',\n    'Latitude', 'Longitude'\n]\n\n# Check which columns exist in the dataset\nexisting_features = [col for col in selected_features if col in df.columns]\nmissing_features = [col for col in selected_features if col not in df.columns]\n\nif missing_features:\n    print(f\"Warning: Missing columns: {missing_features}\")\n    selected_features = existing_features\n\nX = df[selected_features].copy().fillna('Unknown')\ny = df['Severity Level']\n\n# Encode categorical features\ncategorical_cols = [\n    'Country Code', 'Region', 'District', 'City',\n    'Means of attack', 'Attack context'\n]\n\n# Copy data\nX_encoded = X.copy()\n\n# List of all non-numeric columns (categorical)\ncategorical_cols = X_encoded.select_dtypes(include='object').columns.tolist()\n\n# Encode all categorical features using LabelEncoder\nlabel_encoders = {}\nfor col in categorical_cols:\n    le = LabelEncoder()\n    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n    label_encoders[col] = le\n\n# Encode target\nle_target = LabelEncoder()\ny_encoded = le_target.fit_transform(y)\n\n\n\n\nFeature selection using Sequential Forward Floating Selection (SFFS) highlighted the most predictive variables: - Year - Country Code - Region - Means of Attack - Actor Type - Motive - Longitude\nThese features capture both the context and timing of incidents, aiding in distinguishing between severity levels.\n#| code-fold: true\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_encoded, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n)\n\n# Set up model + cross-validation\nclf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Sequential Floating Forward Selection\nsffs = SFS(clf,\n           k_features='best',\n           forward=True,\n           floating=True,\n           scoring='f1_weighted',\n           cv=cv,\n           n_jobs=-1)\n\nsffs.fit(X_train, y_train)\n\nselected_feature_indices = list(sffs.k_feature_idx_)\nselected_feature_names = [X_encoded.columns[i] for i in selected_feature_indices]\nprint(\"Selected features:\", selected_feature_names)\n\n\n\nThe model achieved a moderate overall accuracy of 65.8%, with stronger performance on low-severity incidents but room for improvement in detecting more severe attacks.\n#| code-fold: true\n\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\n# Train model using ONLY selected features\n# Final classifier with class weight handling\nclf_final = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n\nclf_final.fit(X_train.iloc[:, selected_feature_indices], y_train)\n\n# Predict on test set\ny_pred = clf_final.predict(X_test.iloc[:, selected_feature_indices])\n\n# Evaluation\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=le_target.classes_))\n\n# Confusion Matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=le_target.classes_)\ndisp.plot(cmap=\"Reds\")",
    "crumbs": [
      "About",
      "Supervised"
    ]
  },
  {
    "objectID": "projects/Scholarship/supervised.html#multiclass-classification-overview",
    "href": "projects/Scholarship/supervised.html#multiclass-classification-overview",
    "title": "Predicting Attack Severity: Low, Medium, High",
    "section": "",
    "text": "We classify each incident into one of three severity levels — Low, Medium, or High — based on relevant features using a multiclass classification model.\n#| code-fold: true\n\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Create 'Total Affected' and 'Severity Level'\ndf['Total Affected'] = df[['Total killed', 'Total wounded', 'Total kidnapped']].sum(axis=1).fillna(0)\n\ndef categorize_severity(x):\n    if x &lt;= 1:\n        return 'Low'\n    elif x &lt;= 4:\n        return 'Medium'\n    else:\n        return 'High'\n\ndf['Severity Level'] = df['Total Affected'].apply(categorize_severity)\n\n\n#| code-fold: true\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Select relevant features (numeric + categorical)\nselected_features = [\n    'Year', 'Month', 'Country Code', 'Region', 'District', 'City',\n    'Means of attack', 'Attack context', 'Actor type', 'Motive',\n    'Latitude', 'Longitude'\n]\n\n# Check which columns exist in the dataset\nexisting_features = [col for col in selected_features if col in df.columns]\nmissing_features = [col for col in selected_features if col not in df.columns]\n\nif missing_features:\n    print(f\"Warning: Missing columns: {missing_features}\")\n    selected_features = existing_features\n\nX = df[selected_features].copy().fillna('Unknown')\ny = df['Severity Level']\n\n# Encode categorical features\ncategorical_cols = [\n    'Country Code', 'Region', 'District', 'City',\n    'Means of attack', 'Attack context'\n]\n\n# Copy data\nX_encoded = X.copy()\n\n# List of all non-numeric columns (categorical)\ncategorical_cols = X_encoded.select_dtypes(include='object').columns.tolist()\n\n# Encode all categorical features using LabelEncoder\nlabel_encoders = {}\nfor col in categorical_cols:\n    le = LabelEncoder()\n    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n    label_encoders[col] = le\n\n# Encode target\nle_target = LabelEncoder()\ny_encoded = le_target.fit_transform(y)",
    "crumbs": [
      "About",
      "Supervised"
    ]
  },
  {
    "objectID": "projects/Scholarship/supervised.html#feature-selection",
    "href": "projects/Scholarship/supervised.html#feature-selection",
    "title": "Predicting Attack Severity: Low, Medium, High",
    "section": "",
    "text": "Feature selection using Sequential Forward Floating Selection (SFFS) highlighted the most predictive variables: - Year - Country Code - Region - Means of Attack - Actor Type - Motive - Longitude\nThese features capture both the context and timing of incidents, aiding in distinguishing between severity levels.\n#| code-fold: true\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_encoded, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n)\n\n# Set up model + cross-validation\nclf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Sequential Floating Forward Selection\nsffs = SFS(clf,\n           k_features='best',\n           forward=True,\n           floating=True,\n           scoring='f1_weighted',\n           cv=cv,\n           n_jobs=-1)\n\nsffs.fit(X_train, y_train)\n\nselected_feature_indices = list(sffs.k_feature_idx_)\nselected_feature_names = [X_encoded.columns[i] for i in selected_feature_indices]\nprint(\"Selected features:\", selected_feature_names)",
    "crumbs": [
      "About",
      "Supervised"
    ]
  },
  {
    "objectID": "projects/Scholarship/supervised.html#model-evaluation-confusion-matrix",
    "href": "projects/Scholarship/supervised.html#model-evaluation-confusion-matrix",
    "title": "Predicting Attack Severity: Low, Medium, High",
    "section": "",
    "text": "The model achieved a moderate overall accuracy of 65.8%, with stronger performance on low-severity incidents but room for improvement in detecting more severe attacks.\n#| code-fold: true\n\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\n# Train model using ONLY selected features\n# Final classifier with class weight handling\nclf_final = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n\nclf_final.fit(X_train.iloc[:, selected_feature_indices], y_train)\n\n# Predict on test set\ny_pred = clf_final.predict(X_test.iloc[:, selected_feature_indices])\n\n# Evaluation\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=le_target.classes_))\n\n# Confusion Matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred), display_labels=le_target.classes_)\ndisp.plot(cmap=\"Reds\")",
    "crumbs": [
      "About",
      "Supervised"
    ]
  },
  {
    "objectID": "projects/Scholarship/supervised.html#polynomial-degree-tuning",
    "href": "projects/Scholarship/supervised.html#polynomial-degree-tuning",
    "title": "Predicting Attack Severity: Low, Medium, High",
    "section": "Polynomial degree tuning",
    "text": "Polynomial degree tuning\nWe apply hyperparameter tuning via K-Fold cross-validation to select the optimal polynomial degree for modeling the trend of total affected individuals over time.\n#| code-fold: true\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\n\n# --- Load the data ---\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# --- Create Total Affected column if not already present ---\nif 'Total Affected' not in df.columns:\n    df['Total Affected'] = df[['Total killed', 'Total wounded', 'Total kidnapped']].sum(axis=1).fillna(0)\n\n# --- Aggregate by year ---\nyearly = df.groupby('Year').agg(\n    Total_Incidents=('Incident ID', 'count') if 'Incident ID' in df.columns else ('Year', 'count'),\n    Total_Affected=('Total Affected', 'sum')\n).reset_index()\n\n# --- Prepare X and y ---\nX_years = yearly['Year'].values.reshape(-1, 1)\ny_affected = yearly['Total_Affected'].values\n\n# --- Polynomial regression with CV ---\ndegrees = range(1, 6)\ncv = KFold(n_splits=min(5, len(yearly)), shuffle=True, random_state=42)\nmean_r2_scores = []\nstd_r2_scores = []\n\nfor d in degrees:\n    r2_fold_scores = []\n    for train_idx, val_idx in cv.split(X_years):\n        X_train, X_val = X_years[train_idx], X_years[val_idx]\n        y_train, y_val = y_affected[train_idx], y_affected[val_idx]\n\n        # Fit polynomial\n        try:\n            coeffs = np.polyfit(X_train.ravel(), y_train, deg=d)\n            poly = np.poly1d(coeffs)\n\n            # Predict + score\n            y_pred = poly(X_val.ravel())\n            r2 = r2_score(y_val, y_pred)\n            r2_fold_scores.append(r2)\n        except np.RankWarning:\n            print(f\"Warning: Polynomial degree {d} may be too high for the data\")\n            r2_fold_scores.append(-np.inf)\n\n    mean_r2_scores.append(np.mean(r2_fold_scores))\n    std_r2_scores.append(np.std(r2_fold_scores))",
    "crumbs": [
      "About",
      "Supervised"
    ]
  },
  {
    "objectID": "projects/Scholarship/supervised.html#cross-validated-model-performance",
    "href": "projects/Scholarship/supervised.html#cross-validated-model-performance",
    "title": "Predicting Attack Severity: Low, Medium, High",
    "section": "Cross-Validated Model Performance",
    "text": "Cross-Validated Model Performance\nThis plot compares cross-validated R² scores across polynomial degrees from 1 to 5. The optimal degree balances model fit and generalization.\n#| code-fold: true\n\n# --- Plot CV scores ---\nplt.figure(figsize=(7, 4))\nplt.errorbar(degrees, mean_r2_scores, yerr=std_r2_scores, marker='o')\nplt.xticks(degrees)\nplt.xlabel(\"Polynomial Degree\")\nplt.ylabel(\"Cross-validated R² Score\")\nplt.title(\"Polynomial Degree vs CV R² Score\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Report best degree \nbest_degree = degrees[np.argmax(mean_r2_scores)]\nprint(f\"Best polynomial degree: {best_degree}\")\nprint(f\"Mean R² = {max(mean_r2_scores):.4f} ± {std_r2_scores[np.argmax(mean_r2_scores)]:.4f}\")",
    "crumbs": [
      "About",
      "Supervised"
    ]
  },
  {
    "objectID": "projects/Scholarship/supervised.html#forecast-trendline-total-affected-by-year",
    "href": "projects/Scholarship/supervised.html#forecast-trendline-total-affected-by-year",
    "title": "Predicting Attack Severity: Low, Medium, High",
    "section": "Forecast Trendline: Total Affected by Year",
    "text": "Forecast Trendline: Total Affected by Year\n\nLinear Regression\nThe fitted polynomial model (in red) captures the long-term trend in violence against Aid Workers. Actual yearly totals are shown in black. The model forecasts 804 affected individuals in 2025.\n#| code-fold: true\n\nimport pandas as pd\nimport numpy as np\nimport altair as alt\n\n# Load data and filter out 2025\ndf = pd.read_csv(\"security_incidents.csv\")\ndf = df[df['Year'] &lt; 2025]  # Exclude 2025 data for training\n\n# Compute total affected\ndf['Total Affected'] = df[['Total killed', 'Total wounded', 'Total kidnapped']].sum(axis=1).fillna(0)\n\n# Group by year\nyearly = df.groupby('Year').agg(\n    Total_Incidents=('Incident ID', 'count') if 'Incident ID' in df.columns else ('Year', 'count'),\n    Total_Affected=('Total Affected', 'sum')\n).reset_index()\n\n# Fit polynomial regression with optimal degree\ncoeffs = np.polyfit(yearly['Year'], yearly['Total_Affected'], deg=best_degree)\npoly = np.poly1d(coeffs)\n\n# Predict for 2025\npred_2025 = int(round(poly(2025)))\nprint(f\"Predicted Total Affected in 2025: {pred_2025}\")\n\n# Add fitted values\nyearly['Fitted'] = poly(yearly['Year'])\n\n# Append 2025 prediction to the DataFrame\nyearly_extended = yearly.copy()\nyearly_extended = pd.concat([\n    yearly,\n    pd.DataFrame([{\n        'Year': 2025,\n        'Total_Incidents': None,\n        'Total_Affected': None,\n        'Fitted': pred_2025\n    }])\n], ignore_index=True)\n\n# Melt data for Altair plotting\nplot_df = yearly_extended.melt(id_vars='Year', value_vars=['Total_Affected', 'Fitted'], \n                                var_name='Type', value_name='People Affected')\n\nplot_df.loc[plot_df['Type'] == 'Fitted', 'People Affected'] = plot_df.loc[plot_df['Type'] == 'Fitted', 'People Affected'].round(0)\n\n# Replace \"Total_Affected\" with \"Total Affected\" in the Type column\nplot_df['Type'] = plot_df['Type'].replace('Total_Affected', 'Total Affected')\n\n# Plot actual vs fitted\nalt.Chart(plot_df).mark_line(point=True).encode(\n    x=alt.X('Year:O'),\n    y=alt.Y('People Affected:Q'),\n    color=alt.Color('Type:N', title='Line Type', \n                   scale=alt.Scale(domain=['Total Affected', 'Fitted'],\n                                 range=['black', 'crimson'])),\n    tooltip=['Year', 'Type', alt.Tooltip('People Affected:Q', format='.0f')]\n).properties(\n    title='Forecast of Violence Against Aid Workers (1997–2025)',\n    width=600,\n    height=400\n)\n\n\nPolynomial Regression\nWhile cross-validation favors a linear trend due to model simplicity and limited data points, we opted to use a 4th-degree polynomial as it better captures observed spikes and non-linear behavior in the data — particularly post-2010. This model provides a more realistic representation of emerging patterns in humanitarian aid violence, and predicts that aid worker deaths will 804 by the end of 2025.\n#| code-fold: true\n\nimport pandas as pd\nimport numpy as np\nimport altair as alt\n\n# Load data and filter out 2025\ndf = pd.read_csv(\"security_incidents.csv\")\ndf = df[df['Year'] &lt; 2025]  # Exclude 2025 data for training\n\n# Compute total affected\ndf['Total Affected'] = df[['Total killed', 'Total wounded', 'Total kidnapped']].sum(axis=1).fillna(0)\n\n# Group by year\nyearly = df.groupby('Year').agg(\n    Total_Incidents=('Incident ID', 'count') if 'Incident ID' in df.columns else ('Year', 'count'),\n    Total_Affected=('Total Affected', 'sum')\n).reset_index()\n\n# Fit polynomial regression with degree 4\ncoeffs = np.polyfit(yearly['Year'], yearly['Total_Affected'], deg=4)\npoly = np.poly1d(coeffs)\n\n# Predict for 2025\npred_2025 = int(round(poly(2025)))\nprint(f\"Predicted Total Affected in 2025: {pred_2025}\")\n\n# Add fitted values\nyearly['Fitted'] = poly(yearly['Year'])\n\n# Append 2025 prediction to the DataFrame\nyearly_extended = yearly.copy()\nyearly_extended = pd.concat([\n    yearly,\n    pd.DataFrame([{\n        'Year': 2025,\n        'Total_Incidents': None,\n        'Total_Affected': None,\n        'Fitted': pred_2025\n    }])\n], ignore_index=True)\n\n# Melt data for Altair plotting\nplot_df = yearly_extended.melt(id_vars='Year', value_vars=['Total_Affected', 'Fitted'], \n                                var_name='Type', value_name='People Affected')\n\n# Round the Fitted values to integers\nplot_df.loc[plot_df['Type'] == 'Fitted', 'People Affected'] = plot_df.loc[plot_df['Type'] == 'Fitted', 'People Affected'].round(0)\n\n# Replace \"Total_Affected\" with \"Total Affected\" in the Type column\nplot_df['Type'] = plot_df['Type'].replace('Total_Affected', 'Total Affected')\n\n# Plot actual vs fitted\nalt.Chart(plot_df).mark_line(point=True).encode(\n    x=alt.X('Year:O'),\n    y=alt.Y('People Affected:Q'),\n    color=alt.Color('Type:N', title='Line Type', \n                   scale=alt.Scale(domain=['Total Affected', 'Fitted'],\n                                 range=['black', 'crimson'])),\n    tooltip=['Year', 'Type', alt.Tooltip('People Affected:Q', format='.0f')]\n).properties(\n    title='Forecast of Violence Against Aid Workers (1997–2025)',\n    width=600,\n    height=400\n)",
    "crumbs": [
      "About",
      "Supervised"
    ]
  },
  {
    "objectID": "projects/DS5000/report.html#introduction",
    "href": "projects/DS5000/report.html#introduction",
    "title": "Voting Rates in Congressional Districts: Key Findings",
    "section": "Introduction",
    "text": "Introduction\nVoting is an important part of the democratic process, and increasing voter turnout is crucial in allowing all voices to be heard. Voter turnout varies significantly throughout districts, and understanding the variables that drive voter turnout can provide stakeholders with valuable information to increase voter participation.",
    "crumbs": [
      "About",
      "Report"
    ]
  },
  {
    "objectID": "projects/DS5000/report.html#objective",
    "href": "projects/DS5000/report.html#objective",
    "title": "Voting Rates in Congressional Districts: Key Findings",
    "section": "Objective",
    "text": "Objective\nThe objective of this project is:\n1. To collect and clean relevant datasets related to voter turnout and population information.\n2. To perform exploratory data analysis (EDA) to identify key trends and patterns.\n3. To apply unsupervised learning techniques (clustering, dimensionality reduction) and supervised learning models (classification, regression) to uncover insights.\n4. To analyze specific factors such as race, education, or socioeconomic factors, and highlight any patterns or trends revealed in the data.",
    "crumbs": [
      "About",
      "Report"
    ]
  },
  {
    "objectID": "projects/DS5000/report.html#key-findings",
    "href": "projects/DS5000/report.html#key-findings",
    "title": "Voting Rates in Congressional Districts: Key Findings",
    "section": "Key Findings",
    "text": "Key Findings\n\nTop Factors Correlated with Voter Turnout\n\nInternet Access: Areas with Higher Internet Access had a 0.63 correlation with Voter Turnout\nHigher Education: Areas with higher percentages of voters holding a Bachelor’s Degree or Higher had a 0.62 Correlation with Voter Turnout\nIncome Level: Areas with Higher Median Income Level had a 0.56 correlation with Voter Turnout\nPoverty Rate: Areas with Higher levels of Poverty had a -0.66 correlation with Voter Turnout.\nAfrican American: Areas with a Higher density of African American had a -0.25 correlation with Voter Turnout.\nHigh School Degree: Areas with a Higher percentage of voters with a High School Degree had a -0.31 correlation with Voter Turnout.\n\n\n\nCorrelation Heatmap\n\nFigure 1: Correlation Heatmap of all features",
    "crumbs": [
      "About",
      "Report"
    ]
  },
  {
    "objectID": "projects/DS5000/report.html#supervised-learning",
    "href": "projects/DS5000/report.html#supervised-learning",
    "title": "Voting Rates in Congressional Districts: Key Findings",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nRegression\nLinear regression model predicted voter turnout with moderate accuracy.\n\nRMSE:\n\nLinear Regression: 0.571\n\nThis means the model’s predictions are, on average, 0.571 standard deviations away from the actual values.\n\nCross Validation: 0.588\nLasso Regression: 0.576\n\nMAE:\n\nLinear Regression: 0.465\n\nThe average absolute difference between predicted and actual values is 0.465 standard deviations.\n\nCross Validation: 0.456\nLasso Regression: 0.469\n\nR^2:\n\nLinear Regression: 0.688\n\n68.8% of the variance in voter turnout is explained by the model\n\nCross Validation: 0.64\nLasso Regression: 0.682\n\nKey Predictors: High School Graduate, Poverty Rate and Income, Internet Access, Median Age\nThis model has implications for real-world use. However, with a moderately high prediction error, the model still requires further improvement and calibration before being reliably applied in real voting scenarios. Doing this will allow policymakers and analysts to gain insights and make educated decisions.\n\n\n\nLinear Regression Visual\n\nFigure 2: Parity Plot and Accuracy Metrics for Regression\n\n\nBinary Classification\nClassifying Districts between high and low voting rates\n\nLogistic Regression and Random Forest models achieved 82% and 84% accuracy\n\nKey Predictors: Education, Income, Internet Access, Poverty Rate\n\n\n\nRandom Forest Binary Classification Visual\n\nFigure 3: Confusion Diagram and Accuracy Metrics with Random Forest Binary Classification\n\n\nMulti Classification\nClassifying districts between high and medium, and low voting rates\n\nAccuracy\n\nLogistic Regression: 0.68\nRandom Forest: 0.62\nDecision Tree: 0.56\n\nKey Predictors:\n\nHigh Voting Rates\n\nEducation, Income, Internet Access,\n\nMedium Voting Rates\n\nOverlapping patterns make it difficult to pinpoint\n\nLow Voting Rates\n\nPoverty Rate\n\n\n\n\n\nLogistic Regression Multi-Class Classification Visual\n\nFigure 4: Confusion Diagram and Accuracy Metrics for Logistic Regression Binary Classification",
    "crumbs": [
      "About",
      "Report"
    ]
  },
  {
    "objectID": "projects/DS5000/report.html#unsupervised-learning-predictions",
    "href": "projects/DS5000/report.html#unsupervised-learning-predictions",
    "title": "Voting Rates in Congressional Districts: Key Findings",
    "section": "Unsupervised Learning Predictions",
    "text": "Unsupervised Learning Predictions",
    "crumbs": [
      "About",
      "Report"
    ]
  },
  {
    "objectID": "projects/DS5000/report.html#dimensionality-reduction",
    "href": "projects/DS5000/report.html#dimensionality-reduction",
    "title": "Voting Rates in Congressional Districts: Key Findings",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nPCA (Principal Component Analysis)\n\nPC1 (driven by income, education, and population) explains 55% of the variance and is correlated with high voting rates.\n\nInsight: Socioeconomic conditions are strong indicators of voter turnout.\n\n Figure 5: PCA: First Two Principal Components Colored by Voting Rate Estimate\n\n\nt-SNE Clustering\n\nt-SNE visualizations revealed distinct groupings:\n\nHigh Voting Rates: Clustered with areas of high income and education.\n\nLow Voting Rates: Grouped with higher poverty and fewer resources.",
    "crumbs": [
      "About",
      "Report"
    ]
  },
  {
    "objectID": "projects/DS5000/report.html#clustering",
    "href": "projects/DS5000/report.html#clustering",
    "title": "Voting Rates in Congressional Districts: Key Findings",
    "section": "Clustering",
    "text": "Clustering\n\nK-Means Clustering\n\nInsight: Voter turnout patterns show two main groups:\n\nHigh Voting Rates: Areas with higher education levels, Asian and White populations, older age demographics, and Access to the Internet.\n\nLow Voting Rates: Linked to regions with Higher Poverty Levels and larger African American populations.\n\n\n Figure 6: K-Means Clustering Scatterplot: Demographic Features vs. Voting Rate Estimate\n\n\n\nDBSCAN Clustering\n\nInsight: Clustering based on density failed to identify meaningful patterns due to the uniform distribution of data.\n\n\n\n\nHierarchical Clustering\n\nInsight: Similar to K-Means—regions with higher education and income levels demonstrate higher voter turnout, while areas with poverty and lower education lag behind.",
    "crumbs": [
      "About",
      "Report"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html",
    "href": "projects/DS5000/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised Learning is a type of Machine Learning where the model is trained with labeled data, and learns relationships between the input and target variable. Once the model learns the relationships, begins to predict or categorize unseen data.\n\nThe two main types of Supervised learning include\n\nRegression\nClassification\n\n\nUtilizing Supervised Learning for predicting voting rates is important because it allows us to understand the relationship of input features and voting rate. Furthermore, we have a clear objective, with predicting voting rate as the continuous variable for regression and classifying districts between high and low voting rate",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#regression-accuracy-metrics",
    "href": "projects/DS5000/supervised-learning/main.html#regression-accuracy-metrics",
    "title": "Supervised Learning",
    "section": "Regression Accuracy Metrics",
    "text": "Regression Accuracy Metrics\nThe accuracy of a regression model is explained by a series of metrics, MSE, RMSE (Root Mean Square Error), R^2, MAE (Mean Absolute Error), and a parity plot.\n\nMean Squared Error\n\nMSE quantifies the average squared difference between the actual y values and predicted y values.\nSquaring the difference causes the larger errors to be heavily penalized.\nThis is utilized as a metric to evaluate model performance, with smaller values indicating a more accurate prediction.\n\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n\n\n\nRoot Mean Squared Error\n\nRMSE represents the square root of Mean Squared Error (MSE).\n\nAs stated earlier, MSE computes the squared difference between predicted and actual values\nTaking the square root of MSE places the metric in the same units as the target values, making it more easy to understand and interpret.\nA smaller RMSE value indicates a smaller difference between predicted and actual values, and a more accurate model\nWe will be using RMSE instead of MSE to test accuracy due to its increased interpretability.\n\n\\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n\\]\n\n\n\nMean Absolute Error\n\nMAE also measures the difference between actual and predicted values.\n\nHowever, it only measures the average absolute difference between actual and predicted values.\n\nUnlike MSE and RMSE, it does not penalize larger error.\n\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n\\]\n\n\n\nR^2 Value\n\nR-squared computes the proportion of variance in the target variable explained by the features in the data.\n\nThe residual sum of squares (the sum of differences between actual values and predicted values) is divided by Total sum of Squares (sum of differences between the actual value and the mean of all values).\n\nThe Residual Sum of Squares quantifies the sum of differences between actual and predicted values, while the Total Sum of Squares computes the sum of squared deviations between actual values and the mean.\n\n\\[\n\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n\\[\n\\text{TSS} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n\\]\n\nSubtracting this proportion by 1 provides us the proportion of variance that the model explains.\n\nThis value ranges from 0 to 1, a higher value indicating a greater portion of the variance is explained by the features.\n\n\\[\nR^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n\\]\n\n\n\nParity Plot\n\nA Parity Plot is a visual representation that shows the distribution of actual and predicted values.\n\nA line is plotted showcasing what a perfect fit would look like, and both actual and predicted values are plotted.\n\nIf data points are close to the line of best fit, this represents the model is accurate, and if they are far away it indicates the model has errors.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#binary-classification",
    "href": "projects/DS5000/supervised-learning/main.html#binary-classification",
    "title": "Supervised Learning",
    "section": "Binary Classification",
    "text": "Binary Classification\n\nBinary Classification is a form of classification where the model must classify data into two groups.\n\nA binary target is chosen for the model to classify the data into, and similar to regression, the data is split into train and test split, but this time to learn the relationship between the binary target and features.\n\nExamples:\n\nClassifying data into High and Low\n\nClassifying data into Boy and Girl\n\nClassifying data into Cat and Dog",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#multiclass-classification",
    "href": "projects/DS5000/supervised-learning/main.html#multiclass-classification",
    "title": "Supervised Learning",
    "section": "Multiclass Classification",
    "text": "Multiclass Classification\n\nMulticlass classification is a form of Supervised learning where there are many categories, and the model attempts to classify an instance into a category.\n\nExamples:\n\nClassifying an image into categories of Dog, Cat or House.\n\nClassifying data as Low, Medium or High\n\nThere are multiple forms of classification models, including:\n\nLinear models: Logistic Regression\n\nTree-Based models: Decision Trees, Random Forests, and Gradient Boost.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#classification-accuracy-metrics",
    "href": "projects/DS5000/supervised-learning/main.html#classification-accuracy-metrics",
    "title": "Supervised Learning",
    "section": "Classification Accuracy Metrics",
    "text": "Classification Accuracy Metrics\n\nPrecision\n\nPrecision measures the proportion of positive instances that are labeled positive.\n\nIt is used to check for False Positives, where values were incorrectly labeled as True when they were actually False.\n\nA higher precision score indicates fewer False Positives identified.\n\n\\[\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\]\n\n\n\nRecall\n\nRecall, also known as Sensitivity, measures the proportion of actual positives the model correctly identifies.\n\nRecall is used to check for False Negatives, where values were incorrectly labeled as False when they were actually positive.\n\nA higher Recall score indicates fewer False Negatives identified.\n\n\\[\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\n\n\n\nFalse Positive Rate\n\nThe False Positive Rate, or Specificity, indicates the proportion of instances labeled False Positives compared to those that are actually Negative.\n\nFalse positives are instances that are negative but predicted as positive.\n\n\\[\n\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n\\]\n\n\n\nF1 Score\n\nF1 Score combines the Precision and Recall score, utilizing both metrics to evaluate the accuracy of the model.\n\nIt provides a single score that balances the tradeoffs between the two metrics.\n\nA higher F1 score indicates both Precision and Recall are high and accurate.\n\n\\[\n\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\n\n\nROC Curve\n\nAn Receiver Operating Characteristic (ROC) Curve visualizes the accuracy of the model, plotting the True Positive Rate (Recall) against the False Positive Rate.\n\nBy plotting these two values against each other, we can visualize the tradeoff between Recall (accuracy of true positives) and Specificity (accuracy of false positives).\n\nThe diagram showcases a diagonal (baseline), from (0,0) to (1, 1) representing a completely random classifier.\nIf the model’s ROC is higher than this baseline, it means the model performs better than random guessing.\n\nIf below the line, it means the model performs worse.\n\n\n\n\nAUC Score\n\nThe Area Under the Curve (AUC) score quantifies the total area under the ROC curve.\n\nThis measures the overall performance of the model:\n\n1: Perfect model\n\n0.5: Random guessing\n\n&lt; 0.5: Worse than random guessing.\n\n\n\n\n\nAccuracy\n\nThe Accuracy is the ratio of correctly predicted instances to total instances.\n\nIt measures the total true accuracy of the model, with a higher value indicating our model classified predictions correctly.\n\\[\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{Total Observations}}\n\\]\n\n\n\n\nMacro Average\n\nA Macro Average measures the combined accuracy metric performance of all classes.\n\nIt computes the unweighted mean of precision, recall, and F1 scores across all classes.\n\n\\[\n\\text{Macro Average} = \\frac{1}{N} \\sum_{i=1}^N \\text{Metric}_i\n\\]",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#regression-1",
    "href": "projects/DS5000/supervised-learning/main.html#regression-1",
    "title": "Supervised Learning",
    "section": "Regression",
    "text": "Regression\n\nLinear Regression\n\nLinear Regression predicts a continuous target variable by modeling a linear relationship between input features and the target.\nThe model:\n\nTakes in features.\n\nCombines features and weight\n\n\n\\[y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n\\]\n\nThe Y is the predicted value\nUtilizing the predicted Y value, it finds the best-fit line by iteratively reducing the Mean Squared Error between actual and predicted values.\n\n\n\nLasso Regression\n\nLasso Regression adds L1 regularization to linear regression, shrinking some coefficients to zero for feature selection and reducing overfitting.\n\nL1 Regularization (used in Lasso Regression) adds the absolute values of the coefficients as a penalty to the cost function.\n\nThis forces the model to shrink less important coefficients to zero, basically removing features from the model.\n\nWhy L1 Regularization?\n\nReduces overfitting by penalizing large coefficients.\n\nAutomatically performs feature selection, keeping only the most important predictors.\n\nSame as Linear Regression, it finds the best-fit line by iteratively reducing the Mean Squared Error between actual and predicted values, while also affected by L1 regularization\nI decided to use Lasso Regression to minimize non-important features.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#classification-1",
    "href": "projects/DS5000/supervised-learning/main.html#classification-1",
    "title": "Supervised Learning",
    "section": "Classification",
    "text": "Classification\n\nLogistic Regression\n\nLogistic regression uses a probabilistic model to predict the probability of an instance being positive or negative.\n\nThe model:\n\nTakes in features.\n\nTransforms them with weights.\n\nUtilizes a sigmoid function in Binary Classification to generate a probability between 0 and 1.\n\nUtilizes a softmax function in Multiclass CLassification to generate probabilities for multiple classes.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#decision-tree",
    "href": "projects/DS5000/supervised-learning/main.html#decision-tree",
    "title": "Supervised Learning",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nA decision tree utilizes feature conditions to split data into smaller and smaller subsets, forming a tree-like structure.\n\nAt each node:\n\nThe tree evaluates conditions to determine the optimal split.\n\nAccuracy metrics like Gini Index and Entropy measure how well the tree splits data.\n\nLower values indicate a cleaner split.\n\nBy repeatedly splitting and subsetting the data, the decision tree can classify data into both binary and multiclass targets.\n\n\nParameters\nThe parameters for Decision Trees are utilized to prevent overfitting and improve accuracy:\n\nmax_depth: Sets the maximum depth of the tree to prevent overfitting and reduce complexity.\n\nmin_samples_split: Sets the minimum number of samples required to split at a node.\n\nmin_samples_leaf: Indicates the number of samples required to be in a leaf node.\n\nNext step: help understand this better.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#random-forest",
    "href": "projects/DS5000/supervised-learning/main.html#random-forest",
    "title": "Supervised Learning",
    "section": "Random Forest",
    "text": "Random Forest\n\nRandom Forest is similar to Decision Trees but builds multiple trees (ensemble) and combines predictions.\n\n\nHow It Works:\n\nA decision tree is trained on a random subset of data (with replacement).\n\nAt each split, only a random subset of features is considered.\n\nOnce multiple trees are established:\n\nEach tree votes for a class.\n\nThe majority vote determines the final prediction.\n\n\n\nAdvantages:\n\nHelps prevent overfitting.\n\nRobust to outliers.\n\n\n\n\n\nParameters\nThe parameters for Random Forests are the same as Decision Trees but include:\n\nn_estimators: Number of decision trees used in the forest.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#cross-validation",
    "href": "projects/DS5000/supervised-learning/main.html#cross-validation",
    "title": "Supervised Learning",
    "section": "Cross Validation",
    "text": "Cross Validation\nCross Validation is a technique used in Supervised Learning to assess how well the model adapts to unseen data.\n\nThe data is split into k folds of equally sized subjets\n\nFor each iteration, k-1 folds are used to train the model.\nThe remaining fold is used for cross validation.\n\n\nThis is repeated k times, and the results are averaged to provide a reliable metric for accuracy.\n\nWhy Cross Validation is Important\n\nInstead of using just 1 train-test split, the model is able to:\n\nTrain and validate many subset of data.\nEnsure the most reliable model performance\n\n\n\n\nAdvantages\n\nPrevents the data from overfitting\nProvides powerful evaluation metric for model\n\n\n\nDisadvantages\n\nComplex for large datasets",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#one-hot-encoding",
    "href": "projects/DS5000/supervised-learning/main.html#one-hot-encoding",
    "title": "Supervised Learning",
    "section": "One Hot Encoding",
    "text": "One Hot Encoding\nI chose not to use One Hot Encoding for states because:\n- It increased the dimensionality of my data drastically.\n- It caused issues in my regression analysis, with very low accuracy and tendency to overfit.\n- I believed the state’s voting policies, culture, and access to voting were already reflected in the numerical features.\n\nWhat is One Hot Encoding?\n\nConverts categorical features into numerical values.\n\nA new column is created for each category, with values:\n\n1: Indicates True.\n\n0: Indicates False.\n\n\n\n\nFinal Features:\nI chose to leave all the other features as they may influence and indicate different turnouts in the data.\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nmerged_standard_log_df = pd.read_csv('../../data/processed-data/merged_standard_log.csv') \n\n# Drop unnecessary columns\ncolumns_to_drop = ['Nonveterans', 'Congressional_District', 'state_abbreviation', 'District_Name', 'state', 'votes_cast']\nprocessed_df = merged_standard_log_df.drop(columns=columns_to_drop)\n\nprint(\"Final Columns for Modeling:\")\nprint(processed_df.columns)\n\n\nFinal Columns for Modeling:\nIndex(['Total_Population', 'Log_African_American', 'White', 'Log_Asian',\n       'Veterans', 'Below_Poverty_Level', 'Log_Median_Household_Income',\n       'High_School_Graduate', 'Bachelors_Degree_or_Higher',\n       'Households_with_Computers', 'Households_with_Internet',\n       'Log_American_Indian_and_Alaska_Native',\n       'Log_Native_Hawaiian_and_Pacific_Islander', 'Median_Age',\n       'citizen_voting_age_population_estimate', 'voting_rate_estimate'],\n      dtype='object')",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#linear-regression-feature-importance",
    "href": "projects/DS5000/supervised-learning/main.html#linear-regression-feature-importance",
    "title": "Supervised Learning",
    "section": "Linear Regression Feature Importance",
    "text": "Linear Regression Feature Importance\n\n\nCode\n# After training the model\ncoefficients = linear_reg.coef_  # Get coefficients\nfeature_names = X_train.columns  # Get feature names\n\n# Combine coefficients with feature names\nimportance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Coefficient': coefficients\n}).sort_values(by='Coefficient', ascending=False, key=abs)  # Sort by absolute value\n\n# Display top predictors\nprint(\"Top Predictors for Linear Regression:\")\nprint(importance_df.head(10))\n\n\nTop Predictors for Linear Regression:\n                                     Feature  Coefficient\n7                       High_School_Graduate    -0.467101\n5                        Below_Poverty_Level    -0.364904\n10                  Households_with_Internet     0.334820\n14    citizen_voting_age_population_estimate     0.291538\n13                                Median_Age     0.187899\n3                                  Log_Asian    -0.159315\n9                  Households_with_Computers    -0.096102\n12  Log_Native_Hawaiian_and_Pacific_Islander    -0.060645\n4                                   Veterans    -0.044753\n1                       Log_African_American    -0.033687\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['Feature'], importance_df['Coefficient'])\nplt.xlabel(\"Coefficient Value\")\nplt.ylabel(\"Feature\")\nplt.title(\"Linear Regression Coefficients\")\nplt.gca().invert_yaxis()  # Reverse order for better readability\nplt.show()\n\n\n\n\n\n\n\n\n\nhe biggest coefficents was High School Graduate and Poverty Level, both with negative coefficients. Households with Internet, Voting Age population, and MedianAge were the next biggest coefficiences, and were all positive.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#linear-regression-accuracy-analysis",
    "href": "projects/DS5000/supervised-learning/main.html#linear-regression-accuracy-analysis",
    "title": "Supervised Learning",
    "section": "Linear Regression Accuracy Analysis",
    "text": "Linear Regression Accuracy Analysis\n\n\nCode\ndata_metrics = {\n    \"Metric\": [\"RMSE\", \"MAE\", \"R^2\"],\n    \"Train\": [0.5652380229553347, 0.43493826352937637, 0.6765001127743566],\n    \"Test\": [0.5709603067729877, 0.4656940110653836, 0.6885077504960029]\n}\n\n\ndf_metrics = pd.DataFrame(data_metrics)\nprint(df_metrics)\n\n\n  Metric     Train      Test\n0   RMSE  0.565238  0.570960\n1    MAE  0.434938  0.465694\n2    R^2  0.676500  0.688508\n\n\nConducting the Linear regression analysis, the data accuracy metrics indicated that our model performed well on both sets of data.\n\nTrain RMSE had a value of 0.565, and Test RMSE had a value of 0.571.\n\nThis indicates that, on average, when testing, the model’s prediction of voting rate was 0.571 standardized units from the actual value.\n\nThe model had similar RMSE values for the test set and the training set, indicating the model is doing well in applying its training to unseen data.\n\nTrain MAE and Test MAE also indicated the model did very well:\n\nTrain MAE: 0.435\n\nTest MAE: 0.466\n\nThis shows that the average absolute difference between predicted and actual values is very small.\n\nR² values for both the Train and Test sets indicated that more than half of the variance is explained by the model:\n\nTrain R²: 0.677\n\nTest R²: 0.689\n\nThis means that 68.9% of the variance in the test data can be explained.\n\nThe Test R² had a slightly higher (more accurate) value than the Train R², suggesting that the model is very good at predicting unseen values.\n\nThe parity plot of the cross-validated data looks very similar to the non-cross-validated model, indicating that our model is moderately accurate in predicting voting rates.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#cross-validation-1",
    "href": "projects/DS5000/supervised-learning/main.html#cross-validation-1",
    "title": "Supervised Learning",
    "section": "Cross Validation",
    "text": "Cross Validation\n\n\nCode\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n\n# Define scoring metric (negative MSE for compatibility with cross_val_score)\nscoring = make_scorer(mean_squared_error, greater_is_better=False)\n\n# Perform cross-validation (5-fold)\ncv_scores = cross_val_score(linear_reg, features, processed_df[target], cv=5, scoring=scoring)\n\n# Convert negative MSE to positive\ncv_mse_scores = -cv_scores\ncv_rmse_scores = [score ** 0.5 for score in cv_mse_scores]  # RMSE\n\n# Cross-validated predictions (for calculating additional metrics)\ncv_predictions = cross_val_predict(linear_reg, features, processed_df[target], cv=5)\n\n# Compute additional metrics (MAE and R^2)\ncv_mae = mean_absolute_error(processed_df[target], cv_predictions)\ncv_r2 = r2_score(processed_df[target], cv_predictions)\n\n# Print results\nprint(f\"Cross-Validation RMSE Scores: {cv_rmse_scores}\")\nprint(f\"Average Cross-Validation RMSE: {np.mean(cv_rmse_scores):.4f}\")\nprint(f\"Cross-Validation MAE: {cv_mae:.4f}\")\nprint(f\"Cross-Validation R^2: {cv_r2:.4f}\")\n\n# Parity Plot for Cross-Validation\nplt.figure(figsize=(8, 6))\nplt.scatter(processed_df[target], cv_predictions, alpha=0.7, edgecolors='k', label='Cross-Validation Predictions')\nplt.plot([processed_df[target].min(), processed_df[target].max()],\n         [processed_df[target].min(), processed_df[target].max()], color='blue', linestyle='--', label='Perfect Fit')\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Parity Plot - Cross-Validation\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\nCross-Validation RMSE Scores: [0.4842817971199596, 0.5899720563378389, 0.5496217683565214, 0.6775249748666702, 0.6415611537057684]\nAverage Cross-Validation RMSE: 0.5886\nCross-Validation MAE: 0.4564\nCross-Validation R^2: 0.6492",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#cross-validation-linear-regression-accuracy-analysis",
    "href": "projects/DS5000/supervised-learning/main.html#cross-validation-linear-regression-accuracy-analysis",
    "title": "Supervised Learning",
    "section": "Cross Validation Linear Regression Accuracy Analysis",
    "text": "Cross Validation Linear Regression Accuracy Analysis\n\n\nCode\ndata_cross_validation = {\n    \"Metric\": [\"RMSE (Fold 1)\", \"RMSE (Fold 2)\", \"RMSE (Fold 3)\", \"RMSE (Fold 4)\", \"RMSE (Fold 5)\", \n               \"Average RMSE\", \"MAE\", \"R^2\"],\n    \"Score\": [0.4842817971199596, 0.5899720563378389, 0.5496217683565214, 0.6775249748666702, \n              0.6415611537057684, 0.5886, 0.4564, 0.6492]\n}\n\n# Create the DataFrame for Cross-Validation metrics\ndf_cross_validation = pd.DataFrame(data_cross_validation)\n\nprint(df_cross_validation)\n\n\n          Metric     Score\n0  RMSE (Fold 1)  0.484282\n1  RMSE (Fold 2)  0.589972\n2  RMSE (Fold 3)  0.549622\n3  RMSE (Fold 4)  0.677525\n4  RMSE (Fold 5)  0.641561\n5   Average RMSE  0.588600\n6            MAE  0.456400\n7            R^2  0.649200\n\n\nConducting Cross Validation Linear regression analysis, the data accuracy metrics indicated that our model performed similarly to the non-cross-validated Log Regression model.\n\nThe average RMSE had a value of 0.58, slightly higher than 0.565 from the non-cross-validated model.\n\nThe model also had an MAE of 0.45 and R^2 value of 0.69, both higher values than without cross validating.\n\nThe Cross Validation Accuracy Metrics tells us that there was not a big difference in accuracy between this model and the model that didn’t utilize Cross Validation.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#lasso-regression-1",
    "href": "projects/DS5000/supervised-learning/main.html#lasso-regression-1",
    "title": "Supervised Learning",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\n\nCode\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the target and features\ntarget = 'voting_rate_estimate'\nfeatures = processed_df.drop(columns=[target])\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    features, processed_df[target], test_size=0.2, random_state=42\n)\n\n# Initialize Lasso Regression with alpha (regularization strength)\nlasso_reg = Lasso(alpha=0.01, random_state=42)  # You can tune 'alpha' for stronger or weaker regularization\n\n# Train the model\nlasso_reg.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = lasso_reg.predict(X_train)\ny_pred_test = lasso_reg.predict(X_test)\n\n# Evaluate the model\nmse_train = mean_squared_error(y_train, y_pred_train)\nmse_test = mean_squared_error(y_test, y_pred_test)\nr2_train = r2_score(y_train, y_pred_train)\nr2_test = r2_score(y_test, y_pred_test)\n\n# Compute RMSE and MAE\nrmse_train = np.sqrt(mse_train)\nrmse_test = np.sqrt(mse_test)\nmae_train = mean_absolute_error(y_train, y_pred_train)\nmae_test = mean_absolute_error(y_test, y_pred_test)\n\n# Print evaluation metrics\nprint(f\"Lasso Regression Results:\")\nprint(f\"Train RMSE: {rmse_train:.4f}, MAE: {mae_train:.4f}, R²: {r2_train:.4f}\")\nprint(f\"Test RMSE: {rmse_test:.4f}, MAE: {mae_test:.4f}, R²: {r2_test:.4f}\")\n\n# Display coefficients\nlasso_coefficients = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Coefficient': lasso_reg.coef_\n}).sort_values(by='Coefficient', ascending=False)\n\n# Parity Plot\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, y_pred_test, alpha=0.7, edgecolors='k', label='Test Data')\nplt.scatter(y_train, y_pred_train, alpha=0.7, edgecolors='r', label='Train Data')\nplt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], color='blue', linestyle='--', label='Perfect Fit')\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Lasso Regression Parity Plot\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\nLasso Regression Results:\nTrain RMSE: 0.5673, MAE: 0.4337, R²: 0.6741\nTest RMSE: 0.5767, MAE: 0.4691, R²: 0.6822",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#lasso-regression-accuracy-analysis",
    "href": "projects/DS5000/supervised-learning/main.html#lasso-regression-accuracy-analysis",
    "title": "Supervised Learning",
    "section": "Lasso Regression Accuracy Analysis",
    "text": "Lasso Regression Accuracy Analysis\n\n\nCode\nlasso_results = {\n    \"Metric\": [\"RMSE\", \"MAE\", \"R²\"],\n    \"Train\": [0.5673, 0.4337, 0.6741],\n    \"Test\": [0.5767, 0.4691, 0.6822]\n}\n\n# Convert to DataFrame\nlasso_results_df = pd.DataFrame(lasso_results)\n\nprint(lasso_results_df)\n\n\n  Metric   Train    Test\n0   RMSE  0.5673  0.5767\n1    MAE  0.4337  0.4691\n2     R²  0.6741  0.6822\n\n\nConducting the Lasso Regression analysis, the data accuracy metrics indicated that our model perfomed similar to linear regression.\n\nTrain RMSE had a value of 0.567, and Test RMSE had a value of 0.577.\n\nThis indicates that, on average, when testing, the model’s prediction was 0.577 standardized units from the actual value.\n\nThe model had similar RMSE values for the test set and the training set, indicating the model is doing well in applying its training to unseen data.\n\nTrain MAE and Test MAE also indicated the model performed well:\n\nTrain MAE: 0.434\n\nTest MAE: 0.469\n\nThis shows that the average absolute difference between predicted and actual values is very small.\n\nR² values for both the Train and Test sets showed that a significant proportion of the variance is explained by the model:\n\nTrain R²: 0.674\n\nTest R²: 0.682\n\nThis means that 68.2% of the variance in the test data can be explained.\n\nThe Lasso R² value was slightly lower value than the Linear Regressioin R² value(0.0682).\n\nSimilar to Linear Regression, The parity plot of the cross-validated data reflects a consistent alignment of predicted and actual values, suggesting that the model is accurate and reliable for predicting the target variable.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#conclusion",
    "href": "projects/DS5000/supervised-learning/main.html#conclusion",
    "title": "Supervised Learning",
    "section": "Conclusion",
    "text": "Conclusion\nThese metrics tell us that our model is performing moderately well with both sets of data, as well as with and without cross validation.\n\nFor both methods, both RMSE and MAE values are moderately close to zero.\n\nBoth methods have a moderately high R² value, indicating our predicted values are close to actual values.\n\nLinear Regression, Lasso Regression, and implimenting Cross Validation all provided almost identical accuracy metrics, with Linear regression having the lowest RMSE value , and cross validated linear regression having the lowest MAE and R^2 value.\n\nThis tells us that the model has begun to learn trends and relationships in the data to predict voting rates with moderately high accuracy.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#real-world-application",
    "href": "projects/DS5000/supervised-learning/main.html#real-world-application",
    "title": "Supervised Learning",
    "section": "Real World Application",
    "text": "Real World Application\nThis model has implications for real-world use. However, with a prediction error of 0.57 standardized units, the model still requires further improvement and calibration before being reliably applied in real voting scenarios. Doing this will allow policymakers and analysts to gain insights and make educated decisions.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#target-selection",
    "href": "projects/DS5000/supervised-learning/main.html#target-selection",
    "title": "Supervised Learning",
    "section": "Target Selection",
    "text": "Target Selection\nI chose my Binary Classification target as ‘High Voting Rate’ and ‘Low Voting Rate’. Districts were labeled as High Voting Rate if the voting rate exceeded the mean voting rate.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#log-regression-binary-classification",
    "href": "projects/DS5000/supervised-learning/main.html#log-regression-binary-classification",
    "title": "Supervised Learning",
    "section": "Log Regression Binary Classification",
    "text": "Log Regression Binary Classification\n\n\nCode\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nmerged_standard_log_df['High_Voting_Rate'] = (merged_standard_log_df['voting_rate_estimate'] &gt; merged_standard_log_df['voting_rate_estimate'].median()).astype(int)\n\n# Define target and features\nbinary_target = 'High_Voting_Rate'\n\n# Ensure High_Voting_Rate is included in processed_df\nprocessed_df['High_Voting_Rate'] = merged_standard_log_df['High_Voting_Rate']\n\n# Remove binary and regression targets from features\nfeatures = processed_df.drop(columns=[binary_target, 'voting_rate_estimate'])\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    features, processed_df[binary_target], test_size=0.2, random_state=42\n)\n\n# Initialize the Logistic Regression model\nlog_reg = LogisticRegression(max_iter=1000, random_state=42)\n\n# Train the model\nlog_reg.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = log_reg.predict(X_train) #predict classes for training set \ny_pred_test = log_reg.predict(X_test) #predict classes for test set \ny_prob_test = log_reg.predict_proba(X_test)[:, 1]  # Probabilities for ROC curve\n\n# Evaluate the model\nprint(\"Classification Report (Test Data):\")\nprint(classification_report(y_test, y_pred_test))\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred_test)\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, y_prob_test)\nroc_auc = roc_auc_score(y_test, y_prob_test)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Classifier\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic (ROC) Curve\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\nClassification Report (Test Data):\n              precision    recall  f1-score   support\n\n           0       0.89      0.72      0.79        43\n           1       0.77      0.91      0.83        44\n\n    accuracy                           0.82        87\n   macro avg       0.83      0.82      0.81        87\nweighted avg       0.83      0.82      0.81        87",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#log-binary-classification-accuracy-analysis",
    "href": "projects/DS5000/supervised-learning/main.html#log-binary-classification-accuracy-analysis",
    "title": "Supervised Learning",
    "section": "Log Binary Classification Accuracy Analysis",
    "text": "Log Binary Classification Accuracy Analysis\n\n\nCode\ndata_classification_report = {\n    \"Class\": [0, 1, \"accuracy\", \"macro avg\"],\n    \"Precision\": [0.89, 0.77, None, 0.83],\n    \"Recall\": [0.72, 0.91, None, 0.82],\n    \"F1-Score\": [0.79, 0.83, 0.82, 0.81],\n    \"Support\": [43, 44, 87, 87]\n}\n\n# Create the DataFrame for the classification report\ndf_classification_report = pd.DataFrame(data_classification_report)\n\n# Display the DataFrame\nprint(df_classification_report)\n\n\n       Class  Precision  Recall  F1-Score  Support\n0          0       0.89    0.72      0.79       43\n1          1       0.77    0.91      0.83       44\n2   accuracy        NaN     NaN      0.82       87\n3  macro avg       0.83    0.82      0.81       87\n\n\n\nPrecision\nThe accuracy metrics for our model showed that precision scores were very high, with values of 89% for Low Voting Rate and 77% for High Voting Rate.\n\n\nRecall\nRecall scores were also very high, with 91% for High Voting Rate and 72% for Low Voting Rate, indicating that the model captured most true positives.\n\n\nF1\nThe F1 score was 83% for High Voting Rate, reflecting a good balance between Precision and Recall. The overall accuracy was also 82%, showcasing a model that is effective in distinguishing between two classes.\n\n\nConfusion Matrix\nThe Confusion Matrix shows the results of the Log Regression Binary Classification model:\n\nThe model correctly predicted 40 values of ‘High Voting Rate’ (True Positives).\n\nThe model correctly predicted 31 values of ‘Low Voting Rate’ (True Negatives).\n\nThe model incorrectly classified 12 values as ‘High Voting Rate’ when they were actually low (False Positive).\n\nThe model incorrectly classified 4 values as ‘Low Voting Rate’ when they were actually high (False Negative).\n\n\n\nROC Curve\nThe ROC Curve showcases an AUC score of 0.87, indicating an accurate model that distinguishes between Low and High voting rates.\n\nThe curve rises steeply, indicating the model quickly achieves a high True Positive Rate with low False Positive Rate.\n\nFurthermore, the curve is significantly higher than the diagonal baseline, showing that the model is better than a random classifier.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#decision-tree-binary-classification",
    "href": "projects/DS5000/supervised-learning/main.html#decision-tree-binary-classification",
    "title": "Supervised Learning",
    "section": "Decision Tree Binary Classification",
    "text": "Decision Tree Binary Classification\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define the Decision Tree model\ndt_clf = DecisionTreeClassifier(\n    criterion=\"gini\",        # Use 'gini' or 'entropy' to measure split quality\n    max_depth=5,             # Maximum depth of the tree to prevent overfitting\n    min_samples_split=10,    # Minimum samples needed to split an internal node\n    min_samples_leaf=5,      # Minimum samples in each leaf node\n    random_state=42          # Ensure reproducibility\n)\n\n# Train the Decision Tree\ndt_clf.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = dt_clf.predict(X_train)  # Predict on training data\ny_pred_test = dt_clf.predict(X_test)    # Predict on test data\n\n# Evaluate the model\nprint(\"Decision Tree - Classification Report (Binary):\")\nprint(classification_report(y_test, y_pred_test))\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred_test)\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\nplt.title(\"Decision Tree - Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n\nDecision Tree - Classification Report (Binary):\n              precision    recall  f1-score   support\n\n           0       0.76      0.51      0.61        43\n           1       0.64      0.84      0.73        44\n\n    accuracy                           0.68        87\n   macro avg       0.70      0.68      0.67        87\nweighted avg       0.70      0.68      0.67        87",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#decision-tree-binary-classification-accuracy-analysis",
    "href": "projects/DS5000/supervised-learning/main.html#decision-tree-binary-classification-accuracy-analysis",
    "title": "Supervised Learning",
    "section": "Decision Tree Binary Classification Accuracy Analysis",
    "text": "Decision Tree Binary Classification Accuracy Analysis\n\n\nCode\ndata_decision_tree_binary = {\n    \"Class\": [0, 1, \"accuracy\", \"macro avg\"],\n    \"Precision\": [0.76, 0.64, None, 0.70],\n    \"Recall\": [0.51, 0.84, None, 0.68],\n    \"F1-Score\": [0.61, 0.73, 0.68, 0.67],\n    \"Support\": [43, 44, 87, 87]\n}\n\n# Create the DataFrame for the Decision Tree binary report\ndf_decision_tree_binary = pd.DataFrame(data_decision_tree_binary)\n\n# Display the DataFrame\nprint(df_decision_tree_binary)\n\n\n       Class  Precision  Recall  F1-Score  Support\n0          0       0.76    0.51      0.61       43\n1          1       0.64    0.84      0.73       44\n2   accuracy        NaN     NaN      0.68       87\n3  macro avg       0.70    0.68      0.67       87\n\n\n\nPrecision\nThe accuracy metrics for our model showed that precision scores were moderately high but varied, with values of 76% for Low Voting Rate and 76% for High Voting Rate. These two scores were also lower than Binary Classification with Log Regression.\n\n\nRecall\nRecall scores showcased the same pattern of varied values for different classes, with 84% for High Voting Rate and 51% for Low Voting Rate.\n- This indicates that the model captured most true positives for High Voting Rates but struggled with Low Voting Rates.\n\n\nF1\nThe F1 score was 73% for High Voting Rate, reflecting a pretty good balance between Precision and Recall.\n- The overall accuracy was 68%, showcasing a model that is moderately successful in distinguishing between two classes.\n- These values were also lower than Binary Classification with Log Regression.\n\n\nConfusion Matrix\nThe Confusion Matrix shows the results of the Log Regression Binary Classification model:\n\nThe model correctly predicted 37 values of ‘High Voting Rate’ (True Positives).\n\nThe model correctly predicted 22 values of ‘Low Voting Rate’ (True Negatives).\n\nThe model incorrectly classified 21 values as ‘High Voting Rate’ when they were actually low (False Positive).\n\nThe model incorrectly classified 7 values as ‘Low Voting Rate’ when they were actually high (False Negative).\n\n\n\nDecision Tree\n\n\nCode\nfrom sklearn.tree import plot_tree\n\n# Visualize the Decision Tree\nplt.figure(figsize=(32, 15))  # Adjust size for better readability\nplot_tree(\n    dt_clf, \n    feature_names=X_train.columns,  # Use feature names from your dataset\n    class_names=['Low', 'High'],   # Binary class names\n    filled=True,                   # Fill nodes with colors\n    rounded=True,                  # Rounded corners for better visualization\n    fontsize=10                    # Adjust font size for readability\n)\nplt.title(\"Binary Decision Tree Visualization\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe first split is based on if Bachelors_Degree_or_Higher&lt; -0.719, with a Gini index of 0.5. This indicates an impure split, and we can see that with the almost perfectly symmetric data values.\nIf the data point is less than -0.719, it goes left, if it is more than -0.719, it goes right. It then gets asked another criteria, and the data splits.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#random-forest-binary-classification",
    "href": "projects/DS5000/supervised-learning/main.html#random-forest-binary-classification",
    "title": "Supervised Learning",
    "section": "Random Forest Binary Classification",
    "text": "Random Forest Binary Classification\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define the Random Forest model\nrf_clf = RandomForestClassifier(\n    n_estimators=100,  # Number of trees\n    max_depth=10,      # Maximum depth of trees (to avoid overfitting)\n    random_state=42,   # For reproducibility\n    class_weight='balanced'  # Handle any imbalance in data\n)\n\n# Train the model\nrf_clf.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = rf_clf.predict(X_train)  # Predict on training data\ny_pred_test = rf_clf.predict(X_test)  # Predict on test data\ny_prob_test = rf_clf.predict_proba(X_test)[:, 1]  # Probabilities for ROC curve\n\n# Evaluate the model\nprint(\"Random Forest - Classification Report (Test Data):\")\nprint(classification_report(y_test, y_pred_test))\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred_test)\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\nplt.title(\"Random Forest - Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n\nRandom Forest - Classification Report (Test Data):\n              precision    recall  f1-score   support\n\n           0       0.87      0.79      0.83        43\n           1       0.81      0.89      0.85        44\n\n    accuracy                           0.84        87\n   macro avg       0.84      0.84      0.84        87\nweighted avg       0.84      0.84      0.84        87",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#random-forest-binary-classification-feature-importance",
    "href": "projects/DS5000/supervised-learning/main.html#random-forest-binary-classification-feature-importance",
    "title": "Supervised Learning",
    "section": "Random Forest Binary Classification Feature Importance",
    "text": "Random Forest Binary Classification Feature Importance\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Access feature importance\nfeature_importances = rf_clf.feature_importances_\n\n# Combine with feature names\nimportance_df = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': feature_importances\n}).sort_values(by='Importance', ascending=False)\n\n# Plot feature importance\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['Feature'], importance_df['Importance'])\nplt.xlabel(\"Feature Importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"Random Forest Feature Importance\")\nplt.gca().invert_yaxis()  # Reverse order for better readability\nplt.show()\n\n# Display top predictors\nprint(importance_df.head(10))\n\n\n\n\n\n\n\n\n\n                                   Feature  Importance\n8               Bachelors_Degree_or_Higher    0.131918\n5                      Below_Poverty_Level    0.116159\n10                Households_with_Internet    0.105330\n6              Log_Median_Household_Income    0.102438\n9                Households_with_Computers    0.091314\n13                              Median_Age    0.060863\n1                     Log_African_American    0.056157\n4                                 Veterans    0.053923\n14  citizen_voting_age_population_estimate    0.051646\n7                     High_School_Graduate    0.046506\n\n\nThe Random Forest feature importance quantifies how much each feature contributed to the random forest. Each value is computed by how much each feature reduced the Gini score (error).\nFor Binary Classification, the Random Forest deemed Bachelors Degree or Higher, Poverty Level, Internet Accecss, and Median Household Income as the most defining features in making splits.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#random-forest-binary-classification-accuracy-analysis",
    "href": "projects/DS5000/supervised-learning/main.html#random-forest-binary-classification-accuracy-analysis",
    "title": "Supervised Learning",
    "section": "Random Forest Binary Classification Accuracy Analysis",
    "text": "Random Forest Binary Classification Accuracy Analysis\n\n\nCode\ndata_random_forest_updated = {\n    \"Class\": [0, 1, \"accuracy\", \"macro avg\"],\n    \"Precision\": [0.87, 0.81, None, 0.84],\n    \"Recall\": [0.79, 0.89, None, 0.84],\n    \"F1-Score\": [0.83, 0.85, 0.84, 0.84],\n    \"Support\": [43, 44, 87, 87]\n}\n\n# Create the DataFrame for the updated Random Forest report\ndf_random_forest_binary = pd.DataFrame(data_random_forest_updated)\n\nprint(df_random_forest_binary)\n# Display the DataFrame\n\n\n       Class  Precision  Recall  F1-Score  Support\n0          0       0.87    0.79      0.83       43\n1          1       0.81    0.89      0.85       44\n2   accuracy        NaN     NaN      0.84       87\n3  macro avg       0.84    0.84      0.84       87\n\n\n\nPrecision\nThe accuracy metrics for our model showed that precision scores were moderately high but varied, with values of:\n- 76% for Low Voting Rate\n- 76% for High Voting Rate\nThese two scores were also lower than Binary Classification with Log Regression.\n\n\n\nRecall\nRecall scores showcased the same pattern of varied values for different classes:\n- 84% for High Voting Rate\n- 51% for Low Voting Rate\nThis indicates that the model captured most true positives for High Voting Rates but struggled with Low Voting Rates.\n\n\n\nF1\nThe F1 score was 73% for High Voting Rate, reflecting a pretty good balance between Precision and Recall.\n- The overall accuracy was 68%, showcasing a model that is moderately successful in distinguishing between two classes.\n- These values were also lower than Binary Classification with Log Regression.\n\n\n\nConfusion Matrix\nThe Confusion Matrix shows the results of the Random Forest Binary Classification model:\n- The model correctly predicted 39 values of ‘High Voting Rate’ (True Positives).\n- The model correctly predicted 34 values of ‘Low Voting Rate’ (True Negatives).\n- The model incorrectly classified 9 values as ‘High Voting Rate’ when they were actually low (False Positive).\n- The model incorrectly classified 5 values as ‘Low Voting Rate’ when they were actually high (False Negative).",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#target-selection-1",
    "href": "projects/DS5000/supervised-learning/main.html#target-selection-1",
    "title": "Supervised Learning",
    "section": "Target Selection",
    "text": "Target Selection\nI chose my Binary Classification targets as:\n- Low Voting Rate\n- Medium Voting Rate\n- High Voting Rate\nThe data was split into three quantiles:\n- Districts in the upper quartile were labeled as High Voting Rate.\n- Districts in the middle quantile were labeled as Medium Voting Rate.\n- Districts in the lower quantile were labeled as Low Voting Rate.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#log-regression-multiclass-classification",
    "href": "projects/DS5000/supervised-learning/main.html#log-regression-multiclass-classification",
    "title": "Supervised Learning",
    "section": "Log Regression Multiclass Classification",
    "text": "Log Regression Multiclass Classification\n\n\nCode\npercentiles = processed_df['voting_rate_estimate'].quantile([0.33, 0.66])\n\n# Create a new multiclass target column\ndef classify_voting_rate(voting_rate):\n    if voting_rate &lt;= percentiles[0.33]:\n        return 'low'\n    elif voting_rate &lt;= percentiles[0.66]:\n        return 'median'\n    else:\n        return 'high'\n\nprocessed_df['Voting_Rate_Category'] = processed_df['voting_rate_estimate'].apply(classify_voting_rate)\n\n# print(processed_df['Voting_Rate_Category'].value_counts())\n\n\n# Define target and features\nmulticlass_target = 'Voting_Rate_Category'\nfeatures = processed_df.drop(columns=['Voting_Rate_Category', 'voting_rate_estimate', 'High_Voting_Rate'])\n\n# Encode target variable into numeric values\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nprocessed_df['Voting_Rate_Category_Encoded'] = label_encoder.fit_transform(processed_df[multiclass_target])\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    features, \n    processed_df['Voting_Rate_Category_Encoded'], \n    test_size=0.2, \n    random_state=42\n)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Initialize the Logistic Regression model with multinomial option\nlog_reg_multi = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n\n# Train the model\nlog_reg_multi.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = log_reg_multi.predict(X_train)\ny_pred_test = log_reg_multi.predict(X_test)\n\n# Evaluate the model\nprint(\"Log Regression - Classification Report (Test Data):\")\nprint(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_))\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred_test)\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n\nLog Regression - Classification Report (Test Data):\n              precision    recall  f1-score   support\n\n        high       0.76      0.81      0.78        31\n         low       0.83      0.67      0.74        30\n      median       0.47      0.54      0.50        26\n\n    accuracy                           0.68        87\n   macro avg       0.69      0.67      0.67        87\nweighted avg       0.70      0.68      0.68        87\n\n\n\n/Users/iphone10/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#log-regression-multiclass-classification-analysis",
    "href": "projects/DS5000/supervised-learning/main.html#log-regression-multiclass-classification-analysis",
    "title": "Supervised Learning",
    "section": "Log Regression Multiclass Classification Analysis",
    "text": "Log Regression Multiclass Classification Analysis\n\n\nCode\ndata_log_regression = {\n    \"Class\": [\"high\", \"low\", \"median\", \"accuracy\", \"macro avg\"],\n    \"Precision\": [0.76, 0.83, 0.47, None, 0.69],\n    \"Recall\": [0.81, 0.67, 0.54, None, 0.67],\n    \"F1-Score\": [0.78, 0.74, 0.50, 0.68, 0.67],\n    \"Support\": [31, 30, 26, 87, 87]\n}\n\n# Create the DataFrame for Logistic Regression\ndf_log_regression = pd.DataFrame(data_log_regression)\n\n# Display the DataFrame\nprint(df_log_regression)\n\n\n       Class  Precision  Recall  F1-Score  Support\n0       high       0.76    0.81      0.78       31\n1        low       0.83    0.67      0.74       30\n2     median       0.47    0.54      0.50       26\n3   accuracy        NaN     NaN      0.68       87\n4  macro avg       0.69    0.67      0.67       87\n\n\n\n\nHigh Voting Rates\nThe accuracy metrics for our model showed that for classification into High Voting Rates:\n- Precision: 0.76 → Of the instances predicted as ‘high,’ 76% were correct.\n- Recall: 0.81\n- F1 Score: 0.78\nThese metrics indicate our model was pretty accurate in predicting the “High Voting Rate” class.\n\n\n\nLow Voting Rates\nFor classification into Low Voting Rates, our model achieved:\n- Precision: 0.83\n- Recall: 0.67\n- F1 Score: 0.74\nSimilarly to “High Voting Rate,” these metrics indicate our model was accurate in predicting the “Low Voting Rate” class.\n\n\n\nMedian Voting Rates\nFor classification into Median Voting Rates, our model had:\n- Precision: 0.47\n- Recall: 0.54\n- F1 Score: 0.50\nThese metrics indicate our model had a more difficult time classifying “Median Voting Rates.”\n- The overall model accuracy was 0.68, indicating a moderately accurate model but with room for improvement.\n\n\n\nMacro Average\nThe Macro Average metrics were:\n- Precision: 0.69\n- Recall: 0.67\n- F1 Score: 0.67\nThese scores indicate that our model performed pretty well across classes, but was dragged down by the low accuracy scores in classifying “Median Voting Rates.”\n\n\n\nConfusion Matrix\nThe Confusion Matrix shows the results of the multiclass classification model:\n- Correctly predicted 25 values of ‘High Voting Rate’ (True Positives).\n- Correctly predicted 20 instances of ‘Low Voting Rate’ (True Positives).\n- Correctly predicted 14 instances of ‘Median Voting Rate.’\n\n\n\nROC Curve\nThe ROC Curve showcases an AUC score of 0.87, indicating an accurate model that distinguishes between Low and High voting rates.\n- The curve rises steeply, indicating the model quickly achieves a high True Positive Rate with a low False Positive Rate.\n- Furthermore, the curve is significantly higher than the diagonal (baseline) curve, showing that the model is better than a random classifier.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#decision-tree-multiclass-classification",
    "href": "projects/DS5000/supervised-learning/main.html#decision-tree-multiclass-classification",
    "title": "Supervised Learning",
    "section": "Decision Tree Multiclass Classification",
    "text": "Decision Tree Multiclass Classification\n\n\nCode\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Initialize Decision Tree Classifier\ndt_clf_multi = DecisionTreeClassifier(\n    criterion='entropy',    # Use 'entropy' to measure information gain\n    max_depth=5,            # Limit depth to prevent overfitting\n    min_samples_split=10,   # Minimum samples to split a node\n    min_samples_leaf=5,     # Minimum samples in each leaf node\n    random_state=42         # For reproducibility\n)\n\n# Train the model\ndt_clf_multi.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = dt_clf_multi.predict(X_train)\ny_pred_test = dt_clf_multi.predict(X_test)\n\n# Evaluate the model\nprint(\"Decision Tree - Classification Report (Test Data):\")\nprint(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_))\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred_test)\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\nplt.title(\"Decision Tree - Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n\nDecision Tree - Classification Report (Test Data):\n              precision    recall  f1-score   support\n\n        high       0.64      0.68      0.66        31\n         low       0.79      0.50      0.61        30\n      median       0.37      0.50      0.43        26\n\n    accuracy                           0.56        87\n   macro avg       0.60      0.56      0.56        87\nweighted avg       0.61      0.56      0.57        87",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#decision-tree-multiclass-classification-analysis",
    "href": "projects/DS5000/supervised-learning/main.html#decision-tree-multiclass-classification-analysis",
    "title": "Supervised Learning",
    "section": "Decision Tree Multiclass Classification Analysis",
    "text": "Decision Tree Multiclass Classification Analysis\n\n\nCode\ndata_decision_tree = {\n    \"Class\": [\"high\", \"low\", \"median\", \"accuracy\", \"macro avg\"],\n    \"Precision\": [0.64, 0.79, 0.37, None, 0.60],\n    \"Recall\": [0.68, 0.50, 0.50, None, 0.56],\n    \"F1-Score\": [0.66, 0.61, 0.43, 0.56, 0.56],\n    \"Support\": [31, 30, 26, 87, 87]\n}\n\n# Create the DataFrame for Decision Tree\ndf_decision_tree = pd.DataFrame(data_decision_tree)\n\nprint(df_decision_tree)\n\n\n       Class  Precision  Recall  F1-Score  Support\n0       high       0.64    0.68      0.66       31\n1        low       0.79    0.50      0.61       30\n2     median       0.37    0.50      0.43       26\n3   accuracy        NaN     NaN      0.56       87\n4  macro avg       0.60    0.56      0.56       87\n\n\n\nHigh Voting Rates\nThe accuracy metrics for our model showed that for classification into High Voting Rates:\n- Precision: 0.64 → Of the instances predicted as ‘high,’ 64% were correct.\n- Recall: 0.68\n- F1 Score: 0.66\nThese metrics indicate our model was moderately accurate in predicting the “High Voting Rate” class.\n\n\n\nLow Voting Rates\nFor classification into Low Voting Rates, our model achieved:\n- Precision: 0.79\n- Recall: 0.50\n- F1 Score: 0.71\nSimilarly to “High Voting Rate,” these metrics indicate our model was accurate in predicting the “Low Voting Rate” class.\n\n\n\nMedian Voting Rates\nFor classification into Median Voting Rates, our model only achieved:\n- Precision: 0.37\n- Recall: 0.50\n- F1 Score: 0.61\nThese metrics indicate our model had a difficult time classifying “Median Voting Rates.”\n\n\n\nAccuracy\nThe model had an accuracy of 0.56, indicating a moderately accurate model but with room for improvement.\n\n\n\nMacro Average\nThe Macro Average metrics were:\n- Precision: 0.60\n- Recall: 0.56\n- F1 Score: 0.56\nThese scores indicate that our model performed a little better than average in accuracy metrics across classes. However, similar to earlier, performance was dragged down by the low accuracy scores in classifying “Median Voting Rates.”\n\n\n\nConfusion Matrix\nThe Confusion Matrix shows the results of the multiclass classification model:\n- Correctly predicted 21 values of ‘High Voting Rate’ (True Positives).\n- Correctly predicted 15 instances of ‘Low Voting Rate’ (True Positives).\n- Correctly predicted 13 instances of ‘Median Voting Rate.’",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#decision-tree-2",
    "href": "projects/DS5000/supervised-learning/main.html#decision-tree-2",
    "title": "Supervised Learning",
    "section": "Decision Tree",
    "text": "Decision Tree\n\n\nCode\nfrom sklearn.tree import plot_tree\n\nplt.figure(figsize=(20, 10))\nplot_tree(\n    dt_clf_multi, \n    feature_names=X_train.columns, \n    class_names=label_encoder.classes_, \n    filled=True, \n    rounded=True\n)\nplt.show()",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#random-forest-multiclass-classification",
    "href": "projects/DS5000/supervised-learning/main.html#random-forest-multiclass-classification",
    "title": "Supervised Learning",
    "section": "Random Forest Multiclass Classification",
    "text": "Random Forest Multiclass Classification\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize Random Forest Classifier\nrf_clf_multi = RandomForestClassifier(\n    n_estimators=100,       # Number of trees\n    max_depth=10,           # Maximum depth of trees\n    min_samples_split=10,   # Minimum samples to split a node\n    min_samples_leaf=5,     # Minimum samples in each leaf node\n    random_state=42         # For reproducibility\n)\n\n# Train the model\nrf_clf_multi.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = rf_clf_multi.predict(X_train)\ny_pred_test = rf_clf_multi.predict(X_test)\n\n# Evaluate the model\nprint(\"Random Forest - Classification Report (Test Data):\")\nprint(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_))\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred_test)\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\nplt.title(\"Random Forest - Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n\nRandom Forest - Classification Report (Test Data):\n              precision    recall  f1-score   support\n\n        high       0.68      0.81      0.74        31\n         low       0.82      0.60      0.69        30\n      median       0.39      0.42      0.41        26\n\n    accuracy                           0.62        87\n   macro avg       0.63      0.61      0.61        87\nweighted avg       0.64      0.62      0.62        87",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#random-forest-multiclass-classification-feature-importance",
    "href": "projects/DS5000/supervised-learning/main.html#random-forest-multiclass-classification-feature-importance",
    "title": "Supervised Learning",
    "section": "Random Forest Multiclass Classification Feature Importance",
    "text": "Random Forest Multiclass Classification Feature Importance\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Access feature importance\nfeature_importances = rf_clf_multi.feature_importances_\n\n# Combine with feature names\nimportance_df = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': feature_importances\n}).sort_values(by='Importance', ascending=False)\n\n# Plot feature importance\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['Feature'], importance_df['Importance'])\nplt.xlabel(\"Feature Importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"Random Forest Feature Importance\")\nplt.gca().invert_yaxis()  # Reverse order for better readability\nplt.show()\n\n# Display top predictors\nprint(importance_df.head(10))\n\n\n\n\n\n\n\n\n\n                        Feature  Importance\n8    Bachelors_Degree_or_Higher    0.143876\n5           Below_Poverty_Level    0.131789\n10     Households_with_Internet    0.114182\n9     Households_with_Computers    0.092793\n6   Log_Median_Household_Income    0.089420\n13                   Median_Age    0.077783\n4                      Veterans    0.048003\n7          High_School_Graduate    0.045844\n2                         White    0.043331\n3                     Log_Asian    0.042718\n\n\nFor Multiclass Classification, the Random Forest deemed Bachelors Degree or Higher, Poverty Level, Internet Access, Households with Computers, Median Household Income as the most defining features in making splits.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#random-forest-multiclass-classification-analysis",
    "href": "projects/DS5000/supervised-learning/main.html#random-forest-multiclass-classification-analysis",
    "title": "Supervised Learning",
    "section": "Random Forest Multiclass Classification Analysis",
    "text": "Random Forest Multiclass Classification Analysis\n\n\nCode\nimport pandas as pd\n\n# Data for the classification report\nrandom_forest_metrics = {\n    \"Class\": [\"high\", \"low\", \"median\", \"accuracy\", \"macro avg\"],\n    \"Precision\": [0.68, 0.82, 0.39, None, 0.63],\n    \"Recall\": [0.81, 0.60, 0.42, None, 0.61],\n    \"F1-Score\": [0.74, 0.69, 0.41, 0.62, 0.61],\n    \"Support\": [31, 30, 26, 87, 87]\n}\n\n\nrandom_forest_multi_df = pd.DataFrame(random_forest_metrics)\n\nprint(random_forest_multi_df)\n\n\n       Class  Precision  Recall  F1-Score  Support\n0       high       0.68    0.81      0.74       31\n1        low       0.82    0.60      0.69       30\n2     median       0.39    0.42      0.41       26\n3   accuracy        NaN     NaN      0.62       87\n4  macro avg       0.63    0.61      0.61       87\n\n\n\n\nHigh Voting Rates\nThe accuracy metrics for our model showed that for classification into High Voting Rates:\n- Precision: 0.68 → Of the instances predicted as ‘high,’ 68% were correct.\n- Recall: 0.81\n- F1 Score: 0.74\nThis indicates our model was reasonably accurate in predicting the “High Voting Rate” class.\n\n\n\nLow Voting Rates\nFor classification into Low Voting Rates, our model achieved:\n- Precision: 0.82\n- Recall: 0.60\n- F1 Score: 0.69\nThese metrics suggest the model performed well in predicting the “Low Voting Rate” class, but with slightly lower recall.\n\n\n\nMedian Voting Rates\nFor classification into Median Voting Rates, our model had:\n- Precision: 0.39\n- Recall: 0.42\n- F1 Score: 0.41\nThese scores indicate that the model had a difficult time classifying “Median Voting Rates”, which affected overall performance.\n\n\n\nAccuracy\nThe overall model accuracy was 0.62, indicating a moderately accurate model but with room for improvement, particularly for the Median Voting Rate class.\n\n\n\nMacro Average\nThe Macro Average metrics were:\n- Precision: 0.63\n- Recall: 0.61\n- F1 Score: 0.61\nThese scores show that the model performed better overall for High and Low classes, but struggled with the Median class, lowering the average scores.\n\n\n\nConfusion Matrix\nThe Confusion Matrix shows the results of the multiclass classification model:\n- Correctly predicted 25 values of ‘High Voting Rate’ (True Positives).\n- Correctly predicted 18 instances of ‘Low Voting Rate’ (True Positives).\n- Correctly predicted 11 instances of ‘Median Voting Rate.’",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#multiclass-classification-analysis-and-comparison",
    "href": "projects/DS5000/supervised-learning/main.html#multiclass-classification-analysis-and-comparison",
    "title": "Supervised Learning",
    "section": "Multiclass Classification Analysis and Comparison",
    "text": "Multiclass Classification Analysis and Comparison\n\n\nCode\nimport pandas as pd\n\n# Define the data\ndata = {\n    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest'],\n    'High Precision': [0.76, 0.64, 0.68],\n    'High Recall': [0.81, 0.68, 0.81],\n    'High F1-Score': [0.78, 0.66, 0.74],\n    'Low Precision': [0.83, 0.79, 0.82],\n    'Low Recall': [0.67, 0.50, 0.60],\n    'Low F1-Score': [0.74, 0.61, 0.69],\n    'Median Precision': [0.47, 0.37, 0.39],\n    'Median Recall': [0.54, 0.50, 0.42],\n    'Median F1-Score': [0.50, 0.43, 0.41],\n    'Accuracy': [0.68, 0.56, 0.62],\n    'Macro Avg Precision': [0.69, 0.60, 0.63],\n    'Macro Avg Recall': [0.67, 0.56, 0.61],\n    'Macro Avg F1-Score': [0.67, 0.56, 0.61]\n}\n\n# Create a DataFrame\nresults_df = pd.DataFrame(data)\n\n# Display table in a nice format\nfrom IPython.display import display\ndisplay(results_df)\n\n\n\n\n\n\n\n\n\nModel\nHigh Precision\nHigh Recall\nHigh F1-Score\nLow Precision\nLow Recall\nLow F1-Score\nMedian Precision\nMedian Recall\nMedian F1-Score\nAccuracy\nMacro Avg Precision\nMacro Avg Recall\nMacro Avg F1-Score\n\n\n\n\n0\nLogistic Regression\n0.76\n0.81\n0.78\n0.83\n0.67\n0.74\n0.47\n0.54\n0.50\n0.68\n0.69\n0.67\n0.67\n\n\n1\nDecision Tree\n0.64\n0.68\n0.66\n0.79\n0.50\n0.61\n0.37\n0.50\n0.43\n0.56\n0.60\n0.56\n0.56\n\n\n2\nRandom Forest\n0.68\n0.81\n0.74\n0.82\n0.60\n0.69\n0.39\n0.42\n0.41\n0.62\n0.63\n0.61\n0.61",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#supervised-learning",
    "href": "projects/DS5000/supervised-learning/main.html#supervised-learning",
    "title": "Supervised Learning",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nRegression\n\nThe model predicted voting rate estimates with moderate accuracy.\n\nRMSE and R² values indicated the model’s ability to minimize errors and explain trends in the data.\n\n\n\nBinary Classification\n\nLogistic Regression and Random Forest outperformed Decision Tree in distinguishing between High and Low Voting Rates.\n\nTradeoffs were observed between Precision, Recall, and F1 scores across the models.\n\n\n\nMulticlass Classification\n\nAll models struggled to classify Median Voting Rates, which significantly impacted overall performance.\n\nLogistic Regression performed the best with a Macro Average Precision of 0.69, followed by Random Forest with 0.63.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#key-takeaways",
    "href": "projects/DS5000/supervised-learning/main.html#key-takeaways",
    "title": "Supervised Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nLogistic Regression\n\nHad the highest accuracy for both Regression and Classification.\n\nLikely due to its simplicity and interpretability.\n\nDecision Tree\n\nAchieved the least accurate metrics for Regression and Classification.\n\nThis was likely due to overfitting, a common challenge with Decision Trees.\n\nRandom Forest\n\nProvided accurate results, outperforming Decision Tree.\n\nThis was likely due to reduced overfitting through its ensemble approach.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/supervised-learning/main.html#next-steps",
    "href": "projects/DS5000/supervised-learning/main.html#next-steps",
    "title": "Supervised Learning",
    "section": "Next Steps",
    "text": "Next Steps\n\nThe model needs further fine-tuning to increase accuracy.\n\nFor instance, Multiclass Classification struggled with the Median class, scoring poorly.\n\nIntroducing methods like feature engineering or hyperparameter optimization may improve performance.\n\nOnce refined, these models can be utilized for real-world applications, providing policymakers with informative data on voting rates and patterns to guide better decision-making.",
    "crumbs": [
      "About",
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-collection/main.html",
    "href": "projects/DS5000/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "Data collection is the first step in processing data into actionable insights. Data collection involves identifying data sources, selecting relevant data, and retrieving this data. Without doing these first steps carefully, further data analysis would be based on incomplete, irrelevant, or inaccurate data. My goal for data collection was to ensure I used credible sources, obtained all necessary variables, and gathered data in a format that was consistent, organized, and ready for the next stages of data cleaning.",
    "crumbs": [
      "About",
      "Technical details",
      "Data Collection"
    ]
  },
  {
    "objectID": "projects/DS5000/data-collection/main.html#pull-request-for-census-data",
    "href": "projects/DS5000/data-collection/main.html#pull-request-for-census-data",
    "title": "Data Collection",
    "section": "Pull Request for Census Data",
    "text": "Pull Request for Census Data\n\n\nCode\nimport requests\nimport pandas as pd\n\n\nurl = (\n    \"https://api.census.gov/data/2018/acs/acsse?\"\n    \"get=NAME,K200101_001E,K200201_003E,K200201_002E,K200201_005E,K202101_002E,\"\n    \"K202101_006E,K201701_002E,K201902_001E,K201501_004E,K201501_007E,K202801_002E,\"\n    \"K202801_004E,K200201_004E,K200201_006E,K200103_001E&for=congressional%20district:*\"\n)\n\n\nresponse = requests.get(url)\n\n\ndata = response.json()\ndf_raw = pd.DataFrame(data[1:], columns=data[0])\noutput_path = \"../../data/raw-data/census_raw_data.csv\"\ndf_raw.to_csv(output_path, index=False)\nprint(f\"CSV file saved to {output_path}\")\ndf_raw.head()\n\n\nCSV file saved to ../../data/raw-data/census_raw_data.csv\n\n\n\n\n\n\n\n\n\nNAME\nK200101_001E\nK200201_003E\nK200201_002E\nK200201_005E\nK202101_002E\nK202101_006E\nK201701_002E\nK201902_001E\nK201501_004E\nK201501_007E\nK202801_002E\nK202801_004E\nK200201_004E\nK200201_006E\nK200103_001E\nstate\ncongressional district\n\n\n\n\n0\nCongressional District 1 (116th Congress), Ala...\n715346\n196891\n479949\n10627\n51744\n499929\n118962\n46445\n163604\n79243\n239264\n209561\n6992\n183\n40.3\n01\n01\n\n\n1\nCongressional District 3 (116th Congress), Ala...\n708409\n183949\n490987\n13049\n51572\n500213\n124289\n45832\n150206\n61435\n233058\n204349\n1300\n0\n39.2\n01\n03\n\n\n2\nCongressional District 5 (116th Congress), Ala...\n725634\n130913\n539903\n12215\n55771\n510612\n91640\n57174\n135017\n108527\n262575\n240933\n3123\n160\n39.5\n01\n05\n\n\n3\nCongressional District 4 (116th Congress), Ala...\n686297\n51171\n591084\n3072\n39104\n491300\n119113\n45387\n158561\n51598\n217239\n194848\n4391\n677\n40.5\n01\n04\n\n\n4\nCongressional District 7 (116th Congress), Ala...\n660468\n416885\n221541\n5007\n33737\n481571\n156399\n37123\n146697\n56008\n203824\n179334\n1907\n206\n36.4\n01\n07",
    "crumbs": [
      "About",
      "Technical details",
      "Data Collection"
    ]
  },
  {
    "objectID": "projects/DS5000/data-collection/main.html#loading-in-voting-data",
    "href": "projects/DS5000/data-collection/main.html#loading-in-voting-data",
    "title": "Data Collection",
    "section": "Loading in Voting Data",
    "text": "Loading in Voting Data\n\n\nCode\nfile_path = \"../../data/raw-data/Votes2018.xlsx\"\nvote_df = pd.read_excel(file_path)\nvote_df.head()\n\n\n/Users/iphone10/miniconda3/lib/python3.10/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n\n\n\n\n\n\n\n\n\nTable 1. Number of Votes Cast, Citizen Voting-Age Population and Voting Rates for Congressional Districts: 2018\nUnnamed: 1\nUnnamed: 2\nUnnamed: 3\nUnnamed: 4\nUnnamed: 5\nUnnamed: 6\nUnnamed: 7\nUnnamed: 8\n\n\n\n\n0\n[Table with column headings in rows 3 and 4, a...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nLine #\nState abbreviation\nState name\nCongressional district\nVotes cast for congressional representative fo...\nCitizen voting- age population2\nNaN\nVoting rate3\nNaN\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nEstimate\nMargin of error (MOE)\nEstimate\nMargin of error (MOE)\n\n\n3\n1\nAL\nAlabama\n1\n242617\n544464\n3424\n44.560706\n0.280231\n\n\n4\n2\nAL\nAlabama\n2\n226230\n516295\n5674\n43.817972\n0.481553",
    "crumbs": [
      "About",
      "Technical details",
      "Data Collection"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html",
    "href": "projects/DS5000/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Data cleaning is the next step in processing data into actionable insights. Data cleaning involves removing NaNs, removing outliers, renaming columns, and applying transformations. Without performing such actions, we would not be able to draw any conclusions from our data as we will be analyzing inconsistent and incompatible data. My goal for data cleaning was to make sure my data had consistent, easy to read column names and row values, consistently formatted data, and standardization.\n\n\nStandardizaiton, also known as Z-Score Normalization is needed when a distribution is going to be used in a K-Means clustering, PCA, or , as it minimizes the amount of bias larger features get. Z-Score Standardization alters the data set so each variable will have a mean of 0, and a standard deviation of 1. This is done by subtracting the mean by every data point, and dividing by the standard deviation. After this process, data is ready to be further analyzed in clustering and dimensional reduction.\n\\[z = \\frac{x - \\mu}{\\sigma}\\]",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html#introduction",
    "href": "projects/DS5000/data-cleaning/main.html#introduction",
    "title": "Data Cleaning",
    "section": "",
    "text": "Data cleaning is the next step in processing data into actionable insights. Data cleaning involves removing NaNs, removing outliers, renaming columns, and applying transformations. Without performing such actions, we would not be able to draw any conclusions from our data as we will be analyzing inconsistent and incompatible data. My goal for data cleaning was to make sure my data had consistent, easy to read column names and row values, consistently formatted data, and standardization.\n\n\nStandardizaiton, also known as Z-Score Normalization is needed when a distribution is going to be used in a K-Means clustering, PCA, or , as it minimizes the amount of bias larger features get. Z-Score Standardization alters the data set so each variable will have a mean of 0, and a standard deviation of 1. This is done by subtracting the mean by every data point, and dividing by the standard deviation. After this process, data is ready to be further analyzed in clustering and dimensional reduction.\n\\[z = \\frac{x - \\mu}{\\sigma}\\]",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html#data-cleaning-census-data",
    "href": "projects/DS5000/data-cleaning/main.html#data-cleaning-census-data",
    "title": "Data Cleaning",
    "section": "Data Cleaning: Census Data",
    "text": "Data Cleaning: Census Data\nAfter obtaining the data for census_df, it was time to clean. The first thing I did was check for NA values, and found there were none. Next, I chose to perform column mapping on my dataset, as the current columns were code names such as K202801_004E and were confusing to understand. After cleaning the column names, the next step in data cleaning was to fix the state column. Currently this held the state FIPS identifier, with 1 for Alabama, 2 for Alaska, etc. I chose to map each FIPS code to the state name for easier identification and interpretability.\n\n\nCode\nimport pandas as pd\n\ncensus_df = pd.read_csv('../../data/raw-data/census_raw_data.csv') \n\n\n\n\nCode\ncolumn_mapping = {\n    \"K200101_001E\": \"Total_Population\",\n    \"K200201_002E\": \"White\",\n    \"K200201_003E\": \"African_American\",\n    \"K200201_005E\": \"Asian\",\n    \"K200201_004E\": \"American_Indian_and_Alaska_Native\",\n    \"K200201_006E\": \"Native_Hawaiian_and_Pacific_Islander\",\n    \"K202101_002E\": \"Veterans\",\n    \"K202101_006E\": \"Nonveterans\",\n    \"K201701_002E\": \"Below_Poverty_Level\",\n    \"K201902_001E\": \"Median_Household_Income\",\n    \"K201501_004E\": \"High_School_Graduate\",\n    \"K201501_007E\": \"Bachelors_Degree_or_Higher\",\n    \"K202801_002E\": \"Households_with_Computers\",\n    \"K202801_004E\": \"Households_with_Internet\",\n    \"K200103_001E\": \"Median_Age\",\n    \"NAME\": \"District_Name\", \n    \"congressional district\": \"Congressional_District\"\n}\n\ncensus_df.rename(columns=column_mapping, inplace=True)\n\nfips_to_state = {\n    1: \"Alabama\",\n    2: \"Alaska\",\n    4: \"Arizona\",\n    5: \"Arkansas\",\n    6: \"California\",\n    8: \"Colorado\",\n    9: \"Connecticut\",\n    10: \"Delaware\",\n    11: \"District of Columbia\",\n    12: \"Florida\",\n    13: \"Georgia\",\n    15: \"Hawaii\",\n    16: \"Idaho\",\n    17: \"Illinois\",\n    18: \"Indiana\",\n    19: \"Iowa\",\n    20: \"Kansas\",\n    21: \"Kentucky\",\n    22: \"Louisiana\",\n    23: \"Maine\",\n    24: \"Maryland\",\n    25: \"Massachusetts\",\n    26: \"Michigan\",\n    27: \"Minnesota\",\n    28: \"Mississippi\",\n    29: \"Missouri\",\n    30: \"Montana\",\n    31: \"Nebraska\",\n    32: \"Nevada\",\n    33: \"New Hampshire\",\n    34: \"New Jersey\",\n    35: \"New Mexico\",\n    36: \"New York\",\n    37: \"North Carolina\",\n    38: \"North Dakota\",\n    39: \"Ohio\",\n    40: \"Oklahoma\",\n    41: \"Oregon\",\n    42: \"Pennsylvania\",\n    44: \"Rhode Island\",\n    45: \"South Carolina\",\n    46: \"South Dakota\",\n    47: \"Tennessee\",\n    48: \"Texas\",\n    49: \"Utah\",\n    50: \"Vermont\",\n    51: \"Virginia\",\n    53: \"Washington\",\n    54: \"West Virginia\",\n    55: \"Wisconsin\",\n    56: \"Wyoming\",\n    60: \"American Samoa\",\n    66: \"Guam\",\n    69: \"Northern Mariana Islands\",\n    72: \"Puerto Rico\",\n    78: \"Virgin Islands\"\n}\n# Assuming your dataset has a 'FIPS' column\ncensus_df['state'] = census_df['state'].map(fips_to_state)\n\n# Drop specific columns by name\ncolumns_to_drop = ['District_Name']  \ncensus_df = census_df.drop(columns=columns_to_drop)",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html#census-dataframe-first-5-rows",
    "href": "projects/DS5000/data-cleaning/main.html#census-dataframe-first-5-rows",
    "title": "Data Cleaning",
    "section": "Census DataFrame First 5 Rows",
    "text": "Census DataFrame First 5 Rows\n\n\nCode\nprint(census_df.head())\n\n\n   Total_Population  African_American   White  Asian  Veterans  Nonveterans  \\\n0            715346            196891  479949  10627     51744       499929   \n1            708409            183949  490987  13049     51572       500213   \n2            725634            130913  539903  12215     55771       510612   \n3            686297             51171  591084   3072     39104       491300   \n4            660468            416885  221541   5007     33737       481571   \n\n   Below_Poverty_Level  Median_Household_Income  High_School_Graduate  \\\n0               118962                    46445                163604   \n1               124289                    45832                150206   \n2                91640                    57174                135017   \n3               119113                    45387                158561   \n4               156399                    37123                146697   \n\n   Bachelors_Degree_or_Higher  Households_with_Computers  \\\n0                       79243                     239264   \n1                       61435                     233058   \n2                      108527                     262575   \n3                       51598                     217239   \n4                       56008                     203824   \n\n   Households_with_Internet  American_Indian_and_Alaska_Native  \\\n0                    209561                               6992   \n1                    204349                               1300   \n2                    240933                               3123   \n3                    194848                               4391   \n4                    179334                               1907   \n\n   Native_Hawaiian_and_Pacific_Islander  Median_Age    state  \\\n0                                   183        40.3  Alabama   \n1                                     0        39.2  Alabama   \n2                                   160        39.5  Alabama   \n3                                   677        40.5  Alabama   \n4                                   206        36.4  Alabama   \n\n   Congressional_District  \n0                       1  \n1                       3  \n2                       5  \n3                       4  \n4                       7",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html#checking-for-nas",
    "href": "projects/DS5000/data-cleaning/main.html#checking-for-nas",
    "title": "Data Cleaning",
    "section": "Checking for NAs",
    "text": "Checking for NAs\n\n\nCode\ncensus_df.isna().sum()\n\n\nTotal_Population                        0\nAfrican_American                        0\nWhite                                   0\nAsian                                   0\nVeterans                                0\nNonveterans                             0\nBelow_Poverty_Level                     0\nMedian_Household_Income                 0\nHigh_School_Graduate                    0\nBachelors_Degree_or_Higher              0\nHouseholds_with_Computers               0\nHouseholds_with_Internet                0\nAmerican_Indian_and_Alaska_Native       0\nNative_Hawaiian_and_Pacific_Islander    0\nMedian_Age                              0\nstate                                   0\nCongressional_District                  0\ndtype: int64",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html#column-names",
    "href": "projects/DS5000/data-cleaning/main.html#column-names",
    "title": "Data Cleaning",
    "section": "Column Names",
    "text": "Column Names\n\n\nCode\nprint(census_df.columns)\n\n\nIndex(['Total_Population', 'African_American', 'White', 'Asian', 'Veterans',\n       'Nonveterans', 'Below_Poverty_Level', 'Median_Household_Income',\n       'High_School_Graduate', 'Bachelors_Degree_or_Higher',\n       'Households_with_Computers', 'Households_with_Internet',\n       'American_Indian_and_Alaska_Native',\n       'Native_Hawaiian_and_Pacific_Islander', 'Median_Age', 'state',\n       'Congressional_District'],\n      dtype='object')",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html#voting-dataframe-first-5-rows",
    "href": "projects/DS5000/data-cleaning/main.html#voting-dataframe-first-5-rows",
    "title": "Data Cleaning",
    "section": "Voting DataFrame First 5 Rows",
    "text": "Voting DataFrame First 5 Rows\n\n\nCode\nvote_df.head()\n\n\n\n\n\n\n\n\n\nline_number\nstate_abbreviation\nstate\nCongressional_District\nvotes_cast\ncitizen_voting_age_population_estimate\ncitizen_voting_age_population_moe\nvoting_rate_estimate\nvoting_rate_moe\n\n\n\n\n0\n1\nAL\nAlabama\n1\n242617.0\n544464.0\n3424.0\n44.560706\n0.280231\n\n\n1\n2\nAL\nAlabama\n2\n226230.0\n516295.0\n5674.0\n43.817972\n0.481553\n\n\n2\n3\nAL\nAlabama\n3\n231915.0\n543854.0\n4099.0\n42.642878\n0.321397\n\n\n3\n4\nAL\nAlabama\n4\n230969.0\n515701.0\n4678.0\n44.787386\n0.406273\n\n\n4\n5\nAL\nAlabama\n5\n260673.0\n551968.0\n2121.0\n47.226107\n0.181472",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html#checking-for-nas-1",
    "href": "projects/DS5000/data-cleaning/main.html#checking-for-nas-1",
    "title": "Data Cleaning",
    "section": "Checking for NAs",
    "text": "Checking for NAs\n\n\nCode\n\nvote_df.isna().sum()\n\n\nline_number                                4\nstate_abbreviation                        11\nstate                                     11\nCongressional_District                    11\nvotes_cast                                11\ncitizen_voting_age_population_estimate    11\ncitizen_voting_age_population_moe         11\nvoting_rate_estimate                      11\nvoting_rate_moe                           11\ndtype: int64\n\n\n\n\nCode\nrows_with_na = vote_df[vote_df.isna().any(axis=1)]\n\n# Display rows with NA values\nprint((rows_with_na).head())\n\n\n                                           line_number state_abbreviation  \\\n436                                                NaN                NaN   \n437                                                NaN                NaN   \n438                                             Notes:                NaN   \n439  - Congressional districts 10, 14, 21, and 24 i...                NaN   \n440  1 Votes as collected by state election offices...                NaN   \n\n    state Congressional_District  votes_cast  \\\n436   NaN                    NaN         NaN   \n437   NaN                    NaN         NaN   \n438   NaN                    NaN         NaN   \n439   NaN                    NaN         NaN   \n440   NaN                    NaN         NaN   \n\n     citizen_voting_age_population_estimate  \\\n436                                     NaN   \n437                                     NaN   \n438                                     NaN   \n439                                     NaN   \n440                                     NaN   \n\n     citizen_voting_age_population_moe  voting_rate_estimate  voting_rate_moe  \n436                                NaN                   NaN              NaN  \n437                                NaN                   NaN              NaN  \n438                                NaN                   NaN              NaN  \n439                                NaN                   NaN              NaN  \n440                                NaN                   NaN              NaN  \n\n\nThe NA values in the data are due to metadata:\n\n“Florida: Congressional districts 10, 14, 21, and 24 did not report vote totals because state law prohibits unopposed candidates from appearing on the ballot.” (U.S. Census Bureau)\n“North Carolina: District 9 did not report vote totals due to a new election ordered by the state’s Board of Elections in February 2019.” ( U.S. Census Bureau)\n\n(Source: U.S. Census Bureau)\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Remove rows where 'votes_cast' is NaN\nvote_df = vote_df[vote_df['votes_cast'].notna()]\n\n# Ensure there are valid values to plot\n\nvote_df['votes_cast'].plot(kind='hist', bins=20, alpha=0.7)\nplt.title(\"Distribution of Votes Cast\")\nplt.xlabel(\"Votes Cast\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen plotting the data to check for outliers, there showed 5 districts where zero votes were cast.\nThis is what the metadata had stated, with districts 10, 14, 21, and 24 in Florida and district 9 in North Carolina not reporting votes.",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html#handling-missing-values",
    "href": "projects/DS5000/data-cleaning/main.html#handling-missing-values",
    "title": "Data Cleaning",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\n\nIn the process of cleaing, I noticed that states that only contained 1 Congressional District were labeled as “At-large” as opposed to 1, 2, etc, as seen with Delaware and District of Columbia . I utilized code to convert these into ‘1’ for data homogeneity.",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html#handling-outliers",
    "href": "projects/DS5000/data-cleaning/main.html#handling-outliers",
    "title": "Data Cleaning",
    "section": "Handling Outliers",
    "text": "Handling Outliers\nSimilar to the census_data, I believed that removing outliers could result in the loss of valuable insights. However, I felt it was necessary to examine the distribution of features that I believe are key contributors to Voting Rate.\nI also decided to drop the columns:\n- \"line_number\"\n- \"citizen_voting_age_population_moe\"\n- \"voting_rate_moe\"\nThese columns were removed as they may interfere with further analysis down the road.",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html#visual",
    "href": "projects/DS5000/data-cleaning/main.html#visual",
    "title": "Data Cleaning",
    "section": "Visual",
    "text": "Visual\n\nDistribution of Household Income\n\n\nCode\nimport matplotlib.pyplot as plt\ncensus_df.boxplot(column='Median_Household_Income', vert=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nDistribution of White\n\n\nCode\nimport matplotlib.pyplot as plt\ncensus_df.boxplot(column='White', vert=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nDistribution of Poverty Level\n\n\nCode\nimport matplotlib.pyplot as plt\ncensus_df.boxplot(column='Below_Poverty_Level', vert=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nDistribution of African American\n\n\nCode\nimport matplotlib.pyplot as plt\ncensus_df.boxplot(column='African_American', vert=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nAfter exploring the data through plots and statistics, I realized that although there were some variables that were skewed, it was not in best practice to completely omit them, as this could be a key indicator of voter turnout (Urban vs rural, socioeconomic standing, small districts vs big districts).",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html#standardization-1",
    "href": "projects/DS5000/data-cleaning/main.html#standardization-1",
    "title": "Data Cleaning",
    "section": "Standardization",
    "text": "Standardization\nI chose to use standardization instead of omitting outliers to prepare data for further analysis. I made a copy of the original data frame to maintain for testing and comparison, and then called StandardScalar from SKLearn to fit the numeric values of the dataset. After doing so, the mean of every variable is 0, with a standard deviation of 1.\nIn the Census Dataframe, I chose all columns that were numerical values. In the Votes Data frame, I chose all numerical values as well, including voting rate. Even though voting_rate already on a standardized scale (out of 100), to maintain data homogeneity and not let algorithms treat them differently I chose it was best to standardize this variable as well.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# Define columns to standardize\ncolumns_to_standardize = ['votes_cast', 'citizen_voting_age_population_estimate', 'voting_rate_estimate']\n\n# Select numerical columns for standardization\nnumerical_cols = ['Total_Population', 'African_American', 'White', 'Asian', 'Veterans', \n                  'Nonveterans', 'Below_Poverty_Level', 'High_School_Graduate', \n                  \"Bachelors_Degree_or_Higher\", \"Households_with_Computers\",\n                  \"Households_with_Internet\", \"American_Indian_and_Alaska_Native\", \n                  \"Native_Hawaiian_and_Pacific_Islander\", \"Median_Household_Income\", \"Median_Age\"]\n\n# Make a copy of census_df\ncensus_standard_df = census_df.copy()\n\n# Initialize StandardScaler\nscaler = StandardScaler()\n\n# Standardize the numerical columns\ncensus_standard_df[numerical_cols] = scaler.fit_transform(census_standard_df[numerical_cols])\n\n# Check the first few rows to confirm scaling\nprint(census_standard_df.head())\n\n# Initialize StandardScaler for vote_df\nscaler = StandardScaler()\n\nvote_standard_df = vote_df.copy()\n\nvote_standard_df[columns_to_standardize] = scaler.fit_transform(vote_standard_df[columns_to_standardize])\n\n\n   Total_Population  African_American     White     Asian  Veterans  \\\n0         -0.314384          0.970773 -0.432776 -0.601424  0.673281   \n1         -0.368058          0.846057 -0.359566 -0.555218  0.662215   \n2         -0.234783          0.334971 -0.035131 -0.571129  0.932348   \n3         -0.539144         -0.433469  0.304326 -0.745557 -0.139884   \n4         -0.738990          3.090763 -2.146664 -0.708642 -0.485158   \n\n   Nonveterans  Below_Poverty_Level  Median_Household_Income  \\\n0    -0.416259             0.287950                -1.002315   \n1    -0.413525             0.364380                -1.036189   \n2    -0.313431            -0.104054                -0.409433   \n3    -0.499317             0.290117                -1.060779   \n4    -0.592962             0.825080                -1.517445   \n\n   High_School_Graduate  Bachelors_Degree_or_Higher  \\\n0              0.624607                   -0.621344   \n1              0.288741                   -1.086251   \n2             -0.092022                    0.143164   \n3              0.498187                   -1.343062   \n4              0.200776                   -1.227932   \n\n   Households_with_Computers  Households_with_Internet  \\\n0                  -0.440646                 -0.695362   \n1                  -0.592771                 -0.832347   \n2                   0.130768                  0.129174   \n3                  -0.980536                 -1.082058   \n4                  -1.309372                 -1.489806   \n\n   American_Indian_and_Alaska_Native  Native_Hawaiian_and_Pacific_Islander  \\\n0                           0.037155                             -0.234352   \n1                          -0.332663                             -0.268624   \n2                          -0.214220                             -0.238660   \n3                          -0.131836                             -0.141838   \n4                          -0.293226                             -0.230045   \n\n   Median_Age    state  Congressional_District  \n0    0.464422  Alabama                       1  \n1    0.162491  Alabama                       3  \n2    0.244836  Alabama                       5  \n3    0.519318  Alabama                       4  \n4   -0.606060  Alabama                       7",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/data-cleaning/main.html#merged-dataframe-column-names",
    "href": "projects/DS5000/data-cleaning/main.html#merged-dataframe-column-names",
    "title": "Data Cleaning",
    "section": "Merged DataFrame Column Names",
    "text": "Merged DataFrame Column Names\n\n\nCode\nprint(merged_df.columns)\n\n\nIndex(['Total_Population', 'African_American', 'White', 'Asian', 'Veterans',\n       'Nonveterans', 'Below_Poverty_Level', 'Median_Household_Income',\n       'High_School_Graduate', 'Bachelors_Degree_or_Higher',\n       'Households_with_Computers', 'Households_with_Internet',\n       'American_Indian_and_Alaska_Native',\n       'Native_Hawaiian_and_Pacific_Islander', 'Median_Age', 'state',\n       'Congressional_District', 'line_number', 'state_abbreviation',\n       'votes_cast', 'citizen_voting_age_population_estimate',\n       'citizen_voting_age_population_moe', 'voting_rate_estimate',\n       'voting_rate_moe'],\n      dtype='object')",
    "crumbs": [
      "About",
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html",
    "href": "projects/DS5000/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The next step is conducting Exploratory Data Analysis on our data. Exploratory Data Analysis is needed as it lets us understand the data on a deeper level, finding errors or correlations, as well as prepping it for the next step in analysis.\nData Exploratory Analysis contains many aspects, and are complex in determining which types of data are optimal for showing insights. The most basic form of data analysis involves understanding our dataset. This involves looking at table value types, descriptive statistics, and quickly gives us insight into our data distribution.\nExploratory Analysis also includes data visualization, letting us utilize visuals to make complex data easier to understand. Correlation analysis is also commonly used to find relationships between variables, and statistical / Hypothesis testing lets us answer questions regarding the data and relationships between variables.\nIn our Exploratory data analysis, we performed z-score normalization, as explained in data cleaning, and Log Transformation.\n\n\nLog transformation is performed on data that is highly skewed. Data that is highly skewed contains an asymmetric distribution, mainly due to extreme outliers, and can pose many problems in models that utilize distance metrics. This transformation is done by performing a log transformation on each data point utilizing this formula:\n\\(\\text{log\\_transformed\\_value} = \\log_{10}(x + 1)\\)\n\n\n\nIn our EDA analysis, we will be utilizing both non-standardized and standardized data as they are both useful in gathering insights. It is important to remember that standardized and non-standardized data will have same distribution, allowing one to use them interchangeably when visualizing distributions.\nI first plotted the voting rate turnout and congressional districts for each state has to understand the geographical distribution of our data. Minnesota and Oregon had the highest voting rates while Hawaii and Arkansas had the lowest voting rates. California and Texas had the highest amount of congressional districts, combining for 90.\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nmerged_df = pd.read_csv('../../data/processed-data/merged.csv') \nmerged_standard_df = pd.read_csv('../../data/processed-data/merged_standard.csv') \n\nstate_turnout = merged_df.groupby('state_abbreviation')['voting_rate_estimate'].mean().sort_values()\nprint(\"Top 5 states with the highest voting rate:\")\nprint(state_turnout.tail(5))\n\n# Print the bottom 5 states with the lowest voting rate\nprint(\"\\nBottom 5 states with the lowest voting rate:\")\nprint(state_turnout.head(5))\n# Frequency counts for categorical variables\n\n# Bar plot for state frequencies\nmerged_df['state'].value_counts().plot(kind='bar', figsize=(12, 6))\nplt.title(\"State Abbreviation Frequencies\")\nplt.xlabel(\"State\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\nTop 5 states with the highest voting rate:\nstate_abbreviation\nME    58.857784\nOR    59.639056\nCO    60.514654\nMT    61.240815\nMN    62.717493\nName: voting_rate_estimate, dtype: float64\n\nBottom 5 states with the lowest voting rate:\nstate_abbreviation\nHI    39.223979\nAR    39.922292\nWV    40.342983\nOK    41.335192\nMS    41.945696\nName: voting_rate_estimate, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\nI then plotted the univariate variables:\n- median_household_income\n- total_population\n- voting_rate_estimate\nto better understand the distribution of the data.\n\ntotal_population and voting_rate_estimate displayed normal distributions.\n\nThe distribution of median_household_income was skewed to the right, with the majority of values falling below the mean.\n\n\n\nCode\n# Summary statistics for numerical variables\n\n\n# Histograms for selected columns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor col in ['Median_Household_Income', 'Total_Population', 'voting_rate_estimate']:\n    plt.figure(figsize=(8, 4))\n    sns.histplot(merged_df[col], kde=True, bins=30)\n    plt.title(f\"Distribution of {col}\")\n    plt.xlabel(col)\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutlier Removal To address outliers that may affect the mean and skewness of the data, I evaluated the skewness of every variable, those with over 1 were considered skewed and needed to be Log Normalized.\nThis showed that the variables “African_American”, “Asian”, “American_Indian_and_Alaskan_Native”, “Native_Hawaiin_and_Pacific_Islander” and “Median_househould_income” had skew values greater than the absolute value of 1, indicating a highly asymmetric and imbalanced distribution of data. ## Skewness Measurement Before Log\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Calculate skewness for all numerical columns\nfrom scipy.stats import skew\n\n\n# Ensure 'Congressional_District' is treated as a string\nmerged_df['Congressional_District'] = merged_df['Congressional_District'].astype(str)\n\n# Calculate skewness for all numerical columns and filter values greater than 1\nskew_values = {}\nfor col in merged_df.select_dtypes(include=['float64', 'int64']).columns:\n    skew_value = skew(merged_df[col], nan_policy='omit')  # Handle NaN values gracefully\n    if abs(skew_value) &gt; 1:  # Filter for skewness greater than 1 in magnitude\n        skew_values[col] = skew_value\n\n# Display columns with skewness &gt; 1\nprint(\"Columns with skewness &gt; 1:\")\nfor col, skew_value in skew_values.items():\n    print(f\"{col}: {skew_value}\")\n\n\nColumns with skewness &gt; 1:\nAfrican_American: 1.8797342205111223\nAsian: 3.1701394736477755\nMedian_Household_Income: 1.2034986622111112\nAmerican_Indian_and_Alaska_Native: 7.498173419926523\nNative_Hawaiian_and_Pacific_Islander: 13.340848187737828\n\n\nThis analysis showed that the variables African_American, Asian, American_Indian_and_Alaskan_Native, Native_Hawaiian_and_Pacific_Islander, and Median_household_income had skew values greater than the absolute value of 1, indicating a highly asymmetric and imbalanced distribution of data.\n\n\n\n\n\n\nCode\nfrom scipy.stats import skew\nimport numpy as np\n\n# Make a copy of the dataframe for log transformation\nmerged_log_df = merged_df.copy()\n\n# Identify highly skewed columns\nhighly_skewed_cols = [col for col, skewness in skew_values.items() if abs(skewness) &gt; 1]\n\n# Apply log1p transformation to highly skewed columns\nfor col in highly_skewed_cols:\n    if col in merged_log_df.columns:  # Check if the column exists\n        merged_log_df[f'Log_{col}'] = np.log1p(merged_log_df[col])  # log1p handles zeros\n\n# Verify skewness after log transformation\nprint(\"\\nSkewness after log transformation:\")\nfor col in highly_skewed_cols:\n    log_col = f'Log_{col}'  # Construct the name of the log-transformed column\n    if log_col in merged_log_df.columns:  # Ensure the column exists\n        transformed_skew = skew(merged_log_df[log_col])\n        print(f\"{log_col} skewness: {transformed_skew}\")\n    else:\n        print(f\"{log_col} does not exist in the dataframe.\")\n\n\n# List of original columns to drop\noriginal_cols_to_drop = ['African_American', 'Asian', 'Median_Household_Income', \n                         'American_Indian_and_Alaska_Native', \n                         'Native_Hawaiian_and_Pacific_Islander']\n\n# Drop the original columns\nmerged_log_df = merged_log_df.drop(columns=original_cols_to_drop)\n\n\n\nSkewness after log transformation:\nLog_African_American skewness: -0.0037722382003638464\nLog_Asian skewness: 0.23046056077109553\nLog_Median_Household_Income skewness: 0.4321994689438688\nLog_American_Indian_and_Alaska_Native skewness: 0.8066703344257306\nLog_Native_Hawaiian_and_Pacific_Islander skewness: -1.1212873986956227\n\n\nI applied log-normalization to these columns on a copy of the original merged_df. After checking the skewness again:\n- All values had an absolute value &lt; 1, except for Native_Hawaiian_and_Pacific_Islander, which remained slightly over 1.\nI chose not to reapply log-normalization to this variable, as the skew was only slightly above 1 and further transformations could compromise data integrity.\nThe results were stored in a new temporary DataFrame called merged_log_df, with the original skewed columns removed.\n\n\n\nI then decided to standardize the columns once again, as the new merged_log_df has only been log normalized so far, producing a new data set merged_log_standard_df. This data frame will include columns that were logged and standardized. Merged_df, containing the raw data, and merged_standard_df, containing standardized data are kept for comparison.\n\n\nCode\ncolumns_to_standardize = ['Total_Population', 'White',\n                          'Veterans', 'Nonveterans', 'Below_Poverty_Level', 'High_School_Graduate', \n                          'Bachelors_Degree_or_Higher', 'Households_with_Computers', \n                          'Households_with_Internet', 'Median_Age', \n                          'Log_African_American', 'Log_Asian', \n                          'Log_Median_Household_Income', 'Log_American_Indian_and_Alaska_Native', \n                          'Log_Native_Hawaiian_and_Pacific_Islander', 'votes_cast', 'citizen_voting_age_population_estimate',  'voting_rate_estimate']\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Standardize the selected columns\nmerged_standard_log_df = merged_log_df.copy() \nmerged_standard_log_df[columns_to_standardize] = scaler.fit_transform(merged_standard_log_df[columns_to_standardize])\n\nprint(merged_standard_log_df.describe().T)\n\n\n\n                                          count          mean       std  \\\nTotal_Population                          431.0  4.698484e-16  1.001162   \nWhite                                     431.0  1.318873e-16  1.001162   \nVeterans                                  431.0  1.648591e-17  1.001162   \nNonveterans                               431.0  1.191107e-15  1.001162   \nBelow_Poverty_Level                       431.0  5.357921e-17  1.001162   \nHigh_School_Graduate                      431.0 -3.544471e-16  1.001162   \nBachelors_Degree_or_Higher                431.0 -1.895880e-16  1.001162   \nHouseholds_with_Computers                 431.0  4.780914e-16  1.001162   \nHouseholds_with_Internet                  431.0  2.143168e-16  1.001162   \nMedian_Age                                431.0  2.514101e-16  1.001162   \nvotes_cast                                431.0 -3.626900e-16  1.001162   \ncitizen_voting_age_population_estimate    431.0 -7.377445e-16  1.001162   \nvoting_rate_estimate                      431.0  6.882868e-16  1.001162   \nLog_African_American                      431.0  4.142085e-16  1.001162   \nLog_Asian                                 431.0 -1.714535e-15  1.001162   \nLog_Median_Household_Income               431.0  3.977226e-15  1.001162   \nLog_American_Indian_and_Alaska_Native     431.0  8.902392e-16  1.001162   \nLog_Native_Hawaiian_and_Pacific_Islander  431.0 -4.121478e-17  1.001162   \n\n                                               min       25%       50%  \\\nTotal_Population                         -4.062869 -0.606897 -0.098532   \nWhite                                    -3.177154 -0.437689  0.223164   \nVeterans                                 -2.197543 -0.639807  0.039217   \nNonveterans                              -3.410535 -0.597602 -0.064598   \nBelow_Poverty_Level                      -1.861771 -0.730136 -0.107994   \nHigh_School_Graduate                     -2.717688 -0.686154  0.038784   \nBachelors_Degree_or_Higher               -2.138960 -0.719254 -0.148051   \nHouseholds_with_Computers                -3.029705 -0.542686  0.035155   \nHouseholds_with_Internet                 -3.579080 -0.528901  0.021268   \nMedian_Age                               -2.801809 -0.631139 -0.026648   \nvotes_cast                               -2.733969 -0.602509  0.018376   \ncitizen_voting_age_population_estimate   -3.689873 -0.521647  0.089798   \nvoting_rate_estimate                     -2.644234 -0.674530  0.022953   \nLog_African_American                     -2.589643 -0.704148 -0.055633   \nLog_Asian                                -2.916233 -0.671612 -0.114192   \nLog_Median_Household_Income              -2.770396 -0.711817 -0.129623   \nLog_American_Indian_and_Alaska_Native    -2.528258 -0.632916 -0.139876   \nLog_Native_Hawaiian_and_Pacific_Islander -3.245660 -0.441311  0.098166   \n\n                                               75%       max  \nTotal_Population                          0.508237  5.639162  \nWhite                                     0.741616  3.021102  \nVeterans                                  0.539561  3.616628  \nNonveterans                               0.558341  4.894800  \nBelow_Poverty_Level                       0.506680  4.419298  \nHigh_School_Graduate                      0.670886  2.775129  \nBachelors_Degree_or_Higher                0.616793  3.505945  \nHouseholds_with_Computers                 0.577837  4.444702  \nHouseholds_with_Internet                  0.612859  3.940705  \nMedian_Age                                0.577842  4.699368  \nvotes_cast                                0.684237  4.320593  \ncitizen_voting_age_population_estimate    0.594691  5.440290  \nvoting_rate_estimate                      0.702530  2.772851  \nLog_African_American                      0.715286  2.066910  \nLog_Asian                                 0.671755  2.869244  \nLog_Median_Household_Income               0.623249  3.148080  \nLog_American_Indian_and_Alaska_Native     0.532964  4.100022  \nLog_Native_Hawaiian_and_Pacific_Islander  0.587863  2.983121  \n\n\nThe below plot shows the difference in distribution of the logged variables. It is very prominent that the logged values are much more normally distributed than the skewed data.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#basic-eda",
    "href": "projects/DS5000/eda/main.html#basic-eda",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "In our EDA analysis, we will be utilizing both non-standardized and standardized data as they are both useful in gathering insights. It is important to remember that standardized and non-standardized data will have same distribution, allowing one to use them interchangeably when visualizing distributions.\nI first plotted the voting rate turnout and congressional districts for each state has to understand the geographical distribution of our data. Minnesota and Oregon had the highest voting rates while Hawaii and Arkansas had the lowest voting rates. California and Texas had the highest amount of congressional districts, combining for 90.\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nmerged_df = pd.read_csv('../../data/processed-data/merged.csv') \nmerged_standard_df = pd.read_csv('../../data/processed-data/merged_standard.csv') \n\nstate_turnout = merged_df.groupby('state_abbreviation')['voting_rate_estimate'].mean().sort_values()\nprint(\"Top 5 states with the highest voting rate:\")\nprint(state_turnout.tail(5))\n\n# Print the bottom 5 states with the lowest voting rate\nprint(\"\\nBottom 5 states with the lowest voting rate:\")\nprint(state_turnout.head(5))\n# Frequency counts for categorical variables\n\n# Bar plot for state frequencies\nmerged_df['state'].value_counts().plot(kind='bar', figsize=(12, 6))\nplt.title(\"State Abbreviation Frequencies\")\nplt.xlabel(\"State\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\nTop 5 states with the highest voting rate:\nstate_abbreviation\nME    58.857784\nOR    59.639056\nCO    60.514654\nMT    61.240815\nMN    62.717493\nName: voting_rate_estimate, dtype: float64\n\nBottom 5 states with the lowest voting rate:\nstate_abbreviation\nHI    39.223979\nAR    39.922292\nWV    40.342983\nOK    41.335192\nMS    41.945696\nName: voting_rate_estimate, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\nI then plotted the univariate variables:\n- median_household_income\n- total_population\n- voting_rate_estimate\nto better understand the distribution of the data.\n\ntotal_population and voting_rate_estimate displayed normal distributions.\n\nThe distribution of median_household_income was skewed to the right, with the majority of values falling below the mean.\n\n\n\nCode\n# Summary statistics for numerical variables\n\n\n# Histograms for selected columns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor col in ['Median_Household_Income', 'Total_Population', 'voting_rate_estimate']:\n    plt.figure(figsize=(8, 4))\n    sns.histplot(merged_df[col], kde=True, bins=30)\n    plt.title(f\"Distribution of {col}\")\n    plt.xlabel(col)\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutlier Removal To address outliers that may affect the mean and skewness of the data, I evaluated the skewness of every variable, those with over 1 were considered skewed and needed to be Log Normalized.\nThis showed that the variables “African_American”, “Asian”, “American_Indian_and_Alaskan_Native”, “Native_Hawaiin_and_Pacific_Islander” and “Median_househould_income” had skew values greater than the absolute value of 1, indicating a highly asymmetric and imbalanced distribution of data. ## Skewness Measurement Before Log\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Calculate skewness for all numerical columns\nfrom scipy.stats import skew\n\n\n# Ensure 'Congressional_District' is treated as a string\nmerged_df['Congressional_District'] = merged_df['Congressional_District'].astype(str)\n\n# Calculate skewness for all numerical columns and filter values greater than 1\nskew_values = {}\nfor col in merged_df.select_dtypes(include=['float64', 'int64']).columns:\n    skew_value = skew(merged_df[col], nan_policy='omit')  # Handle NaN values gracefully\n    if abs(skew_value) &gt; 1:  # Filter for skewness greater than 1 in magnitude\n        skew_values[col] = skew_value\n\n# Display columns with skewness &gt; 1\nprint(\"Columns with skewness &gt; 1:\")\nfor col, skew_value in skew_values.items():\n    print(f\"{col}: {skew_value}\")\n\n\nColumns with skewness &gt; 1:\nAfrican_American: 1.8797342205111223\nAsian: 3.1701394736477755\nMedian_Household_Income: 1.2034986622111112\nAmerican_Indian_and_Alaska_Native: 7.498173419926523\nNative_Hawaiian_and_Pacific_Islander: 13.340848187737828\n\n\nThis analysis showed that the variables African_American, Asian, American_Indian_and_Alaskan_Native, Native_Hawaiian_and_Pacific_Islander, and Median_household_income had skew values greater than the absolute value of 1, indicating a highly asymmetric and imbalanced distribution of data.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#skewness-measurement-after-log",
    "href": "projects/DS5000/eda/main.html#skewness-measurement-after-log",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Code\nfrom scipy.stats import skew\nimport numpy as np\n\n# Make a copy of the dataframe for log transformation\nmerged_log_df = merged_df.copy()\n\n# Identify highly skewed columns\nhighly_skewed_cols = [col for col, skewness in skew_values.items() if abs(skewness) &gt; 1]\n\n# Apply log1p transformation to highly skewed columns\nfor col in highly_skewed_cols:\n    if col in merged_log_df.columns:  # Check if the column exists\n        merged_log_df[f'Log_{col}'] = np.log1p(merged_log_df[col])  # log1p handles zeros\n\n# Verify skewness after log transformation\nprint(\"\\nSkewness after log transformation:\")\nfor col in highly_skewed_cols:\n    log_col = f'Log_{col}'  # Construct the name of the log-transformed column\n    if log_col in merged_log_df.columns:  # Ensure the column exists\n        transformed_skew = skew(merged_log_df[log_col])\n        print(f\"{log_col} skewness: {transformed_skew}\")\n    else:\n        print(f\"{log_col} does not exist in the dataframe.\")\n\n\n# List of original columns to drop\noriginal_cols_to_drop = ['African_American', 'Asian', 'Median_Household_Income', \n                         'American_Indian_and_Alaska_Native', \n                         'Native_Hawaiian_and_Pacific_Islander']\n\n# Drop the original columns\nmerged_log_df = merged_log_df.drop(columns=original_cols_to_drop)\n\n\n\nSkewness after log transformation:\nLog_African_American skewness: -0.0037722382003638464\nLog_Asian skewness: 0.23046056077109553\nLog_Median_Household_Income skewness: 0.4321994689438688\nLog_American_Indian_and_Alaska_Native skewness: 0.8066703344257306\nLog_Native_Hawaiian_and_Pacific_Islander skewness: -1.1212873986956227\n\n\nI applied log-normalization to these columns on a copy of the original merged_df. After checking the skewness again:\n- All values had an absolute value &lt; 1, except for Native_Hawaiian_and_Pacific_Islander, which remained slightly over 1.\nI chose not to reapply log-normalization to this variable, as the skew was only slightly above 1 and further transformations could compromise data integrity.\nThe results were stored in a new temporary DataFrame called merged_log_df, with the original skewed columns removed.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#standardizing-log-transformed-data",
    "href": "projects/DS5000/eda/main.html#standardizing-log-transformed-data",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "I then decided to standardize the columns once again, as the new merged_log_df has only been log normalized so far, producing a new data set merged_log_standard_df. This data frame will include columns that were logged and standardized. Merged_df, containing the raw data, and merged_standard_df, containing standardized data are kept for comparison.\n\n\nCode\ncolumns_to_standardize = ['Total_Population', 'White',\n                          'Veterans', 'Nonveterans', 'Below_Poverty_Level', 'High_School_Graduate', \n                          'Bachelors_Degree_or_Higher', 'Households_with_Computers', \n                          'Households_with_Internet', 'Median_Age', \n                          'Log_African_American', 'Log_Asian', \n                          'Log_Median_Household_Income', 'Log_American_Indian_and_Alaska_Native', \n                          'Log_Native_Hawaiian_and_Pacific_Islander', 'votes_cast', 'citizen_voting_age_population_estimate',  'voting_rate_estimate']\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Standardize the selected columns\nmerged_standard_log_df = merged_log_df.copy() \nmerged_standard_log_df[columns_to_standardize] = scaler.fit_transform(merged_standard_log_df[columns_to_standardize])\n\nprint(merged_standard_log_df.describe().T)\n\n\n\n                                          count          mean       std  \\\nTotal_Population                          431.0  4.698484e-16  1.001162   \nWhite                                     431.0  1.318873e-16  1.001162   \nVeterans                                  431.0  1.648591e-17  1.001162   \nNonveterans                               431.0  1.191107e-15  1.001162   \nBelow_Poverty_Level                       431.0  5.357921e-17  1.001162   \nHigh_School_Graduate                      431.0 -3.544471e-16  1.001162   \nBachelors_Degree_or_Higher                431.0 -1.895880e-16  1.001162   \nHouseholds_with_Computers                 431.0  4.780914e-16  1.001162   \nHouseholds_with_Internet                  431.0  2.143168e-16  1.001162   \nMedian_Age                                431.0  2.514101e-16  1.001162   \nvotes_cast                                431.0 -3.626900e-16  1.001162   \ncitizen_voting_age_population_estimate    431.0 -7.377445e-16  1.001162   \nvoting_rate_estimate                      431.0  6.882868e-16  1.001162   \nLog_African_American                      431.0  4.142085e-16  1.001162   \nLog_Asian                                 431.0 -1.714535e-15  1.001162   \nLog_Median_Household_Income               431.0  3.977226e-15  1.001162   \nLog_American_Indian_and_Alaska_Native     431.0  8.902392e-16  1.001162   \nLog_Native_Hawaiian_and_Pacific_Islander  431.0 -4.121478e-17  1.001162   \n\n                                               min       25%       50%  \\\nTotal_Population                         -4.062869 -0.606897 -0.098532   \nWhite                                    -3.177154 -0.437689  0.223164   \nVeterans                                 -2.197543 -0.639807  0.039217   \nNonveterans                              -3.410535 -0.597602 -0.064598   \nBelow_Poverty_Level                      -1.861771 -0.730136 -0.107994   \nHigh_School_Graduate                     -2.717688 -0.686154  0.038784   \nBachelors_Degree_or_Higher               -2.138960 -0.719254 -0.148051   \nHouseholds_with_Computers                -3.029705 -0.542686  0.035155   \nHouseholds_with_Internet                 -3.579080 -0.528901  0.021268   \nMedian_Age                               -2.801809 -0.631139 -0.026648   \nvotes_cast                               -2.733969 -0.602509  0.018376   \ncitizen_voting_age_population_estimate   -3.689873 -0.521647  0.089798   \nvoting_rate_estimate                     -2.644234 -0.674530  0.022953   \nLog_African_American                     -2.589643 -0.704148 -0.055633   \nLog_Asian                                -2.916233 -0.671612 -0.114192   \nLog_Median_Household_Income              -2.770396 -0.711817 -0.129623   \nLog_American_Indian_and_Alaska_Native    -2.528258 -0.632916 -0.139876   \nLog_Native_Hawaiian_and_Pacific_Islander -3.245660 -0.441311  0.098166   \n\n                                               75%       max  \nTotal_Population                          0.508237  5.639162  \nWhite                                     0.741616  3.021102  \nVeterans                                  0.539561  3.616628  \nNonveterans                               0.558341  4.894800  \nBelow_Poverty_Level                       0.506680  4.419298  \nHigh_School_Graduate                      0.670886  2.775129  \nBachelors_Degree_or_Higher                0.616793  3.505945  \nHouseholds_with_Computers                 0.577837  4.444702  \nHouseholds_with_Internet                  0.612859  3.940705  \nMedian_Age                                0.577842  4.699368  \nvotes_cast                                0.684237  4.320593  \ncitizen_voting_age_population_estimate    0.594691  5.440290  \nvoting_rate_estimate                      0.702530  2.772851  \nLog_African_American                      0.715286  2.066910  \nLog_Asian                                 0.671755  2.869244  \nLog_Median_Household_Income               0.623249  3.148080  \nLog_American_Indian_and_Alaska_Native     0.532964  4.100022  \nLog_Native_Hawaiian_and_Pacific_Islander  0.587863  2.983121  \n\n\nThe below plot shows the difference in distribution of the logged variables. It is very prominent that the logged values are much more normally distributed than the skewed data.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#correlation-heatmap-of-log-transformed-data",
    "href": "projects/DS5000/eda/main.html#correlation-heatmap-of-log-transformed-data",
    "title": "Exploratory Data Analysis",
    "section": "Correlation Heatmap of Log-Transformed Data",
    "text": "Correlation Heatmap of Log-Transformed Data\n\n\nCode\n# Create a mapping of original to log-transformed column names\noriginal_columns = merged_standard_df.columns\nlog_columns = [\"Log_\" + col if \"Log_\" + col in merged_standard_log_df.columns else col for col in original_columns]\n\n# Reorder the log-transformed dataframe to match the order of original dataframe\nmerged_standard_log_df = merged_standard_log_df[log_columns]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Compute the correlation matrix\nnumeric_cols = merged_standard_log_df.select_dtypes(include=['float64', 'int64'])\ncorr = numeric_cols.corr()\n\n# Create a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Heatmap visualization\nplt.figure(figsize=(10, 10))\nsns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\nplt.title(\"Correlation Matrix of Standard Log Data\")\nplt.show()",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#log-transformed-effect-analysis",
    "href": "projects/DS5000/eda/main.html#log-transformed-effect-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Log Transformed Effect Analysis",
    "text": "Log Transformed Effect Analysis\n\n\nCode\nimport pandas as pd\n\n# Data for the table\ndata = {\n    \"Variable\": [\n        \"African American\",\n        \"Asian\",\n        \"Median Income\",\n        \"American Indian and Alaska Native\",\n        \"Native Hawaiian and Pacific Islander\"\n    ],\n    \"Standard Correlation\": [\n        -0.22, \n        0.05, \n        0.53, \n        -0.09, \n        0.09\n    ],\n    \"Log Correlation\": [\n        -0.24, \n        0.19, \n        0.56, \n        -0.12, \n        -0.04\n    ]\n}\n\n# Create the DataFrame\ncomparison_df = pd.DataFrame(data)\nprint(comparison_df)\n\n\n                               Variable  Standard Correlation  Log Correlation\n0                      African American                 -0.22            -0.24\n1                                 Asian                  0.05             0.19\n2                         Median Income                  0.53             0.56\n3     American Indian and Alaska Native                 -0.09            -0.12\n4  Native Hawaiian and Pacific Islander                  0.09            -0.04\n\n\nThe results showed that the variables that were logged experienced a change in correlation with voting_rate_estimate after the transformation:\n\nAfrican American:\n\nStandard: -0.22 → Log-transformed: -0.24\n\nAsian:\n\nStandard: 0.05 → Log-transformed: 0.19\n\nMedian Income:\n\nStandard: 0.53 → Log-transformed: 0.56\n\nAmerican Indian and Alaska Native:\n\nStandard: -0.05 → Log-transformed: -0.09\n\nNative Hawaiian and Pacific Islander:\n\nStandard: 0.09 → Log-transformed: -0.04\n\n\nOther variables not affected by the log transformation retained the same correlation scores.\nThe biggest difference and surprise was for Asian, where the correlation increased by 0.14. This shift could uncover new insights previously unseen and highlights the importance of applying log transformation to the dataset.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#voting-rate-estimates-correlation",
    "href": "projects/DS5000/eda/main.html#voting-rate-estimates-correlation",
    "title": "Exploratory Data Analysis",
    "section": "Voting Rate Estimates Correlation",
    "text": "Voting Rate Estimates Correlation\n\n\nCode\nvoting_corr = corr['voting_rate_estimate']\n\n# Convert to DataFrame for better readability\nvoting_corr_df = voting_corr.reset_index()\nvoting_corr_df.columns = ['Feature', 'Correlation with Voting Rate']\n\n# Sort by correlation values\nvoting_corr_df = voting_corr_df.sort_values(by='Correlation with Voting Rate', ascending=False)\n\n# Print the DataFrame\nprint(voting_corr_df)\n\n\n                                   Feature  Correlation with Voting Rate\n18                    voting_rate_estimate                      1.000000\n16                              votes_cast                      0.909693\n11                Households_with_Internet                      0.631127\n9               Bachelors_Degree_or_Higher                      0.622064\n10               Households_with_Computers                      0.549691\n7                  Median_Household_Income                      0.523973\n17  citizen_voting_age_population_estimate                      0.417818\n14                              Median_Age                      0.392134\n2                                    White                      0.358125\n5                              Nonveterans                      0.219181\n4                                 Veterans                      0.197612\n0                         Total_Population                      0.117178\n3                                    Asian                      0.042946\n12       American_Indian_and_Alaska_Native                     -0.046798\n13    Native_Hawaiian_and_Pacific_Islander                     -0.087200\n15                  Congressional_District                     -0.220039\n1                         African_American                     -0.230694\n8                     High_School_Graduate                     -0.308645\n6                      Below_Poverty_Level                     -0.661573",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#correlation-analysis",
    "href": "projects/DS5000/eda/main.html#correlation-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nIn analyzing the Log Transformed heatmap, we found that Voting Rate had the highest correlation with Households_with_internet with 0.63 Bachelors_Degree_or Higher, with 0.62. Following, was Log Median Household Income with 0.56, and Households_with_Computers with 0.55.\nBelow Poverty Level had the most negative correlation with -0.66, followed by High_school Graduate with -0.31 and Log_African_American, with -0.25.\nSome other interesting metrics were white with 0.26 and Nonveterans having a higher correlation (0.22) than Veterans (0.2).",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#plotting-log-transformed-values",
    "href": "projects/DS5000/eda/main.html#plotting-log-transformed-values",
    "title": "Exploratory Data Analysis",
    "section": "Plotting Log-Transformed Values",
    "text": "Plotting Log-Transformed Values\n\n\nCode\n# Scatterplot of voting_rate_estimate against key features\nimportant_features = ['Log_Median_Household_Income', 'Log_African_American', 'Log_Asian', 'Below_Poverty_Level', 'White', 'Veterans', 'High_School_Graduate', 'Bachelors_Degree_or_Higher', 'Median_Age']\n\nplt.figure(figsize=(15, 10))\nfor i, feature in enumerate(important_features, 1):\n    plt.subplot(3, 3, i)\n    sns.scatterplot(data=merged_standard_log_df, x=feature, y='voting_rate_estimate')\n    plt.title(f\"Voting Rate vs {feature}\")\n    plt.xlabel(feature)\n    plt.ylabel(\"Voting Rate Estimate\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis showcased very interesting outcomes:\n- Positive correlations between voting rates and:\n- Median Household Income\n- White\n- Asian\n- Bachelors Degree or Higher\n- Median Age\n\nNegative correlations between voting rates and:\n\nPoverty Rate\n\nHigh School Graduate\n\nA slightly negative correlation was observed between voting rates and African American.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#plotting-raw-values",
    "href": "projects/DS5000/eda/main.html#plotting-raw-values",
    "title": "Exploratory Data Analysis",
    "section": "Plotting Raw Values",
    "text": "Plotting Raw Values\n\n\nCode\n# Scatterplot of voting_rate_estimate against key features\nfeatures = ['Median_Household_Income', 'African_American', 'Asian', 'Below_Poverty_Level', 'White', 'Veterans', 'High_School_Graduate', 'Bachelors_Degree_or_Higher', 'Median_Age']\n\nplt.figure(figsize=(15, 10))\nfor i, feature in enumerate(features, 1):\n    plt.subplot(3, 3, i)\n    sns.scatterplot(data=merged_df, x=feature, y='voting_rate_estimate')\n    plt.title(f\"Voting Rate vs {feature}\")\n    plt.xlabel(feature)\n    plt.ylabel(\"Voting Rate Estimate\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen comparing the log-transformed data to the raw data, the both showed patterns where:\n- Median Age, Bachelors Degree, Median Household Income, and White showed positive correlations.\n- Poverty Rate maintained a negative correlation.\nDue to the skewed data in the Raw Data however, analyzing patterns for Asian and African American becomes very difficult. This highlights the importance of log transformation, as it adjusts the data into a more understandable and explainable form.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#comparison-of-raw-vs-log-transformed-data-statistics",
    "href": "projects/DS5000/eda/main.html#comparison-of-raw-vs-log-transformed-data-statistics",
    "title": "Exploratory Data Analysis",
    "section": "Comparison of Raw vs Log-Transformed Data Statistics",
    "text": "Comparison of Raw vs Log-Transformed Data Statistics\n\n\nCode\nlog_transformed_columns = ['Log_African_American', 'Log_Asian', 'Log_Median_Household_Income', \n                           'Log_American_Indian_and_Alaska_Native', 'Log_Native_Hawaiian_and_Pacific_Islander']\n\nraw_columns = ['African_American', 'Asian', 'Median_Household_Income', \n               'American_Indian_and_Alaska_Native', 'Native_Hawaiian_and_Pacific_Islander']\n\n\ndef calculate_stats(df, columns):\n    stats = df[columns].agg(['mean', 'std', 'skew', 'kurtosis']).T\n    stats['min'] = df[columns].min()\n    stats['max'] = df[columns].max()\n    return stats\n\n\nraw_stats = calculate_stats(merged_df, raw_columns)\nlog_stats = calculate_stats(merged_standard_log_df, log_transformed_columns)\n\n# Display statistics\nprint(\"Raw Data Statistics:\")\nprint(raw_stats)\nprint(\"\\nLog-Transformed Data Statistics:\")\nprint(log_stats)\n\n\nRaw Data Statistics:\n                                              mean            std       skew  \\\nAfrican_American                      94183.689095  102405.485798   1.886305   \nAsian                                 42399.661253   52777.917382   3.181222   \nMedian_Household_Income               64795.770302   18072.616613   1.207706   \nAmerican_Indian_and_Alaska_Native      6338.090487   15279.267538   7.524386   \nNative_Hawaiian_and_Pacific_Islander   1448.104408    5381.575914  13.387485   \n\n                                        kurtosis    min     max  \nAfrican_American                        3.165012   3667  491080  \nAsian                                  13.842183   1403  437632  \nMedian_Household_Income                 1.817438  30483  141661  \nAmerican_Indian_and_Alaska_Native      66.960823    265  180970  \nNative_Hawaiian_and_Pacific_Islander  208.339729      0   92334  \n\nLog-Transformed Data Statistics:\n                                                  mean       std      skew  \\\nLog_African_American                      4.142085e-16  1.001162 -0.003785   \nLog_Asian                                -1.714535e-15  1.001162  0.231266   \nLog_Median_Household_Income               3.977226e-15  1.001162  0.433710   \nLog_American_Indian_and_Alaska_Native     8.902392e-16  1.001162  0.809490   \nLog_Native_Hawaiian_and_Pacific_Islander -4.121478e-17  1.001162 -1.125207   \n\n                                          kurtosis       min       max  \nLog_African_American                     -0.608039 -2.589643  2.066910  \nLog_Asian                                -0.191973 -2.916233  2.869244  \nLog_Median_Household_Income               0.093355 -2.770396  3.148080  \nLog_American_Indian_and_Alaska_Native     1.778489 -2.528258  4.100022  \nLog_Native_Hawaiian_and_Pacific_Islander  2.829978 -3.245660  2.983121",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#distribution-of-voting-rate-pairplot",
    "href": "projects/DS5000/eda/main.html#distribution-of-voting-rate-pairplot",
    "title": "Exploratory Data Analysis",
    "section": "Distribution of Voting Rate Pairplot",
    "text": "Distribution of Voting Rate Pairplot\nA pairwise plot showcasing scatterplots between the important features is shown, with the ability to not only check correlation between features and voting rate, but all features amongst each other. - For example, poverty rate and bachelors degree or higher have a negative correlation, while bachelors degree of higher and median income have a positive correlation.\n\n\nCode\n# Distribution of voting_rate_estimate\nsns.histplot(merged_standard_log_df['voting_rate_estimate'], kde=True, bins=30)\nplt.title(\"Distribution of Voting Rate Estimate\")\nplt.xlabel(\"Voting Rate Estimate\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# Relationship with features\nsns.pairplot(data=merged_standard_log_df, vars=important_features + ['voting_rate_estimate'])\nplt.show()",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#anova-test",
    "href": "projects/DS5000/eda/main.html#anova-test",
    "title": "Exploratory Data Analysis",
    "section": "Anova Test",
    "text": "Anova Test\n\n\nCode\n# do voting rates differ for states \nfrom scipy.stats import f_oneway\n\ngroups = [merged_standard_log_df[merged_standard_log_df['state_abbreviation'] == state]['voting_rate_estimate'] \n          for state in merged_standard_log_df['state_abbreviation'].unique()]\n\nf_stat, p_value = f_oneway(*groups)\nprint(f\"F-Statistic: {f_stat}, P-Value: {p_value}\")\n\n\nF-Statistic: 5.939407497543833, P-Value: 2.4710973577299353e-25\n\n\nThe extremely low P value indicates that there is statistically significant differences in voting rate between states.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#t-test-california-vs-texas",
    "href": "projects/DS5000/eda/main.html#t-test-california-vs-texas",
    "title": "Exploratory Data Analysis",
    "section": "T-Test: California vs Texas",
    "text": "T-Test: California vs Texas\nThe two states with the highest amount of Districts, California and Texas are compared against each other\n\n\nCode\nfrom scipy.stats import ttest_ind\nstate_1 = merged_standard_log_df[merged_standard_log_df['state_abbreviation'] == 'CA']['voting_rate_estimate']\nstate_2 = merged_standard_log_df[merged_standard_log_df['state_abbreviation'] == 'TX']['voting_rate_estimate']\n\nt_stat, p_value = ttest_ind(state_1, state_2)\nprint(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n\n\nT-Statistic: 1.663421030911609, P-Value: 0.09982659171288376\n\n\nThe p-value is greater than 0.05, so we fail to reject the null hypothesis. This indicates the difference in voting rates between California and Texas is not statistically significant at the 95% confidence level.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#t-test-massachusetts-vs-rhode-island",
    "href": "projects/DS5000/eda/main.html#t-test-massachusetts-vs-rhode-island",
    "title": "Exploratory Data Analysis",
    "section": "T-Test: Massachusetts vs Rhode Island",
    "text": "T-Test: Massachusetts vs Rhode Island\nTwo states, Massachussets and Rhode Island are compared. Both states are similar in size and in the North East\n\n\nCode\nfrom scipy.stats import ttest_ind\n\n\nstate_1 = merged_standard_log_df[merged_standard_log_df['state_abbreviation'] == 'MA']['voting_rate_estimate']\nstate_2 = merged_standard_log_df[merged_standard_log_df['state_abbreviation'] == 'RI']['voting_rate_estimate']\n\nt_stat, p_value = ttest_ind(state_1, state_2)\nprint(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n\n\nT-Statistic: 1.9987057273754905, P-Value: 0.07671272608947101\n\n\nThe p-value is greater than 0.05, so we fail to reject the null hypothesis. This indicates the difference in voting rates between Massachussets and Rhode Island is not statistically significant at the 95% confidence level.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#t-test-florida-vs-oregon",
    "href": "projects/DS5000/eda/main.html#t-test-florida-vs-oregon",
    "title": "Exploratory Data Analysis",
    "section": "T-Test: Florida vs Oregon",
    "text": "T-Test: Florida vs Oregon\nFlorida and Oregon are compared. These states have drastically different population sizes, geography, and voting culture.\n\n\nCode\nfrom scipy.stats import ttest_ind\n\n\nstate_1 = merged_standard_log_df[merged_standard_log_df['state_abbreviation'] == 'FL']['voting_rate_estimate']\nstate_2 = merged_standard_log_df[merged_standard_log_df['state_abbreviation'] == 'OR']['voting_rate_estimate']\n\nt_stat, p_value = ttest_ind(state_1, state_2)\nprint(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n\n\nT-Statistic: -3.1624093707880396, P-Value: 0.003953916864173521\n\n\nThe P value below 0.05 indicates that there is statistically significant differences in voting rate between these two states.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#distribution-analysis",
    "href": "projects/DS5000/eda/main.html#distribution-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Distribution Analysis",
    "text": "Distribution Analysis\n\nSkewness in Data: Variables such as African_American, Asian, Median_Household_Income, and 2 other features showed high skewness.\n\nLog Transformation Impact: After log transformation, the skewed variables became more normally distributed, improving interpretability and analysis.\n\nFor example, the correlation for Asian improved from 0.05 (non-logged) to 0.19 (logged), revealing previously unseen patterns.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#correlation-analysis-1",
    "href": "projects/DS5000/eda/main.html#correlation-analysis-1",
    "title": "Exploratory Data Analysis",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\n\nVoting Rate had the strongest positive correlations with:\n\nHouseholds_with_Internet (0.63)\n\n`Bachelors_Degree_or_Higher (0.62)\n\nLog Median Household Income (0.56)\n\n\nThe strongest negative correlations were observed with:\n\nBelow_Poverty_Level (-0.66)\n\nHigh_School_Graduate (-0.31)\n\nLog_African_American (-0.25)",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#state-wise-voting-rate",
    "href": "projects/DS5000/eda/main.html#state-wise-voting-rate",
    "title": "Exploratory Data Analysis",
    "section": "State-Wise Voting Rate",
    "text": "State-Wise Voting Rate\n\nStates such as Minnesota had the highest average voting rates (62.7%), while Hawaii had the lowest (39.22%).\n\nSignificant differences in voting rates were confirmed across states through the ANOVA test.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#statistical-testing-1",
    "href": "projects/DS5000/eda/main.html#statistical-testing-1",
    "title": "Exploratory Data Analysis",
    "section": "Statistical Testing",
    "text": "Statistical Testing\n\nCalifornia vs Texas: No statistically significant difference in voting rates.\n\nMassachusetts vs Rhode Island: No statistically significant difference in voting rates.\n\nFlorida vs Oregon: Statistically significant differences were observed, highlighting the impact of geography and culture on voter turnout.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/eda/main.html#policy-recommendations",
    "href": "projects/DS5000/eda/main.html#policy-recommendations",
    "title": "Exploratory Data Analysis",
    "section": "Policy Recommendations",
    "text": "Policy Recommendations\n\nFocus on socioeconomic factors like poverty reduction and education access to improve voter turnout.\n\nIncrease internet access, as these show strong correlations with higher voting rates.",
    "crumbs": [
      "About",
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html",
    "href": "projects/DS5000/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Unsupervised Learning is a type of machine learning that analyzes data in the absence of labels. Unsupervised learning involves clustering and dimensionality reduction, both used to discover patterns and insights in the data not seen before.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#introduction",
    "href": "projects/DS5000/unsupervised-learning/main.html#introduction",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Unsupervised Learning is a type of machine learning that analyzes data in the absence of labels. Unsupervised learning involves clustering and dimensionality reduction, both used to discover patterns and insights in the data not seen before.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#pca",
    "href": "projects/DS5000/unsupervised-learning/main.html#pca",
    "title": "Unsupervised Learning",
    "section": "PCA",
    "text": "PCA\nPrincipal Component Analysis, known as PCA is the most commonly used dimensionality reduction technique, and involves transforming the data points into new calculated variables named “Principal Components”. Each component is comprised of linear combinations of the original feature, and captures the direction of maximum variance. When plotting the top two Principal Components, we essentially are able to use two features that take into account the majority of explained variance for the dataset.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nmerged_standard_log_df = pd.read_csv('../../data/processed-data/merged_standard_log.csv') \n\nmerged_standard_log_df['Congressional_District'] = merged_standard_log_df['Congressional_District'].astype(str)\n\nprint(\"Column Types:\")\nprint(merged_standard_log_df.dtypes)\n\nnumerical_columns = merged_standard_log_df.select_dtypes(include=['float64', 'int64']).columns\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Apply PCA\npca = PCA()\n\n# Fit and transform with PCA\nnumerical_data = merged_standard_log_df[numerical_columns]\n\npca_result = pca.fit_transform(numerical_data)\n\n# Plot explained variance ratio\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='--')\nplt.title('Explained Variance by Principal Components')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.show()\n\n\n# Visualize first two components\nplt.figure(figsize=(8, 6))\nplt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)\nplt.title('PCA: First Two Principal Components')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()\n\n\nColumn Types:\nDistrict_Name                                object\nTotal_Population                            float64\nLog_African_American                        float64\nWhite                                       float64\nLog_Asian                                   float64\nVeterans                                    float64\nNonveterans                                 float64\nBelow_Poverty_Level                         float64\nLog_Median_Household_Income                 float64\nHigh_School_Graduate                        float64\nBachelors_Degree_or_Higher                  float64\nHouseholds_with_Computers                   float64\nHouseholds_with_Internet                    float64\nLog_American_Indian_and_Alaska_Native       float64\nLog_Native_Hawaiian_and_Pacific_Islander    float64\nMedian_Age                                  float64\nstate                                        object\nCongressional_District                       object\nstate_abbreviation                           object\nvotes_cast                                  float64\ncitizen_voting_age_population_estimate      float64\nvoting_rate_estimate                        float64\ndtype: object\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nloadings = pd.DataFrame(pca.components_, columns=numerical_columns, index=[f'PC{i+1}' for i in range(len(pca.components_))])\nprint(loadings.head())\n\n\n     Total_Population  Log_African_American     White  Log_Asian  Veterans  \\\nPC1          0.221261             -0.077622  0.180383   0.135435  0.129197   \nPC2          0.003581              0.071083 -0.307170   0.436838 -0.371627   \nPC3          0.454383              0.309031 -0.118751   0.053967  0.050890   \nPC4          0.078886             -0.503244  0.247943   0.003214  0.166664   \nPC5         -0.202893              0.410686 -0.420307   0.062545  0.413256   \n\n     Nonveterans  Below_Poverty_Level  Log_Median_Household_Income  \\\nPC1     0.250760            -0.247778                     0.248968   \nPC2     0.105086            -0.097141                     0.315748   \nPC3     0.358875             0.409264                    -0.219878   \nPC4    -0.119718            -0.047839                     0.030063   \nPC5    -0.340224            -0.057707                    -0.032332   \n\n     High_School_Graduate  Bachelors_Degree_or_Higher  \\\nPC1             -0.099454                    0.325701   \nPC2             -0.430129                    0.256663   \nPC3              0.073896                   -0.005068   \nPC4             -0.130613                   -0.091365   \nPC5             -0.075391                   -0.029165   \n\n     Households_with_Computers  Households_with_Internet  \\\nPC1                   0.343100                  0.367542   \nPC2                  -0.095719                 -0.035598   \nPC3                   0.198160                  0.121688   \nPC4                  -0.076882                 -0.068557   \nPC5                   0.035117                  0.040119   \n\n     Log_American_Indian_and_Alaska_Native  \\\nPC1                              -0.000999   \nPC2                               0.001622   \nPC3                               0.206009   \nPC4                               0.588957   \nPC5                               0.005147   \n\n     Log_Native_Hawaiian_and_Pacific_Islander  Median_Age  votes_cast  \\\nPC1                                  0.028738    0.133872    0.355285   \nPC2                                  0.133370   -0.253671   -0.132977   \nPC3                                  0.146122   -0.326118   -0.096013   \nPC4                                  0.466289   -0.168528   -0.031685   \nPC5                                  0.504108    0.093730    0.124426   \n\n     citizen_voting_age_population_estimate  voting_rate_estimate  \nPC1                                0.279135              0.315462  \nPC2                               -0.300544              0.002436  \nPC3                                0.171596             -0.255788  \nPC4                               -0.060010             -0.020372  \nPC5                                0.127527              0.120975  \n\n\nFeature loadings are analyzed to understand the driving factors behind each Principal Component. Each feature loading is the weight of the original variables contribution in the PCA.\nThis revealed that PC1 was positively correlated with Total Population, Median Household Income, Nonveterans, Bachelors Degree or Higher, and slightly correlated with White. PC1 was also negatively correlated with Below Poverty Level and slightly negatively correlated with High School Graduate. This indicates that PC1s with higher values are those with a higher total population, higher median household income, more degree of whites etc.\nPC2 was positively correlated with Asian, Median Household Income and Bachelors Degree or Higher, while negatively correlated with Veterans, White, and High School Graduate. PC1’s components of higher education, income, and population size link it to higher voting rate, while PC2’s components of Asian, Veterans, and White reflect a more Cultural and Socioeconomic Difference rather than voting.\n\n\nCode\nplt.figure(figsize=(8, 6))\nplt.scatter(pca_result[:, 0], pca_result[:, 1], c=merged_standard_log_df['voting_rate_estimate'], cmap='viridis', alpha=0.7)\nplt.title('PCA: First Two Principal Components')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.colorbar(label='Voting Rate Estimate')\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen we color our plot by voting rate estimate, however, we can see that PC1 is correlated with a higher voting rate. Values with a higher voting rate are clustered in the positive PC1 direction. PC2 does not seem to have an effect on voting rate estimate. ### PCA Correlation\n\n\nCode\npc_df = pd.DataFrame(pca_result, columns=[f'PC{i+1}' for i in range(pca_result.shape[1])])\npc_df['voting_rate_estimate'] = merged_standard_log_df['voting_rate_estimate']\ncorrelations = pc_df.corr()\nprint(correlations['voting_rate_estimate'].sort_values(ascending=False))\n\n\nvoting_rate_estimate    1.000000\nPC1                     0.789800\nPC9                     0.287184\nPC5                     0.118478\nPC8                     0.116832\nPC11                    0.078606\nPC7                     0.049434\nPC15                    0.013838\nPC14                    0.011249\nPC16                    0.008906\nPC17                    0.005504\nPC2                     0.004660\nPC10                   -0.004199\nPC4                    -0.026708\nPC18                   -0.031082\nPC13                   -0.041294\nPC12                   -0.091670\nPC6                    -0.292916\nPC3                    -0.399221\nName: voting_rate_estimate, dtype: float64\n\n\nAnalyzing Voting Rate correlalation, we see that that PC1 is positively correlated with voting rate, achieving a score of 0.789, while PC2 only had a score of 0.0046.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#t-sne",
    "href": "projects/DS5000/unsupervised-learning/main.html#t-sne",
    "title": "Unsupervised Learning",
    "section": "T-SNE",
    "text": "T-SNE\nT-SNE is another dimensionality reduction method technique that transforms high dimensional data into lower 2D or 3D space. T-SNE first calculates the pairwise similarities between all data points in higher dimensional space.\nThis is done by utilizing a Gaussian distribution, calculating the probability that two points are similar. In reduced space, pairpise similarities are also computed, but using a Student-t distribution.\nt-SNE’s goal is to minimzie the KL-Divergence(the measure of dissimimilarity between two distributions) between the high dimensional and low dimensional representations.\nAn important parameter of t-SNE is perplexity, influencing how local and global structures are represented in the dimensionality reduction. A low perplexity value puts more focus on retaining local structures, retaining local relationships, while a higher performance value puts more attention in the global structure, and may mean focusing on more distant value to preserve global structure.\nPlotting t-SNE with varying levels of Perplexity values to understand the impact and importance of the Perplexity value.\nA perplexity score of 5 is plotted first, and showcases tightly clustered datapoints, representing locally maintained structure.\nA perplexity score of 30 is then plotted, showcasing more a spread out distribution of datapoints, but clusters are still visible. This represents a value that emphasizes both local and global structures, and normally the default when plotting.\nFinally, a perplexity score of 50 is plotted, emphasizing global structure. This showcases a distribution similar to the perplexity score of 30, but with less visible structures.\n\n\nCode\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Prepare the standardized numerical data for t-SNE\nnumerical_data_for_tsne = merged_standard_log_df[numerical_columns]\n\n# Apply t-SNE with different perplexity values\nperplexities = [5, 30, 50]\ntsne_results = {}\n\nfor perplexity in perplexities:\n    tsne = TSNE(n_components=2, perplexity=perplexity)\n    tsne_result = tsne.fit_transform(numerical_data_for_tsne)\n    tsne_results[perplexity] = tsne_result\n\n# Plot t-SNE results for different perplexity values\nplt.figure(figsize=(15, 10))\nfor i, perplexity in enumerate(perplexities, 1):\n    plt.subplot(2, 2, i)\n    plt.scatter(tsne_results[perplexity][:, 0], tsne_results[perplexity][:, 1], alpha=0.7)\n    plt.title(f't-SNE with Perplexity {perplexity}')\n    plt.xlabel('t-SNE 1')\n    plt.ylabel('t-SNE 2')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nPlotting the t-SNE results with voting rate estimate, we see that regardless of perplexity score, each plot displays the pattern of grouping similar voting rates together.\nWith a perplexity of 5, small clusters are made up of similar voting rates, with some containing mixes.\nA perplexity score of 30, despite having fewer distinct clusters, showcases gradient color pattern, splitting the data into regions of higher and lower voting rates.\nA similar pattern is observed in a perplexity score of 50, with a much smoother and distinct gradient separating the difference in voting rates. A perplexity of 30 was selected for further analysis due to its balance of preserving both local and global structures.\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n# Prepare the standardized numerical data for t-SNE\nnumerical_data_for_tsne = merged_standard_log_df[numerical_columns]\nvoting_rate = merged_standard_log_df['voting_rate_estimate']  # Your key feature\n\n# Apply t-SNE with different perplexity values\nperplexities = [5, 30, 50]\ntsne_results = {}\n\nfor perplexity in perplexities:\n    tsne = TSNE(n_components=2, perplexity=perplexity)\n    tsne_result = tsne.fit_transform(numerical_data_for_tsne)\n    tsne_results[perplexity] = tsne_result\n\n# Plot t-SNE results with contextual overlay for different perplexity values\nplt.figure(figsize=(15, 10))\nfor i, perplexity in enumerate(perplexities, 1):\n    plt.subplot(2, 2, i)\n    scatter = plt.scatter(tsne_results[perplexity][:, 0], \n                          tsne_results[perplexity][:, 1], \n                          c=voting_rate, cmap='viridis', alpha=0.7)\n    plt.title(f't-SNE with Perplexity {perplexity}')\n    plt.xlabel('t-SNE 1')\n    plt.ylabel('t-SNE 2')\n    plt.colorbar(scatter, label='Voting Rate Estimate')  # Add a colorbar to show voting rate\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#pca-1",
    "href": "projects/DS5000/unsupervised-learning/main.html#pca-1",
    "title": "Unsupervised Learning",
    "section": "PCA",
    "text": "PCA\nPCA preserves global structures by maximizing variance utilizing the principal components. PCA may have issues with identifying local structures, however, if the PCA does not captures the variance within these clusters.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#t-sne-1",
    "href": "projects/DS5000/unsupervised-learning/main.html#t-sne-1",
    "title": "Unsupervised Learning",
    "section": "t-SNE",
    "text": "t-SNE\nt-SNE is very good at maintaining local structures, making sure similar points stay together in the dimensionality reduction. t-SNE may have a harder time preserving global structures however, with farther distances in lower dimensional space plot not indicating true distances in higher dimensional space.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#pca-2",
    "href": "projects/DS5000/unsupervised-learning/main.html#pca-2",
    "title": "Unsupervised Learning",
    "section": "PCA",
    "text": "PCA\nPCA is very good for visualizing data as it transforms data with many features into just 2, while capturing the majority of the variance. It provides a global broad view of the pattern of data, however has difficulty providing insight into local clusters and patterns for voting rates.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#t-sne-2",
    "href": "projects/DS5000/unsupervised-learning/main.html#t-sne-2",
    "title": "Unsupervised Learning",
    "section": "t-SNE",
    "text": "t-SNE\nt-SNE also provides a clear visual representation of the data in reduced dimensional space, however emphasizes local structure more, and may have a harder time visualizing global structures. t-SNE’s axes, t-SNE 1 and t-SNE 2 also lack definition, making it difficult intepretnt compared to PCA.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#pca-3",
    "href": "projects/DS5000/unsupervised-learning/main.html#pca-3",
    "title": "Unsupervised Learning",
    "section": "PCA",
    "text": "PCA\nStrengths: - Reducing dimensionaly while preserving a large amount of variance and information. - Very good at preserving global structure, making it optimal for identifying trends and patterns. - Very good at handling data with linear relationships.\nWeaknesses: - Not very good at preserving local clusters and patterns if they have minimal contribution to variance. - Not good with handling data with non-linear relationships.\nUse case: Main goal is to reduce dimensions while preserving information, not as interesting clustering or finding local patterns.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#t-sne-3",
    "href": "projects/DS5000/unsupervised-learning/main.html#t-sne-3",
    "title": "Unsupervised Learning",
    "section": "t-SNE",
    "text": "t-SNE\nStrengths: - Optimal for preserving local structures, identifying local clusters and relationships. - Good for identifying hidden patterns utilzing non-linear relationships.\nWeaknesses: - Not as good at preserving global structures and identifying global trends and patterns. - Due to lack of defined axes, results are confusing to interpret.\nUse case: Main goal is for data exploration, finding local clusters and hidden patterns.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#pca-results",
    "href": "projects/DS5000/unsupervised-learning/main.html#pca-results",
    "title": "Unsupervised Learning",
    "section": "PCA Results",
    "text": "PCA Results\nThe PCA results shows a uniform distribution of data points across our data space, not showing many clusters. Analysis tells us that PC1 is strongly correlated with voting rate estimate, linking it with factors such as median income and university education. The PCA does not showcase much patterns or clusters, reinforcing the idea that it is best used to identify global trends and patterns. In this case, it shows us that areas with higher income and education tend to have higher voter turnout.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#t-sne-results",
    "href": "projects/DS5000/unsupervised-learning/main.html#t-sne-results",
    "title": "Unsupervised Learning",
    "section": "t-SNE Results",
    "text": "t-SNE Results\nThe t-SNE result using perplexity of 30 shows a plot with distinct clusters, and after coloring with voting rate estimate, reveals a separation between high and low voting rates. This helps reveal sub-groups in our data, revealing patterns that are non-linear and not obvious. However, due to the axes of t-SNE not actually meaning anything, the clusters formed can only provide hints into relationships, not providing any actionable information.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#conclusion",
    "href": "projects/DS5000/unsupervised-learning/main.html#conclusion",
    "title": "Unsupervised Learning",
    "section": "Conclusion",
    "text": "Conclusion\nUtilizing these two methods together provides insight into what drives the trends and patterns (through PCA) and where the trends and patterns are grouped together.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#k-means",
    "href": "projects/DS5000/unsupervised-learning/main.html#k-means",
    "title": "Unsupervised Learning",
    "section": "K-means",
    "text": "K-means\nThe K-Means algorithm utilizes the n_clusters parameter, a user-defined parameter indicating the number of centroids, or clusters one wishes the algorithm to use. The way K-means works involves randomly placing these centroids in the data space, and then utilizing Euclidean distance, assigns the closest data point to each centroid.\nThis creates the first set of clusters. In the next iteration, the mean of each cluster is set as the new centroid, and the new closest set of points are assigned to this new cluster. This algorithm continues until the clusters stabilize and converge. The goal of K-means clustering is to minimize the sum of square distances between centroid and it’s datapoints.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#dbscan",
    "href": "projects/DS5000/unsupervised-learning/main.html#dbscan",
    "title": "Unsupervised Learning",
    "section": "DBSCAN",
    "text": "DBSCAN\nDBSCAN, short for “Density-Based Spatial Clustering of Applications with Noise,” is another clustering method, but unlike K-means, groups data points based on density rather than distance. DBSCAN is very good at identifying clusters of all shapes in comparison to K-means, and also does not need the user to specify the number of clusters, as it dynamically finds this based on data density.\nDBSCAN takes in two important user-defined parameters, EPS (epsilon) and Minpoints. EPS dictates the maximum distance between two data points for them to be classified as part of the same cluster. Minpoints defines the minimum number of points needed for data points to be classified as a cluster. Similar to K-Means, the distance metric is Euclidean Distance.\nThe algorithm starts with an unvisited point, finds all points within the EPS, and if it reaches the threshold of Minpoints, becomes a cluster. The algorithm then expands, finding more points and clusters, repeating till no further points can be added.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#hierarchical-clustering",
    "href": "projects/DS5000/unsupervised-learning/main.html#hierarchical-clustering",
    "title": "Unsupervised Learning",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical Clustering is a type of clustering that involves a hierarchy of clusters. There are two types of Hierarchical clustering techniques: Agglomerative (Bottom-Up) and Divisive (Top-Down).\n\nAgglomerative Clustering starts off with all data points as its own cluster. The distances between these data points are calculated, and then grouped together based on the linkage criteria chosen.\n\nDivisive Clustering follows a similar pattern, but with a top-down approach. The data points start off as one big cluster, and continually get split based on linkage criteria until each data point is its own cluster.\n\nLinkage is the method used to measure distances between data points in hierarchical clustering.\n\nTypes of Linkage:\n\nSingle Linkage: Closest distance between two points.\nComplete Linkage: Maximal distance between points.\nAverage Linkage: Average distance between all points in a cluster.\nWard Linkage: Measures the amount of variance within a cluster.\n\n\n\nWe will be using Ward Linkage for its minimization of variation within clusters and its practicality. After the first initial cluster is set, the distances are once again computed between clusters, and are continually merged until there is only 1 cluster left.\nHierarchical clustering creates a dendrogram, a tree-like structure, providing information on how clusters merge and split.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#hyperparameter-optimization",
    "href": "projects/DS5000/unsupervised-learning/main.html#hyperparameter-optimization",
    "title": "Unsupervised Learning",
    "section": "Hyperparameter Optimization",
    "text": "Hyperparameter Optimization\nTo maximize the use of these algorithms, we utilize a hyperparameter called Silhouette Score. Silhouette score is the metric used to validate the quality of a cluster, comparing how well each data point is with its own cluster and other clusters.\n\\[\nS(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n\\]\n\nFormula Explanation:\nWith A(i) being the average distance between the point and other points in its own cluster, and B(i) being the average distance between the point and all other points in the next closest cluster. The score is between -1 and 1, with a higher score indicating a more compact, distinct cluster.\n\n\n\nUsing Silhouette Score for Optimization\n\nK-Means:\nUse the silhouette score to calculate the optimal number of clusters by plotting the score against n_clusters. The optimal number of clusters corresponds to the highest silhouette score.\nDBSCAN:\nUse the silhouette score to calculate the optimal EPS parameter. This is done by plotting the silhouette score against EPS, and the optimal clusters will correspond to the maximum silhouette score.\nHierarchical Clustering:\nThe silhouette score is evaluated to find the optimal place to “cut” the dendrogram. The number of clusters at which the silhouette score is maximized is used as the cut-off point for the dendrogram.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#dbscan-with-varied-eps",
    "href": "projects/DS5000/unsupervised-learning/main.html#dbscan-with-varied-eps",
    "title": "Unsupervised Learning",
    "section": "DBSCAN With Varied EPS",
    "text": "DBSCAN With Varied EPS\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\n\n# Define the eps values to test\neps_values = [1.5, 2, 2.5, 3, 4, 5]\n\n# Create subplots\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\n# Apply DBSCAN with different eps values and plot the results\nfor i, eps in enumerate(eps_values):\n    dbscan = DBSCAN(eps=eps, min_samples=15)\n    dbscan_labels = dbscan.fit_predict(numerical_data)\n    \n    axes[i].scatter(\n        numerical_data.iloc[:, 0],\n        numerical_data.iloc[:, 1],\n        c=dbscan_labels,\n        cmap='viridis',\n        s=10\n    )\n    axes[i].set_title(f\"DBSCAN with eps={eps}\")\n    axes[i].set_xlabel(\"Feature 1\")\n    axes[i].set_ylabel(\"Feature 2\")\n    axes[i].grid()\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen plotting the DBSCAN clusters with voting rate, it was difficult to make out any clear patterns or trends. Low Voting rate estimates were in the same clusters a high voting rates, and didn’t provide any new insights or information regarding the data\n## DBSCAN Plotted with Voting Rate\n\n\nCode\ndbscan = DBSCAN(eps=2.5, min_samples=15)\ndbscan_labels = dbscan.fit_predict(numerical_data)\n\n# Overlay voting rate estimate on DBSCAN clusters\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(\n    numerical_data.iloc[:, 0],\n    numerical_data.iloc[:, 1],\n    c=merged_standard_log_df['voting_rate_estimate'],  # Overlay voting rate estimate\n    cmap='viridis',  # Use a diverging colormap for better visibility\n    s=10\n)\nplt.colorbar(scatter, label='Voting Rate Estimate')\nplt.title(\"DBSCAN Clustering (eps=2.5) with Voting Rate Overlay\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.grid()\nplt.show()",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#dbscan-plotted-with-voting-rate-and-key-features",
    "href": "projects/DS5000/unsupervised-learning/main.html#dbscan-plotted-with-voting-rate-and-key-features",
    "title": "Unsupervised Learning",
    "section": "DBSCAN Plotted with Voting Rate and Key Features",
    "text": "DBSCAN Plotted with Voting Rate and Key Features\n\n\nCode\n# Assuming we have numerical_data and merged_standard_log_df loaded with the necessary data\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import DBSCAN\n\n# Features to plot against voting_rate_estimate\nfeatures = [\n  'White', 'Log_African_American', 'Log_Asian', 'Below_Poverty_Level',\n    'Bachelors_Degree_or_Higher', 'Median_Age', 'Households_with_Internet', 'High_School_Graduate', 'Veterans'\n]\n\n\n\n# Apply DBSCAN with eps=2.5 and assign cluster labels\ndbscan = DBSCAN(eps=2.5, min_samples=15)\ndbscan_labels = dbscan.fit_predict(numerical_data)\n\n# Add the DBSCAN cluster labels to the DataFrame\nmerged_standard_log_df['DBSCAN_Cluster'] = dbscan_labels\n\n# Number of rows and columns for subplots\nncols = 3\nnrows = (len(features) + ncols - 1) // ncols\n\n# Create subplots\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, nrows * 5))\naxes = axes.flatten()  # Flatten axes for easier iteration\n\n# Generate scatter plots for each feature against voting_rate_estimate\nfor ax, feature in zip(axes, features):\n    sns.scatterplot(\n        x=feature,\n        y='voting_rate_estimate',  # Voting rate on the y-axis\n        hue='DBSCAN_Cluster',  # Color by DBSCAN cluster labels\n        data=merged_standard_log_df,\n        palette='Set1',\n        alpha=0.8,\n        ax=ax\n    )\n    ax.set_title(f\"{feature} vs Voting Rate\")\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"Voting Rate Estimate\")\n\n# Turn off any unused subplots\nfor ax in axes[len(features):]:\n    ax.axis('off')\n\n# Adjust layout for better spacing\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen plotting important features with voting rate, and labeling with clusters, it similarly does not show any actionable information, with a mixed distribution of clusters spread through distribution. DBSCAN was not the best suited method for this type of data.\nDBSCAN groups data together based on density- if the density of points are all uniformly distributed, it is very difficult for DBSCAN to make any distinctions between groups. This may have occurred with Voting Rate, with this feature having more of a gradual transition across the data, as opposed to distinct clusters that DBSCAN assumes.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#k-means-clustering",
    "href": "projects/DS5000/unsupervised-learning/main.html#k-means-clustering",
    "title": "Unsupervised Learning",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\n\nClusters: Produced 2 clear clusters:\n\nCluster 0: Aligned with higher voting rates.\n\nCluster 1: Aligned with lower voting rates.\n\n\nKey Insights:\n\nHigher voting rates are associated with:\n\nWhite and Asian populations\n\nMedian Age\n\nBachelor’s Degree attainment\n\n\nLower voting rates are associated with:\n\nAfrican American population\n\nBelow Poverty Level\n\n\n\nSummary: K-Means effectively captured strong correlations between clusters and features, proving to be an optimal choice for uncovering relationships in the data.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#dbscan-clustering",
    "href": "projects/DS5000/unsupervised-learning/main.html#dbscan-clustering",
    "title": "Unsupervised Learning",
    "section": "DBSCAN Clustering",
    "text": "DBSCAN Clustering\n\nOptimal EPS:\n\nAt EPS = 5: 1 large cluster, with 1 data point labeled as noise.\n\nAt EPS = 2.5: Clear group separation, but no actionable patterns or insights emerged.\n\n\nSummary: DBSCAN did not produce meaningful or valuable information for this dataset.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#hierarchical-clustering-2",
    "href": "projects/DS5000/unsupervised-learning/main.html#hierarchical-clustering-2",
    "title": "Unsupervised Learning",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n\nClusters: Produced 2 clusters:\n\nCluster 1: Higher voting rates.\n\nCluster 2: Lower voting rates.\n\n\nKey Observations:\n\nSimilar to K-Means, there was a clear cluster separation.\n\nResults were less distinct, with mixed cluster label distributions across many features.\n\n\nSummary: While hierarchical clustering highlighted general patterns, it was less effective than K-Means.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#best-algorithm",
    "href": "projects/DS5000/unsupervised-learning/main.html#best-algorithm",
    "title": "Unsupervised Learning",
    "section": "Best Algorithm",
    "text": "Best Algorithm\n\nThe best clustering algorithm for this dataset was K-Means, followed by hierarchical clustering.\n\nWhy K-Means?\n\nProduced the most distinct and clear insights into relationships between features and voting rates.\n\nLikely due to the linear relationships in the data, which K-Means efficiently uncovers.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/DS5000/unsupervised-learning/main.html#real-world-impact",
    "href": "projects/DS5000/unsupervised-learning/main.html#real-world-impact",
    "title": "Unsupervised Learning",
    "section": "Real World Impact",
    "text": "Real World Impact\nUtilizing the findings from K-Means, we uncovered hidden patterns between voting rates and socioeconomic groups:\n\nLower Education → Lower Voting Rates\n\nActionable Insight: Implement targeted voter education programs in communities with lower education levels, emphasizing the importance of civic participation.\n\nHigher Poverty Levels → Lower Voting Rates\n\nActionable Insight: Address socioeconomic barriers:\n\nImplement paid time off for voting for individuals in poverty, ensuring they have the flexibility to vote.\n\nDevelop childcare support and transportation programs for families and individuals in need on election day.\n\n\n\nBy leveraging these insights, policymakers and stakeholders can design targeted initiatives to increase voter participation and foster a more inclusive democracy.",
    "crumbs": [
      "About",
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "projects/Scholarship/recommendations.html",
    "href": "projects/Scholarship/recommendations.html",
    "title": "Recommendations",
    "section": "",
    "text": "Based on analysis of 25+ years of violence against aid workers, this section outlines key points and recommendations for key stakeholders in how to prevent further incidents and enhance aid worker safety.\n\n\n\n\nInvest in Local Staff Safety:\n\nWith the majority of aid workers killed being nationals (accounting for 97% of aid worker deaths), agencies must learn to take protective measures for local staff.\n\nBetter Preperation:\nBy implementing training programs to mitigate crises, while also employing security measures for high risk regions such as Gaza or South Sudan, local aid workers can be better equipped to respond to emergencies while also reducing their exposure to violence.\n\n\n\n\n\nIdentify Emerging Threats Quickly:\n\nData shows that violence against aid workers spike suddenly, with many agencies not expecting and prepared\n\nImplement Early Warning Systems:\n\nUsing satellite data, or sharing intel amongst different agencies can help mitigate risk and prepare agencies for sudden attacks or incidents.\n\n\n\n\n\n\nEnforce Accountability:\n\nStart Holding perpetrators accountable through the enforcement of international humanitarian law.\n\nClose the Justice Gap:\n\nCurrently, despite Aid workers being protected under international, many incidents go unpunished.\n\nIdentify Repeat Offenders:\n\nIn many cases, the same groups are responsible for the majority of incidents, such as in Palestine, the majority of deaths starting in 2023 were from aerial bombardments, often by state and foreign forces, while in South Sudan, non-state subnational groups are the main perpetrators since 2021, with no clear accountability.\n\n\n\n\n\n\nIn regions that are under severe conflict or blockade, exploring safer options of delivering aid such as drone delivery or telehealth.\n\n\n\n\n\nAllocate resources and security based on data-driven hotspots and high-risk areas, such as:\n\nKabul, Afghanistan\n\nTerrorism and insurgency\n\nGaza, Palestine\n\nMilitary occupation and airstrikes\n\nJuba & Rubkona, South Sudan\n\nCivil unrest and subnational violence\n\n\n\n\n\n\n\nBuild Trust on the Ground:\n\nEngaging with local leaders and communities will lead to shared trust, and can prevent further attacks while making communities safer for all parties involved.\n\nStrengthen Local Coordination:\n\nIn Afghanistan, shootings and ambushes are often targeted at national aid convoys, suggesting that the lack of local trust and coordination can increase risk.\n\nAddress Root Causes of Violence:\n\nIn South Sudan, violence by subnational non-state groups surged post-2021. These groups often emerge from local tensions and issues, indicating a need for local community engagement and support.\n\n\n\n\n\n\nWith violence against aid workers rising, especially since 2022, raising global awareness is essential.\nClear data visualizations and storytelling can highlight overlooked hotspots, such as South Sudan, inform donors and policymakers, and encourage stronger protections through public pressure and policy change.",
    "crumbs": [
      "About",
      "Recommendations"
    ]
  },
  {
    "objectID": "projects/Scholarship/recommendations.html#introduction",
    "href": "projects/Scholarship/recommendations.html#introduction",
    "title": "Recommendations",
    "section": "",
    "text": "Based on analysis of 25+ years of violence against aid workers, this section outlines key points and recommendations for key stakeholders in how to prevent further incidents and enhance aid worker safety.",
    "crumbs": [
      "About",
      "Recommendations"
    ]
  },
  {
    "objectID": "projects/Scholarship/recommendations.html#strengthen-aid-worker-protection",
    "href": "projects/Scholarship/recommendations.html#strengthen-aid-worker-protection",
    "title": "Recommendations",
    "section": "",
    "text": "Invest in Local Staff Safety:\n\nWith the majority of aid workers killed being nationals (accounting for 97% of aid worker deaths), agencies must learn to take protective measures for local staff.\n\nBetter Preperation:\nBy implementing training programs to mitigate crises, while also employing security measures for high risk regions such as Gaza or South Sudan, local aid workers can be better equipped to respond to emergencies while also reducing their exposure to violence.",
    "crumbs": [
      "About",
      "Recommendations"
    ]
  },
  {
    "objectID": "projects/Scholarship/recommendations.html#improve-real-time-risk-monitoring",
    "href": "projects/Scholarship/recommendations.html#improve-real-time-risk-monitoring",
    "title": "Recommendations",
    "section": "",
    "text": "Identify Emerging Threats Quickly:\n\nData shows that violence against aid workers spike suddenly, with many agencies not expecting and prepared\n\nImplement Early Warning Systems:\n\nUsing satellite data, or sharing intel amongst different agencies can help mitigate risk and prepare agencies for sudden attacks or incidents.",
    "crumbs": [
      "About",
      "Recommendations"
    ]
  },
  {
    "objectID": "projects/Scholarship/recommendations.html#impliment-legal-protection",
    "href": "projects/Scholarship/recommendations.html#impliment-legal-protection",
    "title": "Recommendations",
    "section": "",
    "text": "Enforce Accountability:\n\nStart Holding perpetrators accountable through the enforcement of international humanitarian law.\n\nClose the Justice Gap:\n\nCurrently, despite Aid workers being protected under international, many incidents go unpunished.\n\nIdentify Repeat Offenders:\n\nIn many cases, the same groups are responsible for the majority of incidents, such as in Palestine, the majority of deaths starting in 2023 were from aerial bombardments, often by state and foreign forces, while in South Sudan, non-state subnational groups are the main perpetrators since 2021, with no clear accountability.",
    "crumbs": [
      "About",
      "Recommendations"
    ]
  },
  {
    "objectID": "projects/Scholarship/recommendations.html#adapting-operations",
    "href": "projects/Scholarship/recommendations.html#adapting-operations",
    "title": "Recommendations",
    "section": "",
    "text": "In regions that are under severe conflict or blockade, exploring safer options of delivering aid such as drone delivery or telehealth.",
    "crumbs": [
      "About",
      "Recommendations"
    ]
  },
  {
    "objectID": "projects/Scholarship/recommendations.html#prioritize-high-risk-zones",
    "href": "projects/Scholarship/recommendations.html#prioritize-high-risk-zones",
    "title": "Recommendations",
    "section": "",
    "text": "Allocate resources and security based on data-driven hotspots and high-risk areas, such as:\n\nKabul, Afghanistan\n\nTerrorism and insurgency\n\nGaza, Palestine\n\nMilitary occupation and airstrikes\n\nJuba & Rubkona, South Sudan\n\nCivil unrest and subnational violence",
    "crumbs": [
      "About",
      "Recommendations"
    ]
  },
  {
    "objectID": "projects/Scholarship/recommendations.html#supporting-local-communities",
    "href": "projects/Scholarship/recommendations.html#supporting-local-communities",
    "title": "Recommendations",
    "section": "",
    "text": "Build Trust on the Ground:\n\nEngaging with local leaders and communities will lead to shared trust, and can prevent further attacks while making communities safer for all parties involved.\n\nStrengthen Local Coordination:\n\nIn Afghanistan, shootings and ambushes are often targeted at national aid convoys, suggesting that the lack of local trust and coordination can increase risk.\n\nAddress Root Causes of Violence:\n\nIn South Sudan, violence by subnational non-state groups surged post-2021. These groups often emerge from local tensions and issues, indicating a need for local community engagement and support.",
    "crumbs": [
      "About",
      "Recommendations"
    ]
  },
  {
    "objectID": "projects/Scholarship/recommendations.html#raise-global-awareness",
    "href": "projects/Scholarship/recommendations.html#raise-global-awareness",
    "title": "Recommendations",
    "section": "",
    "text": "With violence against aid workers rising, especially since 2022, raising global awareness is essential.\nClear data visualizations and storytelling can highlight overlooked hotspots, such as South Sudan, inform donors and policymakers, and encourage stronger protections through public pressure and policy change.",
    "crumbs": [
      "About",
      "Recommendations"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Soong Ping Hill. I am from San Francisco, California, and have a Bachelors Degree in Neuroscience and Minor in Data Science. I am currently pursuing a M.S degree in Data Science and Analytics at Georgetown University’s DSAN program.\nThis program has allowed me to develop a strong foundation in data-driven methodologies, machine learning, and advanced analytics while honing my skills in extracting meaningful insights from large datasets to solve real-world problems.\nI am passionate about leveraging data science to uncover patterns, drive decision-making, and contribute to impactful solutions across industries."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nGeorgetown University | Washington, D.C.\n- Master’s in Data Science (DSAN)\n- GPA: 3.95\nBoston University College of Arts and Science | Boston, MA\n- Bachelor of Arts in Neuroscience, Minor in Data Science\n- GPA: 3.57"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About Me",
    "section": "Work Experience",
    "text": "Work Experience\nSilkroll | Oakland, CA\nData Analyst | April 2025 – August 2025\n\nDesigned and executed MongoDB aggregation pipelines utilizing Metabase to analyze company-wide financial transactions, service fees, and inventory across cash and points-based orders.\n\nBuilt and optimized interactive dashboards to track sales, returns, and revenue trends, ensuring data accuracy and reliability across historical records.\n\nAutomated financial reporting processes, reducing manual effort and improving visibility for stakeholders.\n\nCenter for Retirement Initiatives | Washington, D.C.\nGraduate Student Research Assistant (Georgetown University) | October 2024 – March 2025\n\nAnalyzed state program data, creating monthly reports on financial metrics and retirement trends in Tableau.\n\nConducted data collection, cleaning, and dataset uploads using SQL and Python, ensuring data integrity.\n\nCollaborated with CRI stakeholders to identify new research inquiries and provide evidence-based insights.\n\nRESET.BUILD | Shanghai, China\nData Science Intern (BU Shanghai Internship Program) | June 2024 – August 2024\n\nDeveloped Python scripts to automate big data cleaning, extraction, format corrections, and air quality analysis, saving 8 hours of manual work per dataset.\n\nCollaborated with cross-functional teams to design and implement data dashboards in Google Looker Studio, improving data access and driving internal insights.\n\nEnhanced front-end UI using HTML and CSS, ensuring a professional and user-friendly experience.\n\nEcommerce Store | San Francisco, CA\nOwner and Manager | June 2023 – Present\n\nManaged e-commerce store with $9,000 in revenue, overseeing inventory, customer service, SEO, and marketing strategies.\n\nApplied business insights to adjust pricing strategies and product offerings, enhancing store profitability.\n\nRadboud University | Nijmegen, Netherlands\nMachine Learning Research Intern | May 2023 – July 2023\n\nProgrammed pipeline to cut, synchronize, and analyze over 2 TB of neuronal data and video.\n\nUtilized DeepLabCut pose-estimation to analyze cortico-hippocampal activity during memory consolidation.\n\nPartnered with a diverse research team and mentored new interns, creating a written guide for onboarding.\n\nUCSF Medical Center – Anesthesia | San Francisco, CA\nData Research Intern | May 2022 – August 2022\n\nContributed to a research study on pupillary unrest in ambient light in response to opioids.\n\nCleaned and visualized data for analysis, designing figures and reports.\n\nPublication: Merlin D. Larsson, Rachel McKay, Soong Ping Hill, Practical Guide to Portable Pupillometry"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About Me",
    "section": "Projects",
    "text": "Projects\n\nVoting Rate Analysis and Predictive Modeling\n\nIntegrated U.S Census Bureau API to collect and process demographic and socioeconomic data related to voting rates\nApplied clustering techniques (K-Means, DBSCAN, Agglomerative Clustering) to segment voters by behavior\nBuilt and evaluated predictive models (Random Forest, Regression) to analyze key factors influencing voting rates and predict voter participation trends\n\n\n\nStatistical Analysis of Airbnb Listing Prices Across Cities\n\nConducted statistical analysis (t-tests, ANOVAs, correlation, chi-squared) to evaluate how location, host status, and other factors impact Airbnb pricing\nVisualized data patterns with ggplot and presented actionable insights to optimize pricing strategies for hosts\n\n\n\nCovid-19 Data Analysis and Policy Identification\n\nIntegrated COVID-19 data from three sources into data lake via Azure Data Factory, prepared for processing in Azure Synapse, and analyzed in Power BI to identify the top three policies for controlling COVID spread\n\n\n\nAnalysis of U.S Airline Operational Performance\n\nQueried Bureau of Transportation Statistics flight data using SQL to analyze delays and cancellations across airlines and airports\nCreated an R-based report with visualizations to assess operational efficiency and highlight opportunities for improving flight scheduling and customer experience\nIdentified key delay and cancellation factors, providing data-driven insights to enhance airline performance\n\n\n\nNBA Player Performance Model\n\nBuilt and optimized an LSTM deep learning model to forecast NBA player performance based on historical game data\nModel explained 68% of variation in player performance, improving predictive power over traditional methods\nProvided data-driven insights for player evaluation and strategy optimization in sports management"
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nProgramming & Data Science\nProficient: Python (pandas, numpy, scikit-learn, matplotlib, seaborn), R (ggplot2, dplyr, tidyverse), SQL, PyTorch, TensorFlow\nFamiliar: Java, C++, MATLAB, Swift, JavaScript, HTML, CSS\n\n\nMachine Learning & Analytics\nProficient: Supervised Learning (Random Forest, Regression, SVM), Unsupervised Learning (K-Means, DBSCAN, Hierarchical Clustering), Deep Learning (LSTM, Neural Networks), Feature Selection, Cross-Validation, Statistical Analysis\nFamiliar: NLP, Computer Vision, Time Series Analysis, A/B Testing, Bayesian Methods\n\n\nData Engineering & Cloud\nProficient: AWS (S3, EC2, Lambda), Microsoft Azure (Data Factory, Synapse), SQL Server, PostgreSQL, Git/GitHub, Hadoop, Snowflake\nFamiliar: Google Cloud Platform, Apache Spark, Docker, Kubernetes, DBT, Apache Airflow\n\n\nVisualization & BI Tools\nProficient: Tableau, Looker Studio, Power BI, Matplotlib, Seaborn, ggplot2, Plotly\nFamiliar: D3.js, Bokeh, ArcGIS,\n\n\nDevelopment & Collaboration\nProficient: Git/GitHub, Jupyter, RStudio, VS Code, Excel, PowerPoint, Visio\nFamiliar: Linux/Unix, Bash scripting\nSpoken Languages: English (Native), Mandarin (Fluent), Cantonese (Conversational)\nSpoken Languages: English, Mandarin, Cantonese\nDownload Resume (PDF)"
  },
  {
    "objectID": "projects/DS5000/index.html",
    "href": "projects/DS5000/index.html",
    "title": "Predicting Voting Trends: Uncovering Patterns Through Data Science",
    "section": "",
    "text": "This project explores the factors that influence voting rates. By utilizing machine learning methods such as Dimensionality Reduction, Clustering, Regression, and Classification, the project identifies trends, evaluates model performance, and provides insights into voter participation patterns.\n\n\nVoting is a fundamental part of the democratic process, yet every year millions of people refrain from voting. Voting allows the people to speak for what they want, drive change, and elect new leaders. Despite the importance of voting, many states continue to have low voting rates, with many falling below the 50% mark.\nUnderstanding voting patterns is important for increasing voter turn out and participation. By using data science tools, this project addresses real-world challenges in predicting voter behavior and identifying areas of improvement\n\n\n\nWhat are the biggest variables that influence voter turn out?\nHow accurately can machine learning models predict voting rates?\nCan models effectively distinguish between Low, Medium, and High Voting Rates?\nWhat variable or demographic is responsible for highest voting rates\nWhat and who can government officials and initiatives target to raise voting rates in their district?",
    "crumbs": [
      "About",
      "Home"
    ]
  },
  {
    "objectID": "projects/DS5000/index.html#motivation",
    "href": "projects/DS5000/index.html#motivation",
    "title": "Predicting Voting Trends: Uncovering Patterns Through Data Science",
    "section": "",
    "text": "Voting is a fundamental part of the democratic process, yet every year millions of people refrain from voting. Voting allows the people to speak for what they want, drive change, and elect new leaders. Despite the importance of voting, many states continue to have low voting rates, with many falling below the 50% mark.\nUnderstanding voting patterns is important for increasing voter turn out and participation. By using data science tools, this project addresses real-world challenges in predicting voter behavior and identifying areas of improvement",
    "crumbs": [
      "About",
      "Home"
    ]
  },
  {
    "objectID": "projects/DS5000/index.html#research-question",
    "href": "projects/DS5000/index.html#research-question",
    "title": "Predicting Voting Trends: Uncovering Patterns Through Data Science",
    "section": "",
    "text": "What are the biggest variables that influence voter turn out?\nHow accurately can machine learning models predict voting rates?\nCan models effectively distinguish between Low, Medium, and High Voting Rates?\nWhat variable or demographic is responsible for highest voting rates\nWhat and who can government officials and initiatives target to raise voting rates in their district?",
    "crumbs": [
      "About",
      "Home"
    ]
  },
  {
    "objectID": "projects/DS5000/index.html#factors-of-voter-turnout-effects-of-habit-social-pressure-and-gerrymandering-on-voting-behavior",
    "href": "projects/DS5000/index.html#factors-of-voter-turnout-effects-of-habit-social-pressure-and-gerrymandering-on-voting-behavior",
    "title": "Predicting Voting Trends: Uncovering Patterns Through Data Science",
    "section": "Factors of Voter Turnout: Effects of Habit, Social Pressure, and Gerrymandering on Voting Behavior",
    "text": "Factors of Voter Turnout: Effects of Habit, Social Pressure, and Gerrymandering on Voting Behavior\nGibbons 2022\n\nIn this review, Gibbons, rather than analyzing the socio-economic and racial differences that may influence voter turnout, analyzes the history of the United States, social pressures, and voter behaviors as key factors for voting.\nGibbons analyzes Gerrymandering (the manipulation of district boundaries in favor of a political party) and concludes:\n\nGerrymandering does not necessarily correlate to voter turnout but rather influences district results more.\n\nVoters who habitually vote have higher likelihoods of continually voting:\n\nCitizens who exhibit voting as a habit have a higher influence on voting rates than other factors such as outcome preferences or political ties.\n\nGibbons touches on the influence of Social Pressure:\n\nAltruism: Emphasizing collective benefits.\n\nTribalism: Loyalty to a group.\n\nSocial media has contributed to voter turnout by amplifying social influence.\n\nSolutions Proposed:\n\nImplementing more K-12 Civic Education Programs, such as “Kids Voting USA”, that emphasize civic skills and understanding.\n\nAssigning election days as holidays to reduce barriers for working-class individuals.\n\nPublic support: 71% of Democrats and 59% of Republicans are in favor.\n\n\nIntroducing mandatory voting, which is controversial but successful in countries like Australia, where citizens face minor penalties for not voting.",
    "crumbs": [
      "About",
      "Home"
    ]
  },
  {
    "objectID": "projects/DS5000/index.html#what-affects-voter-turnout",
    "href": "projects/DS5000/index.html#what-affects-voter-turnout",
    "title": "Predicting Voting Trends: Uncovering Patterns Through Data Science",
    "section": "What Affects Voter Turnout?",
    "text": "What Affects Voter Turnout?\nBlais 2016\n\nIn “What Affects Voter Turnout?” by Andre Blais, Blais analyzes voter turnout in different countries to identify the biggest factors influencing turnout.\nKey Findings:\n\nCountries that require citizens to vote, such as Australia, consistently had 10-15% higher voter turnout compared to countries without mandatory voting.\n\nCountries with an Electoral System showed higher turnouts in advanced democracies but failed to show similar results in less established democracies (e.g., Latin America).\n\nIn smaller countries and advanced economies, voter turnout tends to be higher.\n\nClosely contested elections consistently resulted in higher voter turnout.",
    "crumbs": [
      "About",
      "Home"
    ]
  },
  {
    "objectID": "projects/DS5000/index.html#a-study-of-key-factors-influencing-youth-voter-turnout",
    "href": "projects/DS5000/index.html#a-study-of-key-factors-influencing-youth-voter-turnout",
    "title": "Predicting Voting Trends: Uncovering Patterns Through Data Science",
    "section": "A Study of Key Factors Influencing Youth Voter Turnout",
    "text": "A Study of Key Factors Influencing Youth Voter Turnout\nZhu 2021\n\nIn “A Study of Key Factors Influencing Youth Voter Turnout”, Claire Zhu analyzes the major factors influencing youth voter turnout.\nRace and Ethnicity:\n\nZhu found that race and ethnicity had a significant impact on voter turnout, but the gaps are narrowing:\n\nExample: Voter turnout between White and Black voters has become closer in some regions due to increased awareness and civic understanding.\n\nDisparities remain: Hispanic and Asian populations consistently vote at lower rates compared to White and Black populations.\n\n\nSocial Media:\n\nSocial media is a huge driver of voting for youth voters, increasing motivation and awareness.\n\nGender Differences:\n\nWomen are slightly more likely to vote than men, but the difference is minimal and has little impact on overall turnout.\n\nSocioeconomic Factors:\n\nAreas with low-income levels are correlated with lower voter participation.\n\nAreas with higher education levels correlate with higher voter participation.\n\nReasons for Low Turnout:\n\nDistrust in candidates.\n\nThe inconvenience of voting.\n\nA general disinterest in politics.\n\nZhu concluded that youth voter turnout is influenced by a combination of:\n\nRacial factors.\n\nSocioeconomic inequalities.\n\nInstitutional barriers.\n\nProposed Solutions:\n\nAddress socioeconomic inequalities.\n\nMake voting easier.\n\nIncrease social media presence to drive awareness.",
    "crumbs": [
      "About",
      "Home"
    ]
  },
  {
    "objectID": "projects/Scholarship/data_chloropeth.html",
    "href": "projects/Scholarship/data_chloropeth.html",
    "title": "Soong Ping Hill",
    "section": "",
    "text": "Code\nimport altair as alt\nimport pandas as pd\nfrom vega_datasets import data\n\n# Load data\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Complete mapping of ISO alpha-2 codes to numeric codes\ncountry_code_map = {\n    'AF': 4,    # Afghanistan\n    'AL': 8,    # Albania\n    'AM': 51,   # Armenia\n    'AO': 24,   # Angola\n    'AR': 32,   # Argentina\n    'AZ': 31,   # Azerbaijan\n    'BD': 50,   # Bangladesh\n    'BF': 854,  # Burkina Faso\n    'BI': 108,  # Burundi\n    'BJ': 204,  # Benin\n    'BO': 68,   # Bolivia\n    'BW': 72,   # Botswana\n    'CD': 180,  # Democratic Republic of the Congo\n    'CF': 140,  # Central African Republic\n    'CG': 178,  # Republic of the Congo\n    'CI': 384,  # Côte d'Ivoire\n    'CL': 152,  # Chile\n    'CM': 120,  # Cameroon\n    'CO': 170,  # Colombia\n    'DO': 214,  # Dominican Republic\n    'DZ': 12,   # Algeria\n    'EC': 218,  # Ecuador\n    'EG': 818,  # Egypt\n    'EH': 732,  # Western Sahara\n    'ER': 232,  # Eritrea\n    'ET': 231,  # Ethiopia\n    'FJ': 242,  # Fiji\n    'GE': 268,  # Georgia\n    'GN': 324,  # Guinea\n    'GT': 320,  # Guatemala\n    'GW': 624,  # Guinea-Bissau\n    'GY': 328,  # Guyana\n    'HN': 340,  # Honduras\n    'HT': 332,  # Haiti\n    'ID': 360,  # Indonesia\n    'IL': 376,  # Israel\n    'IN': 356,  # India\n    'IQ': 368,  # Iraq\n    'IR': 364,  # Iran\n    'JM': 388,  # Jamaica\n    'JO': 400,  # Jordan\n    'KE': 404,  # Kenya\n    'KG': 417,  # Kyrgyzstan\n    'KH': 116,  # Cambodia\n    'LB': 422,  # Lebanon\n    'LK': 144,  # Sri Lanka\n    'LR': 430,  # Liberia\n    'LS': 426,  # Lesotho\n    'LY': 434,  # Libya\n    'MG': 450,  # Madagascar\n    'ML': 466,  # Mali\n    'MM': 104,  # Myanmar\n    'MR': 478,  # Mauritania\n    'MU': 480,  # Mauritius\n    'MW': 454,  # Malawi\n    'MX': 484,  # Mexico\n    'MZ': 508,  # Mozambique\n    'NE': 562,  # Niger\n    'NG': 566,  # Nigeria\n    'NI': 558,  # Nicaragua\n    'NP': 524,  # Nepal\n    'PE': 604,  # Peru\n    'PG': 598,  # Papua New Guinea\n    'PH': 608,  # Philippines\n    'PK': 586,  # Pakistan\n    'PL': 616,  # Poland\n    'PS': 275,  # Palestinian Territories (using unofficial code)\n    'RW': 646,  # Rwanda\n    'SA': 682,  # Saudi Arabia\n    'SD': 729,  # Sudan\n    'SL': 694,  # Sierra Leone\n    'SN': 686,  # Senegal\n    'SO': 706,  # Somalia\n    'SS': 728,  # South Sudan\n    'SV': 222,  # El Salvador\n    'SY': 760,  # Syria\n    'SZ': 748,  # Eswatini (formerly Swaziland)\n    'TD': 148,  # Chad\n    'TH': 764,  # Thailand\n    'TJ': 762,  # Tajikistan\n    'TN': 788,  # Tunisia\n    'TR': 792,  # Turkey\n    'TZ': 834,  # Tanzania\n    'UA': 804,  # Ukraine\n    'UG': 800,  # Uganda\n    'UY': 858,  # Uruguay\n    'VE': 862,  # Venezuela\n    'VN': 704,  # Vietnam\n    'XK': 0,    # Kosovo (no official ISO numeric code, using 0 as placeholder)\n    'YE': 887,  # Yemen\n    'ZA': 710,  # South Africa\n    'ZM': 894,  # Zambia\n    'ZW': 716,  # Zimbabwe\n}\n\n# Convert country codes to numeric IDs for the map\n# Convert country codes to numeric IDs for the map\nif 'Country Code' in df.columns:\n    # Map ISO alpha-2 codes to numeric codes\n    df['id'] = df['Country Code'].map(country_code_map)\n    \n    # Handle missing values\n    print(f\"Rows before dropping NaN 'id': {len(df)}\")\n    df = df.dropna(subset=['id'])\n    print(f\"Rows after dropping NaN 'id': {len(df)}\")\n    \n    # Convert id to integer for proper joining\n    df['id'] = df['id'].astype(int)\n    \n    # 🔑 NEW: Convert id to string for Altair map matching\n    df['id'] = df['id'].astype(str)\nelse:\n    print(\"Warning: 'Country Code' column not found. Check your data.\")\n\n\n# Group by country (using mapped id) and year, counting incidents\n# Group by country (using mapped id) and year, counting incidents\ndf_grouped = df.groupby(['id', 'Year']).size().reset_index(name='Incident Count')\n\n# 🔑 NEW: Ensure df_grouped['id'] is also a string\ndf_grouped['id'] = df_grouped['id'].astype(str)\n\n\n# Get unique years for the dropdown\nyears = sorted(df_grouped['Year'].unique())\n\n# Load world map TopoJSON\ncountries = alt.topo_feature(data.world_110m.url, 'countries')\n\n# Create year selection dropdown\ninput_dropdown = alt.binding_select(options=years, name=\"Select_Year\")\nselection = alt.selection_point(fields=['Year'], bind=input_dropdown)\n\n# Base map (always visible regardless of data)\nbase_map = alt.Chart(countries).mark_geoshape(\n    fill='#f0f0f0',  # Light gray for countries with no data\n    stroke='white',\n    strokeWidth=0.5\n).properties(\n    width=800,\n    height=450\n)\n\n# Data layer showing incidents\nchoropleth = alt.Chart(countries).mark_geoshape().encode(\n    color=alt.Color(\n        'Incident Count:Q',\n        scale=alt.Scale(scheme='reds'),\n        legend=alt.Legend(title=\"Incident Count\")\n    ),\n    tooltip=[\n        alt.Tooltip('id:N', title='Country Code'),\n        alt.Tooltip('Incident Count:Q', title='Incidents'),\n        alt.Tooltip('Year:N', title='Year')\n    ]\n).transform_filter(\n    selection  # FIRST: filter by year\n).transform_lookup(\n    lookup='id',\n    from_=alt.LookupData(\n        df_grouped, 'id', ['Incident Count', 'Year']\n    ),\n    default={'Incident Count': 0, 'Year': None}\n).project(\n    type='equirectangular'\n)\n\n\n\n# Combine base map and data layer\nfinal_map = alt.layer(\n    base_map,\n    choropleth\n).properties(\n    title='Incidents Against Aid Workers by Country and Year'\n).add_selection(\n    selection\n)\n\nfinal_map\n\n\nRows before dropping NaN 'id': 4337\nRows after dropping NaN 'id': 4304\n\n\n/var/folders/4_/dx87jsdx70xbn5dlmqlqrgkr0000gn/T/ipykernel_6132/3555367369.py:185: AltairDeprecationWarning:\n\n\nDeprecated since `altair=5.0.0`. Use add_params instead.\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport altair as alt\nimport pandas as pd\nfrom vega_datasets import data\n\n# Load data\ndf = pd.read_csv(\"security_incidents.csv\")\nprint(\"🔹 Initial rows:\", len(df))\n\n# Check year column types and missing values\nprint(\"\\n🧠 Year column type:\", df['Year'].dtype)\nprint(\"❓ Null years:\", df['Year'].isna().sum())\nprint(\"🪵 Sample years:\")\nprint(df['Year'].value_counts(dropna=False).sort_index().head(20))\n\n# Show rows where year is NaN or not an integer\nprint(\"\\n🔎 Rows with weird years:\")\nprint(df[df['Year'].isna() | ~df['Year'].apply(lambda x: str(x).isdigit())].head(10))\n\n\n# Clean up year column\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')  # force numbers, turn bad ones to NaN\ndf = df.dropna(subset=['Year'])  # drop rows with missing year\ndf['Year'] = df['Year'].astype(int)  # make sure it's an int\n\n\n# Convert country codes\ncountry_code_map = {\n    'AF': 4, 'AL': 8, 'AM': 51, 'AO': 24, 'AR': 32, 'AZ': 31, 'BD': 50, 'BF': 854,\n    'BI': 108, 'BJ': 204, 'BO': 68, 'BW': 72, 'CD': 180, 'CF': 140, 'CG': 178,\n    'CI': 384, 'CL': 152, 'CM': 120, 'CO': 170, 'DO': 214, 'DZ': 12, 'EC': 218,\n    'EG': 818, 'EH': 732, 'ER': 232, 'ET': 231, 'FJ': 242, 'GE': 268, 'GN': 324,\n    'GT': 320, 'GW': 624, 'GY': 328, 'HN': 340, 'HT': 332, 'ID': 360, 'IL': 376,\n    'IN': 356, 'IQ': 368, 'IR': 364, 'JM': 388, 'JO': 400, 'KE': 404, 'KG': 417,\n    'KH': 116, 'LB': 422, 'LK': 144, 'LR': 430, 'LS': 426, 'LY': 434, 'MG': 450,\n    'ML': 466, 'MM': 104, 'MR': 478, 'MU': 480, 'MW': 454, 'MX': 484, 'MZ': 508,\n    'NE': 562, 'NG': 566, 'NI': 558, 'NP': 524, 'PE': 604, 'PG': 598, 'PH': 608,\n    'PK': 586, 'PL': 616, 'PS': 275, 'RW': 646, 'SA': 682, 'SD': 729, 'SL': 694,\n    'SN': 686, 'SO': 706, 'SS': 728, 'SV': 222, 'SY': 760, 'SZ': 748, 'TD': 148,\n    'TH': 764, 'TJ': 762, 'TN': 788, 'TR': 792, 'TZ': 834, 'UA': 804, 'UG': 800,\n    'UY': 858, 'VE': 862, 'VN': 704, 'XK': 0, 'YE': 887, 'ZA': 710, 'ZM': 894, 'ZW': 716\n}\n\n# Map ISO alpha-2 to numeric\ndf['id'] = df['Country Code'].map(country_code_map)\nprint(\"🔹 Rows with mapped country codes:\", df['id'].notnull().sum())\n\n# Drop nulls, convert to string for TopoJSON\ndf = df.dropna(subset=['id'])\ndf['id'] = df['id'].astype(int).astype(str)\n\nprint(\"🔹 Sample of mapped 'id':\", df['id'].unique()[:5])\nprint(\"🔹 Year range:\", df['Year'].min(), \"to\", df['Year'].max())\n\n# Group by id + year\ndf_grouped = df.groupby(['id', 'Year']).size().reset_index(name='Incident Count')\ndf_grouped['id'] = df_grouped['id'].astype(str)\n\nprint(\"🔹 Sample grouped data:\")\nprint(df_grouped.head(10))\n\n# Show how many countries have incidents in a specific year\ntarget_year = 2002\nsample = df_grouped[df_grouped['Year'] == target_year]\nprint(f\"🔹 Countries with incidents in {target_year}: {len(sample)}\")\nprint(sample)\n\n# Create dropdown\nyears = sorted(df_grouped['Year'].unique())\ninput_dropdown = alt.binding_select(options=years, name=\"Select_Year\")\nselection = alt.selection_point(fields=['Year'], bind=input_dropdown)\n\n# Load map\ncountries = alt.topo_feature(data.world_110m.url, 'countries')\n\n# Base map\nbase_map = alt.Chart(countries).mark_geoshape(\n    fill='#f0f0f0',\n    stroke='white',\n    strokeWidth=0.5\n).properties(\n    width=800,\n    height=450\n)\n\n# Data layer\n# Data layer\nchoropleth = alt.Chart(countries).mark_geoshape().encode(\n    color=alt.Color(\n        'Incident Count:Q',\n        scale=alt.Scale(scheme='reds'),\n        legend=alt.Legend(title=\"Incident Count\")\n    ),\n    tooltip=[\n        alt.Tooltip('id:N', title='Country Code'),\n        alt.Tooltip('Incident Count:Q', title='Incidents'),\n        alt.Tooltip('Year:N', title='Year')\n    ]\n)\n# NEW: filter by selected year first, THEN lookup incidents\nchoropleth = choropleth.transform_filter(\n    selection\n).transform_lookup(\n    lookup='id',\n    from_=alt.LookupData(\n        df_grouped, 'id', ['Incident Count', 'Year']\n    ),\n    default={'Incident Count': 0, 'Year': None}\n).project(\n    type='equirectangular'\n)\n\n\n# Combine layers\nfinal_map = alt.layer(\n    base_map,\n    choropleth\n).properties(\n    title='Incidents Against Aid Workers by Country and Year'\n).add_selection(\n    selection\n)\n\nfinal_map\n\n\n🔹 Initial rows: 4337\n\n🧠 Year column type: int64\n❓ Null years: 0\n🪵 Sample years:\nYear\n1997     34\n1998     26\n1999     32\n2000     42\n2001     29\n2002     46\n2003     63\n2004     64\n2005     74\n2006    107\n2007    124\n2008    165\n2009    156\n2010    132\n2011    152\n2012    170\n2013    265\n2014    194\n2015    150\n2016    164\nName: count, dtype: int64\n\n🔎 Rows with weird years:\nEmpty DataFrame\nColumns: [Incident ID, Year, Month, Day, Country Code, Country, Region, District, City, UN, INGO, ICRC, NRCS and IFRC, NNGO, Other, Nationals killed, Nationals wounded, Nationals kidnapped, Total nationals, Internationals killed, Internationals wounded, Internationals kidnapped, Total internationals, Total killed, Total wounded, Total kidnapped, Total affected, Gender Male, Gender Female, Gender Unknown, Means of attack, Attack context, Location, Latitude, Longitude, Motive, Actor type, Actor name, Details, Verified, Source]\nIndex: []\n\n[0 rows x 41 columns]\n🔹 Rows with mapped country codes: 4304\n🔹 Sample of mapped 'id': ['116' '646' '762' '706' '180']\n🔹 Year range: 1997 to 2025\n🔹 Sample grouped data:\n    id  Year  Incident Count\n0    0  1998               2\n1    0  2000               1\n2    0  2001               1\n3  104  2011               1\n4  104  2013               1\n5  104  2014               1\n6  104  2015               2\n7  104  2017               1\n8  104  2018               1\n9  104  2019               3\n🔹 Countries with incidents in 2002: 16\n      id  Year  Incident Count\n20   108  2002               3\n91   170  2002               1\n102  178  2002               1\n108  180  2002               2\n168   24  2002               2\n176  275  2002               5\n244  368  2002               1\n276    4  2002               6\n300  400  2002               2\n339  430  2002               1\n511  706  2002               7\n534  710  2002               1\n543  728  2002               1\n565  729  2002               2\n618  800  2002               3\n660  887  2002               1\n\n\n/var/folders/4_/dx87jsdx70xbn5dlmqlqrgkr0000gn/T/ipykernel_6132/3499150095.py:119: AltairDeprecationWarning:\n\n\nDeprecated since `altair=5.0.0`. Use add_params instead.\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Re-run without altair for now, just to inspect data\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Clean and process Year column\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\ndf = df.dropna(subset=['Year', 'Country'])\ndf['Year'] = df['Year'].astype(int)\n\n# Group total incidents per year\nyearly_counts = df.groupby('Year').size().reset_index(name='Incident Count')\n\n# Identify baseline (e.g., year 2000)\nbaseline = yearly_counts[yearly_counts['Year'] == 2000]['Incident Count'].values[0]\nlatest = yearly_counts[yearly_counts['Year'] == yearly_counts['Year'].max()]['Incident Count'].values[0]\ntripled = latest &gt;= 3 * baseline\n\n# Group total incidents by country\ncountry_counts = df.groupby('Country').size().reset_index(name='Incident Count')\ntop_countries = country_counts.sort_values(by='Incident Count', ascending=False).head(10)\n\nprint(top_countries)\n\n\n                             Country  Incident Count\n0                        Afghanistan             610\n76                       South Sudan             583\n78                             Sudan             381\n80              Syrian Arab Republic             368\n74                           Somalia             315\n21                          DR Congo             233\n64  Occupied Palestinian Territories             158\n14          Central African Republic             142\n53                              Mali             126\n65                          Pakistan             113\n\n\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Load the dataset\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Clean and prepare the data\ndf['Year'] = pd.to_numeric(df['Year'], errors='coerce')\ndf = df.dropna(subset=['Year', 'Country'])\ndf['Year'] = df['Year'].astype(int)\n\n# Group by year for incidents trend\nyearly_incidents = df.groupby('Year').size().reset_index(name='Incident Count')\n\n# Filter from 2000 onwards\nyearly_incidents_filtered = yearly_incidents[yearly_incidents['Year'] &gt;= 2000]\n\n# Group by country for top 10 dangerous countries\ncountry_incidents = df.groupby('Country').size().reset_index(name='Incident Count')\ntop_countries = country_incidents.sort_values('Incident Count', ascending=False).head(10)\n\n# Line chart: Incidents over time\nline_chart = alt.Chart(yearly_incidents_filtered).mark_line(point=True).encode(\n    x=alt.X('Year:O', title='Year'),\n    y=alt.Y('Incident Count:Q', title='Total Incidents'),\n    tooltip=['Year', 'Incident Count']\n).properties(\n    width=700,\n    height=400,\n    title='Total Aid Worker Incidents by Year (2000–2025)'\n)\n\n# Bar chart: Top 10 countries\nbar_chart = alt.Chart(top_countries).mark_bar().encode(\n    x=alt.X('Incident Count:Q', title='Total Incidents'),\n    y=alt.Y('Country:N', sort='-x', title='Country'),\n    tooltip=['Country', 'Incident Count']\n).properties(\n    width=700,\n    height=400,\n    title='Top 10 Countries with Most Aid Worker Incidents (1997–2025)'\n)\n\n\n\nline_chart & bar_chart\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv(\"security_incidents.csv\")\n\n# Filter to Afghanistan (or whatever country)\ndf_afg = df[df['Country'] == 'Afghanistan']\ndf_afg = df_afg.dropna(subset=['Latitude', 'Longitude'])\n\n\nimport altair as alt\n\n# Load your custom GeoJSON as a TopoFeature\nafghanistan_map = alt.Data(url=\"custom.geo.json\", format=alt.DataFormat(property='features', type='json'))\n\n# Base map from GeoJSON\nbase_map = alt.Chart(afghanistan_map).mark_geoshape(\n    fill='lightgray',\n    stroke='white'\n).properties(\n    width=800,\n    height=500\n).project(\n    type='mercator',\n    center=[66.5, 33.5],\n    scale=3600   # Slightly less zoom than before\n)\n\n       # Zoom level\n\n\n\nbase_map\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot points\npoints = alt.Chart(df_afg).mark_circle(opacity=0.6, color='red').encode(\n    longitude='Longitude:Q',\n    latitude='Latitude:Q',\n    size=alt.value(30),\n    tooltip=[\n        alt.Tooltip('Year:O'),\n        alt.Tooltip('City:N'),\n        alt.Tooltip('Means of attack:N'),\n        alt.Tooltip('Total killed:Q'),\n        alt.Tooltip('Actor type:N')\n    ]\n)\n\nafghanistan_map_chart = base_map + points\nafghanistan_map_chart",
    "crumbs": [
      "About",
      "Data Choropleth"
    ]
  }
]